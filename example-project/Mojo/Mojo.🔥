Mojo🔥Systems programming language built using MLIR.
MLIR - a modern compiler infrastructure for heterogeneous hardware (CPUs, GPUs, ASICs or AI chips).
That means you can use one language to write all your code, from high-level AI applications to
all the way down to low-level GPU kernels, without using any hardware-specific libraries
(such as CUDA and ROCm).
• Python syntax & interop:
    Mojo adopts (and extends) Python's syntax and integrates with existing Python ecosystem,
    including its wealth of AI libraries. Mojo's interoperability works in both directions,
    so you can import Python libraries into Mojo and create Mojo bindings to call from Python.
• Struct-based types: 
    All data types are defined as structs (including basic types such as String and Int). 
    No types are built into the language itself. That means you can define your own types 
    that have all the the same capabilities as the standard library types.
• Zero-cost traits:
    Mojo's trait system solves the problem of static typing by letting you define a shared
    set of behaviors that types (structs) can implement. It allows you to write functions
    that depend on traits rather than specific types, except with compile-time type checking
    and no run-time performance cost.
• Value ownership:
    Mojo's ownership system ensures that only one variable "owns" a specific value at a given
    time-such that Mojo can safely deallocate the value when the owner's lifetime ends-while
    still allowing you to share references to the value. This provides safety from errors such
    as use-after-free, double-free, and memory leaks without the overhead cost of a garbage collector.
    Mojo incorporates features like static type checking, memory safety.
• Compile-time metaprogramming: 
    Mojo's parameterization system enables powerful metaprogramming in
    which the compiler generates a unique version of a type or function based on parameter values.
• Hardware portability:
    Mojo compiler makes no assumptions about whether your code is written for CPUs, GPUs, or
    something else. Instead, hardware behaviors are handled by Mojo libraries, as demonstrated
    by types such as SIMD that allows you to write vectorized code for CPUs, and the GPU package
    that enables hardware-agnostic GPU programming.

def main():
    print("Hello, world!")

Every Mojo program must include a function named main() as the entry point,
write def main(): followed by an indented function body.
The print() statement prints its arguments to the standard output.

Variables
Declare a variable by simply assigning a value to a new named variable:

def main():
    x = 10
    y = x * x
    print(y)

You can also explicitly declare variables with the var keyword:

var x = 10

When declaring a variable with var, you can also declare a variable type, with or without an assignment:

def main():
    var x: Int = 10
    var sum: Int
    sum = x + x

Both implicitly declared and explicitly declared variables are statically typed:
that is, the type is set at compile time, and doesn't change at runtime.
If you don't specify a type, Mojo uses the type of the first value assigned to the variable.

x = 10
x = "Foo" # Error: Cannot convert "StringLiteral" value to "Int"

Blocks and statements​
Code blocks such as functions, conditions, and loops are defined with a colon followed by indented lines.
For example:

def loop():
    for x in range(5):
        if x % 2 == 0:
            print(x)

You can use any number of spaces or tabs for your indentation (we prefer 4 spaces).
All code statements in Mojo end with a newline. However, statements can span multiple lines if you
indent the following lines. For example, this long string spans two lines:

def print_line():
    long_text = "This is a long line of text that is a lot easier to read if"
                " it is broken up across two lines instead of one long line."
    print(long_text)

And you can chain function calls across lines:

def print_hello():
    text = ",".join("Hello", " world!")
    print(text)

Functions​
You can define a Mojo function using either the def or fn keyword. For example, the following uses the
def keyword to define a function named greet that requires a single String argument and returns a String:
def greet(name: String) -> String:
    return "Hello, " + name + "!"
Where def and fn differ is error handling and argument mutability defaults.
Code comments​
You can create a one-line comment using the hash # symbol:
# This is a comment. The Mojo compiler ignores this line.
Comments may also follow some code:
var message = "Hello, World!" # This is also a valid comment
API documentation comments are enclosed in triple quotes. For example:
fn print(x: String):
    """Prints a string.

    Args:
        x: The string to print.
    """
    ...
Documenting your code with these kinds of comments (known as "docstrings") is a topic we've yet to fully specify, but you can generate an API reference from docstrings using the mojo doc command.
Technically, docstrings aren't comments, they're a special use of Mojo's syntax for multi-line string literals.
Structs​
You can build high-level abstractions for types (or "objects") as a struct.
A struct in Mojo is similar to a class in Python: they both support methods, fields, operator overloading, decorators for metaprogramming, and so on. However, Mojo structs are completely static-they are bound at compile-time, so they do not allow dynamic dispatch or any runtime changes to the structure. (Mojo will also support Python-style classes in the future.)
For example, here's a basic struct:
struct MyPair(Copyable):
    var first: Int
    var second: Int

    fn __init__(out self, first: Int, second: Int):
        self.first = first
        self.second = second

    fn __copyinit__(out self, existing: Self):
        self.first = existing.first
        self.second = existing.second

    def dump(self):
        print(self.first, self.second)
And here's how you can use it:
def use_mypair():
    var mine = MyPair(2, 4)
    mine.dump()
Note that some functions are declared with fn function, while the dump() function is declared with def. In general, you can use either form in a struct.
The MyPair struct contains two special methods, __init__(), the constructor, and __copyinit__(), the copy constructor. Lifecycle methods like this control how a struct is created, copied, moved, and destroyed.
For most simple types, you don't need to write the lifecycle methods. You can use the @fieldwise_init decorator to generate the boilerplate field-wise initializer for you, and Mojo will synthesize copy and move constructors if you ask for them with trait conformance. So the MyPair struct can be simplified to this:
@fieldwise_init
struct MyPair(Copyable, Movable):
    var first: Int
    var second: Int

    def dump(self):
        print(self.first, self.second)
Traits​
A trait is like a template of characteristics for a struct. If you want to create a struct with the characteristics defined in a trait, you must implement each characteristic (such as each method). Each characteristic in a trait is a "requirement" for the struct, and when your struct implements all of the requirements, it's said to "conform" to the trait.
Using traits allows you to write generic functions that can accept any type that conforms to a trait, rather than accept only specific types.
For example, here's how you can create a trait:
trait SomeTrait:
    fn required_method(self, x: Int): ...
The three dots following the method signature are Mojo syntax indicating that the method is not implemented.
Here's a struct that conforms to SomeTrait:
@fieldwise_init
struct SomeStruct(SomeTrait):
    fn required_method(self, x: Int):
        print("hello traits", x)
Then, here's a function that uses the trait as an argument type (instead of the struct type):
fn fun_with_traits[T: SomeTrait](x: T):
    x.required_method(42)

fn use_trait_function():
    var thing = SomeStruct()
    fun_with_traits(thing)
You'll see traits used in a lot of APIs provided by Mojo's standard library. For example, Mojo's collection types like List and Dict can store any type that conforms to the Copyable and Movable traits. You can specify the type when you create a collection:
my_list = List[Float64]()
You're probably wondering about the square brackets on fun_with_traits(). These aren't function arguments (which go in parentheses); these are function parameters, which we'll explain next.
Without traits, the x argument in fun_with_traits() would have to declare a specific type that implements required_method(), such as SomeStruct (but then the function would accept only that type). With traits, the function can accept any type for x as long as it conforms to (it "implements") SomeTrait. Thus, fun_with_traits() is known as a "generic function" because it accepts a generalized type instead of a specific type.
Parameterization​
In Mojo, a parameter is a compile-time variable that becomes a runtime constant, and it's declared in square brackets on a function or struct. Parameters allow for compile-time metaprogramming, which means you can generate or modify code at compile time.
Many other languages use "parameter" and "argument" interchangeably, so be aware that when we say things like "parameter" and "parametric function," we're talking about these compile-time parameters. Whereas, a function "argument" is a runtime value that's declared in parentheses.
Parameterization is a complex topic that's covered in much more detail in the Metaprogramming section, but we want to break the ice just a little bit here. To get you started, let's look at a parametric function:
def repeat[count: Int](msg: String):
    @parameter # evaluate the following for loop at compile time
    for i in range(count):
        print(msg)
This function has one parameter of type Int and one argument of type String. To call the function, you need to specify both the parameter and the argument:
def call_repeat():
    repeat[3]("Hello")
    # Prints "Hello" 3 times
By specifying count as a parameter, the Mojo compiler is able to optimize the function because this value is guaranteed to not change at runtime. And the @parameter decorator in the code tells the compiler to evaluate the for loop at compile time, not runtime.
The compiler effectively generates a unique version of the repeat() function that repeats the message only 3 times. This makes the code more performant because there's less to compute at runtime.
Similarly, you can define a struct with parameters, which effectively allows you to define variants of that type at compile-time, depending on the parameter values.
Python integration​
Mojo supports the ability to import Python modules as-is, so you can leverage existing Python code right away.
For example, here's how you can import and use NumPy:
from python import Python

def main():
    var np = Python.import_module("numpy")
    var ar = np.arange(15).reshape(3, 5)
    print(ar)
    print(ar.shape)
You must have the Python module (such as numpy) installed in the environment where you're using Mojo.



Functions
As mentioned in the syntax overview, Mojo supports two keywords to declare functions: def and fn. You can use either declaration with any function, including the main() function, but they have different default behaviors, as described on this page.
We believe both def and fn have good use cases and don't consider either to be better than the other. Deciding which to use is a matter of personal taste as to which style best fits a given task.
Functions declared inside a struct are called "methods," but they have all the same qualities as "functions" described here.
Anatomy of a function​
Both def and fn function declarations have the same basic components (here demonstrated with a def function):
def function_name[
​    parameters ...
](
​    arguments ...
) -> return_value_type:
​    function_body
Functions can have:
* Parameters: A function can optionally take one or more compile-time parameter values used for metaprogramming.
* Arguments: A function can also optionally take one or more run-time arguments.
* Return value: A function can optionally return a value.
* Function body: Statements that are executed when you call the function. Function definitions must include a body.
All of the optional parts of the function can be omitted, so the minimal function is something like this:
def do_nothing():
    pass
If a function takes no parameters, you can omit the square brackets, but the parentheses are always required.
Although you can't leave out the function body, you can use the pass statement to define a function that does nothing.
Arguments and parameters​
Functions take two kinds of inputs: arguments and parameters. Arguments are familiar from many other languages: they are run-time values passed into the function.
def add(a: Int, b: Int) -> Int:
    return a+b
On the other hand, you can think of a parameter as a compile-time variable that becomes a run-time constant. For example, consider the following function with a parameter:
def add_tensors[rank: Int](a: MyTensor[rank], b: MyTensor[rank]) -> MyTensor[rank]:
    # ...
In this case, the rank value needs to be specified in a way that can be determined at compilation time, such as a literal or expression.
When you compile a program that uses this code, the compiler produces a unique version of the function for each unique rank value used in the program, with rank treated as a constant within each specialized version.
This usage of "parameter" is probably different from what you're used to from other languages, where "parameter" and "argument" are often used interchangeably. In Mojo, "parameter" and "parameter expression" refer to compile-time values, and "argument" and "expression" refer to run-time values.
By default, both arguments and parameters can be specified either by position or by keyword. These forms can also be mixed in the same function call.
# positional
x = add(5, 7)      # Positionally, a=5 and b=7
# keyword
y = add(b=3, a=9)
# mixed
z = add(5, b=7)    # Positionally, a=5

def and fn comparison​
Defining a function using def and fn have much in common. They both have the following requirements:
* You must declare the type of each function parameter and argument.
* If a function doesn't return a value, you can either omit the return type or declare None as the return type.
# The following function definitions are equivalent

def greet(name: String):
  print("Hello," name)

def greet(name: String) -> None:
  print("Hello," name)
* If the function returns a value, you must either declare the return type using the -> type syntax or provide a named result in the argument list.
# The following function definitions are equivalent

def incr(a: Int) -> Int:
  return a + 1

def incr(a: Int, out b: Int):
  b = a + 1

Where def and fn differ is error handling.
* The compiler doesn't allow a function declared with fn to raise an error condition unless it explicitly includes a raises declaration. In contrast, the compiler assumes that all functions declared with def might raise an error.
As far as a function caller is concerned, there is no difference between invoking a function declared with def vs a function declared with fn. You could reimplement a def function as an fn function without making any changes to code that calls the function.
Function arguments​
The rules for arguments described in this section apply to both def and fn functions.
Functions with / and * in the argument list
You might see the following characters in place of arguments: slash (/) and/or star (*). For example:
def myfunc(pos_only, /, pos_or_keyword, *, keyword_only):
Arguments before the / can be passed only by position. Arguments after the * can be passed only by keyword.
You may also see argument names prefixed with one or two stars (*):
def myfunc2(*names, **attributes):
An argument name prefixed by a single star character, like *names identifies a variadic argument, while an argument name prefixed with a double star, like **attributes identifies a variadic keyword-only argument.
Optional arguments​
An optional argument is one that includes a default value, such as the exp argument here:
fn my_pow(base: Int, exp: Int = 2) -> Int:
    return base ** exp

fn use_defaults():
    # Uses the default value for `exp`
    var z = my_pow(3)
    print(z)
However, you can't define a default value for an argument that's declared with the mut argument convention.
Any optional arguments must appear after any required arguments. Keyword-only arguments, discussed later, can also be either required or optional.
Keyword arguments​
You can also use keyword arguments when calling a function. Keyword arguments are specified using the format argument_name = argument_value. You can pass keyword arguments in any order:
fn my_pow(base: Int, exp: Int = 2) -> Int:
    return base ** exp

fn use_keywords():
    # Uses keyword argument names (with order reversed)
    var z = my_pow(exp=3, base=2)
    print(z)
Variadic arguments​
Variadic arguments let a function accept a variable number of arguments. To define a function that takes a variadic argument, use the variadic argument syntax *argument_name:
fn sum(*values: Int) -> Int:
  var sum: Int = 0
  for value in values:
    sum = sum + value
  return sum
The variadic argument values here is a placeholder that accepts any number of passed positional arguments.
You can define zero or more arguments before the variadic argument. When calling the function, any remaining positional arguments are assigned to the variadic argument, so any arguments declared after the variadic argument can only be specified by keyword (see Positional-only and keyword-only arguments).
Variadic arguments can be divided into two categories:
* Homogeneous variadic arguments, where all of the passed arguments are the same type-all Int, or all String, for example.
* Heterogeneous variadic arguments, which can accept a set of different argument types.
The following sections describe how to work with homogeneous and heterogeneous variadic arguments.
Variadic parameters
Mojo also supports variadic parameters, but with some limitations-for details see variadic parameters.
Homogeneous variadic arguments​
When defining a homogeneous variadic argument (all arguments must be the same type), use *argument_name: argument_type:
def greet(*names: String):
    ...
Inside the function body, the variadic argument is available as an iterable list for ease of use. Currently there are some differences in handling the list depending on whether the arguments are register-passable types (such as Int) or memory-only types (such as String).
TODO
We hope to remove these differences in the future.
Register-passable types, such as Int, are available as a VariadicList type. As shown in the previous example, you can iterate over the values using a for..in loop.
fn sum(*values: Int) -> Int:
  var sum: Int = 0
  for value in values:
    sum = sum+value
  return sum
Memory-only types, such as String, are available as a VariadicListMem. Iterating over this list directly with a for..in loop currently produces a reference to the element, which can be mutable with a mut variadic list. Use the ref binding pattern to capture a mutable reference if you want to mutate the elements of the list:
def make_worldly(mut *strs: String):
    for ref i in strs:
        i += " world"
You can also directly index the list with integers as well:
fn make_worldly(mut *strs: String):
    for i in range(len(strs)):
        strs[i] += " world"
Heterogeneous variadic arguments​
Implementing heterogeneous variadic arguments (each argument type may be different) is somewhat more complicated than homogeneous variadic arguments. To handle multiple argument types, the function must be generic, which requires using traits and parameters. So the syntax may look a little unfamiliar if you haven't worked with those features.
The signature for a function with a heterogeneous variadic argument looks like this:
def count_many_things[*ArgTypes: Intable](*args: *ArgTypes):
    ...
The parameter list, [*ArgTypes: Intable] specifies that the function takes an ArgTypes parameter, which is a list of types, all of which conform to the Intable trait. The asterisk in *ArgTypes indicates that ArgTypes is a variadic type parameter (a list of types).
The argument list, (*args: *ArgTypes) has the familiar *args for the variadic argument, but instead of a single type, its type is defined as the variadic type list *ArgTypes. The asterisk in *args indicates a variadic argument, and the asterisk in *ArgTypes refers to the variadic type parameter.
This means that each argument in args has a corresponding type in ArgTypes, so args[n] is of type ArgTypes[n].
Inside the function, args becomes a VariadicPack because the syntax *args: *ArgTypes creates a heterogeneous variadic argument. That means each element in args can be a different type that requires a different amount of memory. To iterate through the VariadicPack, the compiler must know each element's type (its memory size), so you must use a parametric for loop:
fn count_many_things[*ArgTypes: Intable](*args: *ArgTypes) -> Int:
    var total = 0

    @parameter
    for i in range(args.__len__()):
        total += Int(args[i])

    return total

def main():
    print(count_many_things(5, 11.7, 12))
28
Notice that when calling count_many_things(), you don't actually pass in a list of argument types. You only need to pass in the arguments, and Mojo generates the ArgTypes list itself.
Variadic keyword arguments​
Mojo functions also support variadic keyword arguments (**kwargs). Variadic keyword arguments allow the user to pass an arbitrary number of keyword arguments. To define a function that takes a variadic keyword argument, use the variadic keyword argument syntax **kw_argument_name:
fn print_nicely(**kwargs: Int) raises:
  for key in kwargs.keys():
      print(key, "=", kwargs[key])

 # prints:
 # `a = 7`
 # `y = 8`
print_nicely(a=7, y=8)
In this example, the argument name kwargs is a placeholder that accepts any number of keyword arguments. Inside the body of the function, you can access the arguments as a dictionary of keywords and argument values (specifically, an instance of OwnedKwargsDict).
There are currently a few limitations:
* Variadic keyword arguments are always implicitly treated as if they were declared with the owned argument convention, and can't be declared otherwise:
# Not supported yet.
fn read_var_kwargs(read **kwargs: Int): ...
* All the variadic keyword arguments must have the same type, and this determines the type of the argument dictionary. For example, if the argument is **kwargs: Float64 then the argument dictionary will be a OwnedKwargsDict[Float64].
* The argument type must conform to both the Movable and Copyable traits.
* Dictionary unpacking is not supported yet:
fn takes_dict(d: Dict[String, Int]):
  print_nicely(**d)  # Not supported yet.
* Variadic keyword parameters are not supported yet:
# Not supported yet.
fn var_kwparams[**kwparams: Int](): ...
Positional-only and keyword-only arguments​
When defining a function, you can restrict some arguments so that they can be passed only as positional arguments, or they can be passed only as keyword arguments.
To define positional-only arguments, add a slash character (/) to the argument list. Any arguments before the / are positional-only: they can't be passed as keyword arguments. For example:
fn min(a: Int, b: Int, /) -> Int:
    return a if a < b else b
This min() function can be called with min(1, 2) but can't be called using keywords, like min(a=1, b=2).
There are several reasons you might want to write a function with positional-only arguments:
* The argument names aren't meaningful for the caller.
* You want the freedom to change the argument names later on without breaking backward compatibility.
For example, in the min() function, the argument names don't add any real information, and there's no reason to specify arguments by keyword.
Keyword-only arguments are the inverse of positional-only arguments: they can be specified only by keyword. If a function accepts variadic arguments, any arguments defined after the variadic arguments are treated as keyword-only. For example:
fn sort(*values: Float64, ascending: Bool = True): ...
In this example, the user can pass any number of Float64 values, optionally followed by the keyword ascending argument:
var a = sort(1.1, 6.5, 4.3, ascending=False)
If the function doesn't accept variadic arguments, you can add a single star (*) to the argument list to separate the keyword-only arguments:
fn kw_only_args(a1: Int, a2: Int, *, double: Bool) -> Int:
    var product = a1 * a2
    if double:
        return product * 2
    else:
        return product
Keyword-only arguments often have default values, but this is not required. If a keyword-only argument doesn't have a default value, it is a required keyword-only argument. It must be specified, and it must be specified by keyword.
Any required keyword-only arguments must appear in the signature before any optional keyword-only arguments. That is, arguments appear in the following sequence a function signature:
* Required positional arguments.
* Optional positional arguments.
* Variadic arguments.
* Required keyword-only arguments.
* Optional keyword-only arguments.
* Variadic keyword arguments.
Overloaded functions​
All function declarations must specify argument types, so if you want a function to work with different data types, you need to implement separate versions of the function that each specify different argument types. This is called "overloading" a function.
For example, here's an overloaded add() function that can accept either Int or String types:
fn add(x: Int, y: Int) -> Int:
    return x + y

fn add(x: String, y: String) -> String:
    return x + y
If you pass anything other than Int or String to the add() function, you'll get a compiler error. That is, unless Int or String can implicitly cast the type into their own type. For example, String includes an overloaded version of its constructor (__init__()) that supports implicit conversion from a StringLiteral value. Thus, you can also pass a StringLiteral to a function that expects a String.
When resolving an overloaded function call, the Mojo compiler tries each candidate function and uses the one that works (if only one version works), or it picks the closest match (if it can determine a close match), or it reports that the call is ambiguous (if it can't figure out which one to pick). For details on how Mojo picks the best candidate, see Overload resolution.
If the compiler can't figure out which function to use, you can resolve the ambiguity by explicitly casting your value to a supported argument type. For example, the following code calls the overloaded foo() function, but both implementations accept an argument that supports implicit conversion from StringLiteral. So, the call to foo(string) is ambiguous and creates a compiler error. You can fix this by casting the value to the type you really want:
struct MyString:
    @implicit
    fn __init__(out self, string: StringLiteral):
        pass

fn foo(name: String):
    print("String")

fn foo(name: MyString):
    print("MyString")

fn call_foo():
    alias string = "Hello"
    # foo(string) # error: ambiguous call to 'foo' ... This call is ambiguous because two `foo` functions match it
    foo(MyString(string))
Overloading also works with combinations of both fn and def function declarations.
Overload resolution​
When resolving an overloaded function, Mojo does not consider the return type or other contextual information at the call site-it considers only parameter and argument types and whether the functions are instance methods or static methods.
The overload resolution logic filters for candidates according to the following rules, in order of precedence:
1. Candidates requiring the smallest number of implicit conversions (in both arguments and parameters).
2. Candidates without variadic arguments.
3. Candidates without variadic parameters.
4. Candidates with the shortest parameter signature.
5. Non-@staticmethod candidates (over @staticmethod ones, if available).
If there is more than one candidate after applying these rules, the overload resolution fails. For example:
@register_passable("trivial")
struct MyInt:
    """A type that is implicitly convertible to `Int`."""
    var value: Int

    @implicit
    fn __init__(out self, _a: Int):
        self.value = _a

fn foo[x: MyInt, a: Int]():
    print("foo[x: MyInt, a: Int]()")

fn foo[x: MyInt, y: MyInt]():
    print("foo[x: MyInt, y: MyInt]()")

fn bar[a: Int](b: Int):
    print("bar[a: Int](b: Int)")

fn bar[a: Int](*b: Int):
    print("bar[a: Int](*b: Int)")

fn bar[*a: Int](b: Int):
    print("bar[*a: Int](b: Int)")

fn parameter_overloads[a: Int, b: Int, x: MyInt]():
    # `foo[x: MyInt, a: Int]()` is called because it requires no implicit
    # conversions, whereas `foo[x: MyInt, y: MyInt]()` requires one.
    foo[x, a]()

    # `bar[a: Int](b: Int)` is called because it does not have variadic
    # arguments or parameters.
    bar[a](b)

    # `bar[*a: Int](b: Int)` is called because it has variadic parameters.
    bar[a, a, a](b)

parameter_overloads[1, 2, MyInt(3)]()

struct MyStruct:
    fn __init__(out self):
        pass

    fn foo(mut self):
        print("calling instance method")

    @staticmethod
    fn foo():
        print("calling static method")

fn test_static_overload():
    var a = MyStruct()
    # `foo(mut self)` takes precedence over a static method.
    a.foo()
foo[x: MyInt, a: Int]()
bar[a: Int](b: Int)
bar[*a: Int](b: Int)
Return values​
Return value types are declared in the signature using the -> type syntax. Values are passed using the return keyword, which ends the function and returns the identified value (if any) to the caller.
def get_greeting() -> String:
    return "Hello"
By default, the value is returned to the caller as an owned value. As with arguments, a return value may be implicitly converted to the named return type. For example, the previous example calls return with a string literal, "Hello", which is implicitly converted to a String.
Returning a reference
A function can also return a mutable or immutable reference using a ref return value.
Named results​
Named function results allow a function to return a value that can't be moved or copied. Named result syntax lets you specify a named, uninitialized variable to return to the caller using the out argument convention:
def get_name_tag(var name: String, out name_tag: NameTag):
    name_tag = NameTag(name^)
The out argument convention identifies an uninitialized variable that the function must initialize. (This is the same as the out convention used in struct constructors.) The out argument for a named result can appear anywhere in the argument list, but by convention, it should be the last argument in the list.
A function can declare only one return value, whether it's declared using an out argument or using the standard -> type syntax.
A function with a named result argument doesn't need to include an explicit return statement, as shown above. If the function terminates without a return, or at a return statement with no value, the value of the out argument is returned to the caller. If it includes a return statement with a value, that value is returned to the caller, as usual.
The fact that a function uses a named result is transparent to the caller. That is, these two signatures are interchangeable to the caller:
def get_name_tag(var name: String) -> NameTag:
    ...
def get_name_tag(var name: String, out name_tag: NameTag):
    ...
In both cases, the call looks like this:
tag = get_name_tag("Judith")
Because the return value is assigned to this special out variable, it doesn't need to be moved or copied when it's returned to the caller. This means that you can create a function that returns a type that can't be moved or copied, and which takes several steps to initialize:
struct ImmovableObject:
    var name: String

    fn __init__(out self, var name: String):
        self.name = name^

def create_immovable_object(var name: String, out obj: ImmovableObject):
    obj = ImmovableObject(name^)
    obj.name += "!"
    # obj is implicitly returned

def main():
    my_obj = create_immovable_object("Blob")
By contrast, the following function with a standard return value doesn't work:
def create_immovable_object2(var name: String) -> ImmovableObject:
    obj = ImmovableObject(name^)
    obj.name += "!"
    return obj^ # Error: ImmovableObject is not copyable or movable
Because create_immovable_object2 uses a local variable to store the object while it's under construction, the return call requires it to be either moved or copied to the callee. This isn't an issue if the newly-created value is returned immediately:
def create_immovable_object3(var name: String) -> ImmovableObject:
    return ImmovableObject(name^) # OK
Raising and non-raising functions​
By default, when a function raises an error, the function terminates immediately and the error propagates to the calling function. If the calling function doesn't handle the error, it continues to propagate up the call stack.
def raises_error():
    raise Error("There was an error.")
The Mojo compiler always treats a function declared with def as a raising function, even if the body of the function doesn't contain any code that could raise an error.
Functions declared with fn without the raises keyword are non-raising functions-that is, they are not allowed to propagate an error to the calling function. If a non-raising function calls a raising function, it must handle any possible errors.
# This function will not compile
fn unhandled_error():
    raises_error()   # Error: can't call raising function in a non-raising context

# Explicitly handle the error
fn handle_error():
    try:
        raises_error()
    except e:
        print("Handled an error:", e)

# Explicitly propagate the error
fn propagate_error() raises:
    raises_error()

If you're writing code that you expect to use widely or distribute as a package, you may want to use fn functions for APIs that don't raise errors to limit the number of places users need to add unnecessary error handling code. For some extremely performance-sensitive code, it may be preferable to avoid run-time error-handling.
Variables
A variable is a name that holds a value or object. All variables in Mojo are mutable-their value can be changed. (If you want to define a constant value that can't change at runtime, see the alias keyword.)
When you declare a variable in Mojo, you allocate a logical storage location, and bind a name to that storage location.
var greeting: String = "Hello World"
The var statement above does three things:
* It declares a logical storage location (in this case, a storage location sized to hold a String struct).
* It binds the name greeting to this logical storage location.
* It initializes the storage space with a newly-created String value, with the text, "Hello World". The new value is owned by the variable. No other variable can own this value unless we intentionally transfer it.
Variable declarations​
Mojo has two ways to declare a variable:
* Explicitly-declared variables are created with the var keyword.
var a = 5
var b: Float64 = 3.14
var c: String
* Implicitly-declared variables are created the first time the variable is used, either with an assignment statement, or with a type annotation:
a = 5
b: Float64 = 3.14
c: String
Both types of variables are strongly typed-the type is either set explicitly with a type annotation or implicitly when the variable is first initialized with a value.
Either way, the variable receives a type when it's created, and the type never changes. So you can't assign a variable a value of a different type:
count = 8 # count is type Int
count = "Nine?" # Error: can't implicitly convert 'StringLiteral' to 'Int'
Some types support implicit conversions from other types. For example, an integer value can implicitly convert to a floating-point value:
var temperature: Float64 = 99
print(temperature)
99.0
In this example, the temperature variable is explicitly typed as Float64, but assigned an integer value, so the value is implicitly converted to a Float64.
Implicitly-declared variables​
You can create a variable with just a name and a value. For example:
name = "Sam"
user_id = 0
Implicitly-declared variables are strongly typed: they take the type from the first value assigned to them. For example, the user_id variable above is type Int, while the name variable is type String. You can't assign a string to user_id or an integer to name.
You can also use a type annotation with an implicitly-declared variable, either as part of an assignment statement, or on its own:
name: String = "Sam"
user_id: Int
Here the user_id variable has a type, but is uninitialized.
Implicitly-declared variables are scoped at the function level. You create an implicitly-declared variable the first time you assign a value to a given name inside a function. Any subsequent references to that name inside the function refer to the same variable.
Explicitly-declared variables​
You can declare a variable with the var keyword. For example:
var name = "Sam"
var user_id: Int
The name variable is initialized to the string "Sam". The user_id variable is uninitialized, but it has a declared type, Int for an integer value.
Since variables are strongly typed, you can't assign a variable a value of a different type, unless those types can be implicitly converted. For example, this code will not compile:
var user_id: Int = "Sam"
Explicitly-declared variables follow lexical scoping, unlike implicitly-declared variables.
Type annotations​
Although Mojo can infer a variable type from the first value assigned to a variable, it also supports static type annotations on variables. Type annotations provide a more explicit way of specifying the variable's type.
To specify the type for a variable, add a colon followed by the type name:
var name: String = get_name()
# Or
name: String = get_name()
This makes it clear that name is type String, without knowing what the get_name() function returns. The get_name() function may return a String, or a value that's implicitly convertible to a String.
If a type has a constructor with just one argument, you can initialize it in two ways:
var name1: String = "Sam"
var name2 = String("Sam")
var name3 = "Sam"
All of these lines invoke the same constructor to create a String from a StringLiteral.
Late initialization​
Using type annotations allows for late initialization. For example, notice here that the z variable is first declared with just a type, and the value is assigned later:
fn my_function(x: Int):
    var z: Float32
    if x != 0:
        z = 1.0
    else:
        z = foo()
    print(z)

fn foo() -> Float32:
    return 3.14
If you try to pass an uninitialized variable to a function or use it on the right-hand side of an assignment statement, compilation fails.
var z: Float32
var y = z # Error: use of uninitialized value 'z'
Late initialization works only if the variable is declared with a type.
Implicit type conversion​
Some types include built-in type conversion (type casting) from one type into its own type. For example, if you assign an integer to a variable that has a floating-point type, it converts the value instead of giving a compiler error:
var number: Float64 = Int(1)
print(number)
1.0
As shown above, value assignment can be converted into a constructor call if the target type has a constructor that meets the following criteria:
* It's decorated with the @implicit decorator.
* It takes a single required argument that matches the value being assigned.
So, this code uses the Float64 constructor that takes an integer: __init__(out self, value: Int).
In general, implicit conversions should only be supported where the conversion is lossless.
Implicit conversion follows the logic of overloaded functions. If the destination type has a viable implicit conversion constructor for the source type, it can be invoked for implicit conversion.
So assigning an integer to a Float64 variable is exactly the same as this:
var number = Float64(1)
Similarly, if you call a function that requires an argument of a certain type (such as Float64), you can pass in any value as long as that value type can implicitly convert to the required type (using one of the type's overloaded constructors).
For example, you can pass an Int to a function that expects a Float64, because Float64 includes an implicit conversion constructor that takes an Int:
fn take_float(value: Float64):
    print(value)

fn pass_integer():
    var value: Int = 1
    take_float(value)
Variable scopes​
Variables declared with var are bound by lexical scoping. This means that nested code blocks can read and modify variables defined in an outer scope. But an outer scope cannot read variables defined in an inner scope at all.
For example, the if code block shown here creates an inner scope where outer variables are accessible to read/write, but any new variables do not live beyond the scope of the if block:
def lexical_scopes():
    var num = 1
    var dig = 1
    if num == 1:
        print("num:", num)  # Reads the outer-scope "num"
        var num = 2         # Creates new inner-scope "num"
        print("num:", num)  # Reads the inner-scope "num"
        dig = 2             # Updates the outer-scope "dig"
    print("num:", num)      # Reads the outer-scope "num"
    print("dig:", dig)      # Reads the outer-scope "dig"
num: 1
num: 2
num: 1
dig: 2
Note that the var statement inside the if creates a new variable with the same name as the outer variable. This prevents the inner loop from accessing the outer num variable. (This is called "variable shadowing," where the inner scope variable hides or "shadows" a variable from an outer scope.)
The lifetime of the inner num ends exactly where the if code block ends, because that's the scope in which the variable was defined.
This is in contrast to implicitly-declared variables (those without the var keyword), which use function-level scoping (consistent with Python variable behavior). That means, when you change the value of an implicitly-declared variable inside the if block, it actually changes the value for the entire function.
For example, here's the same code but without the var declarations:
def function_scopes():
    num = 1
    if num == 1:
        print(num)   # Reads the function-scope "num"
        num = 2      # Updates the function-scope variable
        print(num)   # Reads the function-scope "num"
    print(num)       # Reads the function-scope "num"
1
2
2
Now, the last print() function sees the updated num value from the inner scope, because implicitly-declared variables (Python-style variables) use function-level scope (instead of lexical scope).
Copying and moving values​
Remember that a variable owns its value, and only one variable can own a given value at a time. To take this one step further, you can think of an assignment statement as assigning ownership of a value to a variable:
owning_variable = "Owned value"
This means the value on the right-hand side of the assignment statement must be transferrable to the new variable. Here's an example where that doesn't work:
first = [1, 2, 3]
second = first  # error: 'List[Int]' is not implicitly copyable because it does
                 # not conform to 'ImplicitlyCopyable'
The first assignment is no problem: the expression [1, 2, 3] creates a new List value that doesn't belong to any variable, so its ownership can be transferred directly to the first variable.
But the second assignment causes an error. Since the List is owned by the first variable, it can't simply be transferred to the second variable without an explicit signal from the user. Does the user want to transfer the value from first to second? Or create a copy of the original value?
These choices depend on some features of the type of the values involved: specifically, if the values are movable, copyable, or implicitly copyable.
* A copyable type can be copied explicitly, by calling its copy() method.
second = first.copy()
This leaves first unchanged and assigns second its own, uniquely owned copy of the list.
* An implicitly copyable type can be copied without an explicit signal from the user.
one_value = 15
another_value = one_value  # implicit copy
Here one_value is unchanged, and another_value gets a copy of the value.
Implicitly copyable types are generally simple value types like Int, Float64, and Bool which can be copied trivially.
* The ownership of a value can be be explicitly transferred from one variable to another by appending the transfer sigil (^) after the value to transfer:
second = first^
This moves the value to second, and leaves first uninitialized.
In many cases, this ownership transfer also involves moving the value from one memory location to another, which requires the value to be either movable or copyable.
Reference bindings​
Some APIs return references to values owned elsewhere. References can be useful to avoid copying values. For example, when you retrieve a value from a collection, the collection returns a reference, instead of a copy.
animals: List[String] = ["Cats", "Dogs, "Zebras"]
print(animals[2])  # Prints "Zebras", does not copy the value.
But if you assign a reference to a variable, it creates a copy (if the value is implicitly copyable) or produces an error (if it isn't).
items = [99, 77, 33, 12]
item = items[1]  # item is a copy of items[1]
item += 1  # increments item
print(items[1])  # prints 77
To hold on to a reference, use the ref keyword to create a reference binding:
ref item_ref = items[1]  # item_ref is a reference to item[1]
item_ref += 1  # increments items[1]
print(items[1])  # prints 78
Here the name item_ref is bound to the reference to items[1]. All reads and writes to item_ref go to the referenced item.
Reference bindings can also be used when iterating through collections with for loops.
Once a reference binding is assigned, it can't be re-bound to a different location. For example:
ref item_ref = items[2]  # error: invalid redefinition of item_ref
Types
All values in Mojo have an associated data type. Most of the types are nominal types, defined by a struct. These types are nominal (or "named") because type equality is determined by the type's name, not its structure.
There are some types that aren't defined as structs:
* Functions are typed based on their signatures.
* NoneType is a type with one instance, the None object, which is used to signal "no value."
Mojo comes with a standard library that provides a number of useful types and utility functions. These standard types aren't privileged. Each of the standard library types is defined just like user-defined types-even basic types like Int and String. But these standard library types are the building blocks you'll use for most Mojo programs.
The most common types are built-in types, which are always available and don't need to be imported. These include types for numeric values, strings, boolean values, and others.
The standard library also includes many more types that you can import as needed, including collection types, utilities for interacting with the filesystem and getting system information, and so on.
Numeric types​
Mojo's most basic numeric type is Int, which represents a signed integer of the largest size supported by the system-typically 64 bits or 32 bits.
Mojo also has built-in types for integer, unsigned integer, and floating-point values of various precisions:
Type name
Description
Int8
8-bit signed integer
UInt8
8-bit unsigned integer
Int16
16-bit signed integer
UInt16
16-bit unsigned integer
Int32
32-bit signed integer
UInt32
32-bit unsigned integer
Int64
64-bit signed integer
UInt64
64-bit unsigned integer
Int128
128-bit signed integer
UInt128
128-bit unsigned integer
Int256
256-bit signed integer
UInt256
256-bit unsigned integer
Float16
16-bit floating point number (IEEE 754-2008 binary16)
Float32
32-bit floating point number (IEEE 754-2008 binary32)
Float64
64-bit floating point number (IEEE 754-2008 binary64)
Table 1. Numeric types with specific precision 
The types in Table 1 are actually all aliases to a single type, SIMD, which is discussed later.
All of the numeric types support the usual numeric and bitwise operators. The math module provides a number of additional math functions.
You may wonder when to use Int and when to use the other integer types. In general, Int is a good safe default when you need an integer type and you don't require a specific bit width. Using Int as the default integer type for APIs makes APIs more consistent and predictable.
Signed and unsigned integers​
Mojo supports both signed (Int) and unsigned (UInt) integers. You can use the general Int or UInt types when you do not require a specific bit width. Note that any alias to a fixed-precision type will be of type SIMD.
You might prefer to use unsigned integers over signed integers in conditions where you don't need negative numbers, are not writing for a public API, or need additional range.
Mojo's UInt type represents an unsigned integer of the word size of the CPU, which is 64 bits on 64-bit CPUs and 32 bits on 32-bit CPUs. If you wish to use a fixed size unsigned integer, you can use UInt8, UInt16, UInt32, or UInt64, which are aliases to the SIMD type.
Signed and unsigned integers of the same bit width can represent the same number of values, but have different ranges. For example, an Int8 can represent 256 values ranging from -128 to 127. A UInt8 can also represent 256 values, but represents a range of 0 to 255.
Signed and unsigned integers also have different overflow behavior. When a signed integer overflows outside the range of values that its type can represent, the value overflows to negative numbers. For example, adding 1 to var si: Int8 = 127 results in -128.
When an unsigned integer overflows outside the range of values that its type can represent, the value overflows to zero. So, adding 1 to var ui: UInt8 = 255 is equal to 0.
Floating-point numbers​
Floating-point types represent real numbers. Because not all real numbers can be expressed in a finite number of bits, floating-point numbers can't represent every value exactly.
The floating-point types listed in Table 1-Float64, Float32, and Float16-follow the IEEE 754-2008 standard for representing floating-point values. Each type includes a sign bit, one set of bits representing an exponent, and another set representing the fraction or mantissa. Table 2 shows how each of these types are represented in memory.
Type name
Sign
Exponent
Mantissa
Float64
1 bit
11 bits
52 bits
Float32
1 bit
8 bits
23 bits
Float16
1 bit
5 bits
10 bits
Table 2. Details of floating-point types 
Numbers with exponent values of all ones or all zeros represent special values, allowing floating-point numbers to represent infinity, negative infinity, signed zeros, and not-a-number (NaN). For more details on how numbers are represented, see IEEE 754 on Wikipedia.
A few things to note with floating-point values:
* Rounding errors. Rounding may produce unexpected results. For example, 1/3 can't be represented exactly in these floating-point formats. The more operations you perform with floating-point numbers, the more the rounding errors accumulate.
* Space between consecutive numbers. The space between consecutive numbers is variable across the range of a floating-point number format. For numbers close to zero, the distance between consecutive numbers is very small. For large positive and negative numbers, the space between consecutive numbers is greater than 1, so it may not be possible to represent consecutive integers.
Because the values are approximate, it is rarely useful to compare them with the equality operator (==). Consider the following example:
var big_num = 1.0e16
var bigger_num = big_num+1.0
print(big_num == bigger_num)
True
Comparison operators (< >= and so on) work with floating point numbers. You can also use the math.isclose() function to compare whether two floating-point numbers are equal within a specified tolerance.
Numeric literals​
In addition to these numeric types, the standard libraries provides integer and floating-point literal types, IntLiteral and FloatLiteral.
These literal types are used at compile time to represent literal numbers that appear in the code. In general, you should never instantiate these types yourself.
Table 3 summarizes the literal formats you can use to represent numbers.
Format
Examples
Notes
Integer literal
1760
Integer literal, in decimal format.
Hexadecimal literal
0xaa, 0xFF
Integer literal, in hexadecimal format.
Hex digits are case-insensitive.
Octal literal
0o77
Integer literal, in octal format.
Binary literal
0b0111
Integer literal, in binary format.
Floating-point literal
3.14, 1.2e9
Floating-point literal.
Must include the decimal point to be interpreted as floating-point.
Table 3. Numeric literal formats 
At compile time, the literal types are arbitrary-precision (also called infinite-precision) values, so the compiler can perform compile-time calculations without overflow or rounding errors.
At runtime the values are converted to finite-precision types-Int for integer values, and Float64 for floating-point values. (This process of converting a value that can only exist at compile time into a runtime value is called materialization.)
The following code sample shows the difference between an arbitrary-precision calculation and the same calculation done using Float64 values at runtime, which suffers from rounding errors.
var arbitrary_precision = 3.0 * (4.0 / 3.0 - 1.0)
# use a variable to force the following calculation to occur at runtime
var three = 3.0
var finite_precision = three * (4.0 / three - 1.0)
print(arbitrary_precision, finite_precision)
1.0 0.99999999999999978
SIMD and DType​
To support high-performance numeric processing, Mojo uses the SIMD type as the basis for its numeric types. SIMD (single instruction, multiple data) is a processor technology that allows you to perform an operation on an entire set of operands at once. Mojo's SIMD type abstracts SIMD operations. A SIMD value represents a SIMD vector-that is, a fixed-size array of values that can fit into a processor's register. SIMD vectors are defined by two parameters:
* A DType value, defining the data type in the vector (for example, 32-bit floating-point numbers).
* The number of elements in the vector, which must be a power of two.
For example, you can define a vector of four Float32 values like this:
var vec = SIMD[DType.float32, 4](3.0, 2.0, 2.0, 1.0)
Math operations on SIMD values are applied elementwise, on each individual element in the vector. For example:
var vec1 = SIMD[DType.int8, 4](2, 3, 5, 7)
var vec2 = SIMD[DType.int8, 4](1, 2, 3, 4)
var product = vec1 * vec2
print(product)
[2, 6, 15, 28]
Scalar values​
The SIMD module defines several type aliases that are shorthand for different types of SIMD vectors. In particular, the Scalar type is just a SIMD vector with a single element. The numeric types listed in Table 1, like Int8 and Float32 are actually type aliases for different types of scalar values:
alias Scalar = SIMD[size=1]
alias Int8 = Scalar[DType.int8]
alias Float32 = Scalar[DType.float32]
This may seem a little confusing at first, but it means that whether you're working with a single Float32 value or a vector of float32 values, the math operations go through exactly the same code path.
The DType type​
The DType struct describes the different data types that a SIMD vector can hold, and defines a number of utility functions for operating on those data types. The DType struct defines a set of aliases that act as identifiers for the different data types, like DType.int8 and DType.float32. You use these aliases when declaring a SIMD vector:
var v: SIMD[DType.float64, 16]
Note that DType.float64 isn't a type, it's a value that describes a data type. You can't create a variable with the type DType.float64. You can create a variable with the type SIMD[DType.float64, 1] (or Float64, which is the same thing).
from utils.numerics import max_finite, min_finite

def describeDType[dtype: DType]():
    print(dtype, "is floating point:", dtype.is_floating_point())
    print(dtype, "is integral:", dtype.is_integral())
    print("Min/max finite values for", dtype)
    print(min_finite[dtype](), max_finite[dtype]())

describeDType[DType.float32]()
float32 is floating point: True
float32 is integral: False
Min/max finite values for float32
-3.4028234663852886e+38 3.4028234663852886e+38
There are several other data types in the standard library that also use the DType abstraction.
Numeric type conversion​
Constructors and implicit conversion documents the circumstances in which Mojo automatically converts a value from one type to another. Importantly, numeric operators don't automatically narrow or widen operands to a common type.
You can explicitly convert a SIMD value to a different SIMD type either by invoking its cast() method or by passing it as an argument to the constructor of the target type. For example:
simd1 = SIMD[DType.float32, 4](2.2, 3.3, 4.4, 5.5)
simd2 = SIMD[DType.int16, 4](-1, 2, -3, 4)
simd3 = simd1 * simd2.cast[DType.float32]()  # Convert with cast() method
print("simd3:", simd3)
simd4 = simd2 + SIMD[DType.int16, 4](simd1)  # Convert with SIMD constructor
print("simd4:", simd4)
simd3: [-2.2, 6.6, -13.200001, 22.0]
simd4: [1, 5, 1, 9]
You can convert a Scalar value by passing it as an argument to the constructor of the target type. For example:
var my_int: Int16 = 12                 # SIMD[DType.int16, 1]
var my_float: Float32 = 0.75           # SIMD[DType.float32, 1]
result = Float32(my_int) * my_float    # Result is SIMD[DType.float32, 1]
print("Result:", result)
Result: 9.0
You can convert a scalar value of any numeric type to Int by passing the value to the Int() constructor method. Additionally, you can pass an instance of any struct that implements the Intable trait or IntableRaising trait to the Int() constructor to convert that instance to an Int.
You can convert an Int or IntLiteral value to the UInt type by passing the value to the UInt() constructor. You can't convert other numeric types to UInt directly, though you can first convert them to Int and then to UInt.
Strings​
Mojo's String type represents a mutable string. (For Python programmers, note that this is different from Python's standard string, which is immutable.) Strings support a variety of operators and common methods.
var s: String = "Testing"
s += " Mojo strings"
print(s)
Testing Mojo strings
Most standard library types conform to the Stringable trait, which represents a type that can be converted to a string. Use String(value) to explicitly convert a value to a string:
var s = "Items in list: " + String(5)
print(s)
Items in list: 5
Or use String.write to take variadic Stringable types, so you don't have to call String() on each value:
var s = String("Items in list: ", 5)
print(s)
Items in list: 5
String literals​
As with numeric types, the standard library includes a string literal type used to represent literal strings in the program source. String literals are enclosed in either single or double quotes.
Adjacent literals are concatenated together, so you can define a long string using a series of literals broken up over several lines:
alias s = "A very long string which is "
        "broken into two literals for legibility."
To define a multi-line string, enclose the literal in three single or double quotes:
alias s = """
Multi-line string literals let you
enter long blocks of text, including
newlines."""
Note that the triple double quote form is also used for API documentation strings.
A StringLiteral will materialize to a String when used at run-time:
alias param = "foo"        # type = StringLiteral
var runtime_value = "bar"  # type = String
var runtime_value2 = param # type = String
Booleans​
Mojo's Bool type represents a boolean value. It can take one of two values, True or False. You can negate a boolean value using the not operator.
var conditionA = False
var conditionB: Bool
conditionB = not conditionA
print(conditionA, conditionB)
False True
Many types have a boolean representation. Any type that implements the Boolable trait has a boolean representation. As a general principle, collections evaluate as True if they contain any elements, False if they are empty; strings evaluate as True if they have a non-zero length.
Tuples​
Mojo's Tuple type represents an immutable tuple consisting of zero or more values, separated by commas. Tuples can consist of multiple types and you can index into tuples in multiple ways.
# Tuples are immutable and can hold multiple types
example_tuple = Tuple[Int, String](1, "Example")

# Assign multiple variables at once
x, y = example_tuple
print(x, y)

# Get individual values with an index
s = example_tuple[1]
print(s)
1 Example
Example
You can also create a tuple without explicit typing.
example_tuple = (1, "Example")
s = example_tuple[1]
print(s)
Example
When defining a function, you can explicitly declare the type of tuple elements in one of two ways:
def return_tuple_1() -> Tuple[Int, Int]:
    return Tuple[Int, Int](1, 1)

def return_tuple_2() -> (Int, Int):
    return (2, 2)
Collection types​
The Mojo standard library also includes a set of basic collection types that can be used to build more complex data structures:
* List, a dynamically-sized array of items.
* Dict, an associative array of key-value pairs.
* Set, an unordered collection of unique items.
* Optional represents a value that may or may not be present.
The collection types are generic types: while a given collection can only hold a specific type of value (such as Int or Float64), you specify the type at compile time using a parameter. For example, you can create a List of Int values like this:
var l: List[Int] = [1, 2, 3, 4]
# l.append(3.14) # error: FloatLiteral cannot be converted to Int
You don't always need to specify the type explicitly. If Mojo can infer the type, you can omit it. For example, when you construct a list from a set of integer literals, Mojo creates a List[Int].
# Inferred type == List[Int]
var l1 = [1, 2, 3, 4]
Where you need a more flexible collection, the Variant type can hold different types of values. For example, a Variant[Int32, Float64] can hold either an Int32 or a Float64 value at any given time. (Using Variant is not covered in this section, see the API docs for more information.)
The following sections give brief introduction to the main collection types.
List​
List is a dynamically-sized array of elements. List elements need to conform to the Copyable and Movable traits. Most of the common standard library primitives, like Int, String, and SIMD conform to this trait. You can create a List by passing the element type as a parameter, like this:
var l = List[String]()
The List type supports a subset of the Python list API, including the ability to append to the list, pop items out of the list, and access list items using subscript notation.
var list = [2, 3, 5]
list.append(7)
list.append(11)
print("Popping last item from list: ", list.pop())
for idx in range(len(list)):
      print(list[idx], end=", ")

Popping last item from list:  11
2, 3, 5, 7,
Note that the previous code sample leaves out the type parameter when creating the list. Because the list is being created with a set of Int values, Mojo can infer the type from the arguments.
* Mojo supports list, set, and dictionary literals for collection initialization:
# List literal, element type infers to Int.
var nums = [2, 3, 5]
You can also use an explicit type if you want a specific element type:
var list : List[UInt8] = [2, 3, 5]
You can also use list "comprehensions" for compact conditional initialization:
var list2 = [x*Int(y) for x in nums for y in list if x != 3]
* You can't print() a list, or convert it directly into a string.
# Does not work
print(list)
As shown above, you can print the individual elements in a list as long as they're a Stringable type.
* Iterating a List returns an immutable reference to each item:
var list = [2, 3, 4]
for item in list:
      print(item, end=", ")
2, 3, 4,
If you would like to mutate the elements of the list, capture the reference to the element with ref instead of making a copy:
var list = [2, 3, 4]
for ref item in list:     # Capture a ref to the list element
      print(item, end=", ")
      item = 0  # Mutates the element inside the list
print("\nAfter loop:", list[0], list[1], list[2])
2, 3, 4,
After loop: 0 0 0
You can see that the original loop entries were modified.
Dict​
The Dict type is an associative array that holds key-value pairs. You can create a Dict by specifying the key type and value type as parameters and using dictionary literals:
# Empty dictionary
var empty_dict: Dict[String, Float64] = {}

# Dictionary with initial key-value pairs
var values: Dict[String, Float64] = {"pi": 3.14159, "e": 2.71828}
You can also use the constructor syntax:
var values = Dict[String, Float64]()
The dictionary's key type must conform to the KeyElement trait, and value elements must conform to the Copyable and Movable traits.
You can insert and remove key-value pairs, update the value assigned to a key, and iterate through keys, values, or items in the dictionary.
The Dict iterators all yield references, which are copied into the declared name by default, but you can use the ref marker to avoid the copy:
var d: Dict[String, Float64] = {
    "plasticity": 3.1,
    "elasticity": 1.3,
    "electricity": 9.7
}
for item in d.items():
    print(item.key, item.value)
plasticity 3.1000000000000001
elasticity 1.3
electricity 9.6999999999999993
This is an unmeasurable micro-optimization in this case, but is useful when working with types that aren't Copyable.
Set​
The Set type represents a set of unique values. You can add and remove elements from the set, test whether a value exists in the set, and perform set algebra operations, like unions and intersections between two sets.
Sets are generic and the element type must conform to the KeyElement trait. Like lists and dictionaries, sets support standard literal syntax, as well as generator comprehensions:
i_like = {"sushi", "ice cream", "tacos", "pho"}
you_like = {"burgers", "tacos", "salad", "ice cream"}
we_like = i_like.intersection(you_like)

print("We both like:")
for item in we_like:
    print("-", item)
We both like:
- ice cream
- tacos
Optional​
An Optional represents a value that may or may not be present. Like the other collection types, it is generic, and can hold any type that conforms to the Copyable and Movable traits.
# Two ways to initialize an Optional with a value
var opt1 = Optional(5)
var opt2: Optional[Int] = 5
# Two ways to initialize an Optional with no value
var opt3 = Optional[Int]()
var opt4: Optional[Int] = None
An Optional evaluates as True when it holds a value, False otherwise. If the Optional holds a value, you can retrieve a reference to the value using the value() method. But calling value() on an Optional with no value results in undefined behavior, so you should always guard a call to value() inside a conditional that checks whether a value exists.
var opt: Optional[String] = "Testing"
if opt:
    var value_ref = opt.value()
    print(value_ref)
Testing
Alternately, you can use the or_else() method, which returns the stored value if there is one, or a user-specified default value otherwise:
var custom_greeting: Optional[String] = None
print(custom_greeting.or_else("Hello"))

custom_greeting = "Hi"
print(custom_greeting.or_else("Hello"))

Hello
Hi
Register-passable, memory-only, and trivial types​
In various places in the documentation you'll see references to register-passable, memory-only, and trivial types. Register-passable and memory-only types are distinguished based on how they hold data:
* Register-passable types are composed exclusively of fixed-size data types, which can (theoretically) be stored in a machine register. A register-passable type can include other types, as long as they are also register-passable. Int, Bool, and SIMD, for example, are all register-passable types. So a register-passable struct could include Int and Bool fields, but not a String field. Register-passable types are declared with the @register_passable decorator.
* Memory-only types consist of all other types that aren't specifically designated as register-passable types. These types often have pointers or references to dynamically-allocated memory. String, List, and Dict are all examples of memory-only types.
Register-passable types have a slightly different lifecycle than memory-only types, which is discussed in Life of a value.
There are also a number of low-level differences in how the Mojo compiler treats register-passable types versus memory-only types, which you probably won't have to think about for most Mojo programming. For more information, see the @register_passable decorator reference.
Our long-term goal is to make this distinction transparent to the user, and ensure all APIs work with both register-passable and memory-only types. But right now you will see a few standard library types that only work with register-passable types or only work with memory-only types.
In addition to these two categories, Mojo also has "trivial" types. Conceptually a trivial type is simply a type that doesn't require any custom logic in its lifecycle methods. The bits that make up an instance of a trivial type can be copied or moved without any knowledge of what they do. Currently, trivial types are declared using the @register_passable(trivial) decorator. Trivial types shouldn't be limited to only register-passable types, so in the future we intend to separate trivial types from the @register_passable decorator.
AnyType and AnyTrivialRegType​
Two other things you'll see in Mojo APIs are references to AnyType and AnyTrivialRegType. These are effectively metatypes, that is, types of types.
* AnyType is a trait that represents a type with a destructor. You'll find more discussion of it on the Traits page.
* AnyTrivialRegType is a metatype representing any Mojo type that's marked as a trivial type.
You'll see them in signatures like this:
fn any_type_function[ValueType: AnyTrivialRegType](value: ValueType):
    ...
You can read this as any_type_function has an argument, value of type ValueType, where ValueType is a register-passable type, determined at compile time.
There is still some code like this in the standard library, but it's gradually being migrated to more generic code that doesn't distinguish between register-passable and memory-only types.Was this page helpful?

Operators, expressions, and dunder methods
Mojo includes a variety of operators for manipulating values of different types. Generally, the operators are equivalent to those found in Python, though many operators also work with additional Mojo types such as SIMD vectors. Additionally, Mojo allows you to define the behavior of most of these operators for your own custom types by implementing special dunder (double underscore) methods.
This document contains the following three sections:
* Operators and expressions discusses Mojo's built-in operators and how they work with commonly used Mojo types.
* Implement operators for custom types describes the dunder methods that you can implement to support using operators with custom structs that you create.
* An example of implementing operators for a custom type shows a progressive example of writing a custom struct with support for several operators.
Operators and expressions​
This section lists the operators that Mojo supports, their order or precedence and associativity, and describes how these operators behave with several commonly used built-in types.
Operator precedence and associativity​
The table below lists the various Mojo operators, along with their order of precedence and associativity (also referred to as grouping). This table lists operators from the highest precedence to the lowest precedence.
Operators
Description
Associativity (Grouping)
()
Parenthesized expression
Left to right
x[index], x[index:index]
Subscripting, slicing
Left to right
**
Exponentiation
Right to left
+x, -x, ~x
Positive, negative, bitwise NOT
Right to left
*, @, /, //, %
Multiplication, matrix multiplication, division, floor division, remainder
Left to right
+, -
Addition and subtraction
Left to right
<<, >>
Shifts
Left to right
&
Bitwise AND
Left to right
^
Bitwise XOR
Left to right
|
Bitwise OR
Left to right
in, not in, is, is not, <, <=, >, >=, !=, ==
Comparisons, membership tests, identity tests
Left to Right
not x
Boolean NOT
Right to left
x and y
Boolean AND
Left to right
x or y
Boolean OR
Left to right
if-else
Conditional expression
Right to left
:=
Assignment expression (walrus operator)
Right to left
Mojo supports the same operators as Python (plus a few extensions), and they have the same precedence levels. For example, the following arithmetic expression evaluates to 40:
5 + 4 * 3 ** 2 - 1
It is equivalent to the following parenthesized expression to explicitly control the order of evaluation:
(5 + (4 * (3 ** 2))) - 1
Associativity defines how operators of the same precedence level are grouped into expressions. The table indicates whether operators of a given level are left- or right-associative. For example, multiplication and division are left associative, so the following expression results in a value of 3:
3 * 4 / 2 / 2
It is equivalent to the following parenthesized expression to explicitly control the order of evaluation:
((3 * 4) / 2) / 2
Whereas in the following, exponentiation operators are right associative resulting in a value of 264,144:
4 ** 3 ** 2
It is equivalent to the following parenthesized expression to explicitly control the order of evaluation:
4 ** (3 ** 2)
Mojo also uses the caret (^) as the transfer sigil. In expressions where its use might be ambiguous, Mojo treats the character as the bitwise XOR operator. For example, x^+1 is treated as (x)^(+1).
Arithmetic and bitwise operators​
Numeric types describes the different numeric types provided by the Mojo standard library. The arithmetic and bitwise operators have slightly different behavior depending on the types of values provided.
Int and UInt values​
The Int and UInt types represent signed and unsigned integers of the word size of the CPU, typically 64 bits or 32 bits.
The Int and UInt types support all arithmetic operators except matrix multiplication (@), as well as all bitwise and shift operators. If both operands to a binary operator are Int values the result is an Int, if both operands are UInt values the result is a UInt, and if one operand is Int and the other UInt the result is an Int. The one exception for these types is true division, /, which always returns a Float64 type value.
var a_int: Int = -7
var b_int: Int = 4
sum_int = a_int + b_int  # Result is type Int
print("Int sum:", sum_int)

var i_uint: UInt = 9
var j_uint: UInt = 8
sum_uint = i_uint + j_uint  # Result is type UInt
print("UInt sum:", sum_uint)

sum_mixed = a_int + Int(i_uint)  # Result is type Int
print("Mixed sum:", sum_mixed)

quotient_int = a_int / b_int  # Result is type Float64
print("Int quotient:", quotient_int)
quotient_uint = i_uint / j_uint  # Result is type Float64
print("UInt quotient:", quotient_uint)
Int sum: -3
UInt sum: 17
Mixed sum: 2
Int quotient: -1.75
UInt quotient: 1.125
SIMD values​
The Mojo standard library defines the SIMD type to represent a fixed-size array of values that can fit into a processor's register. This allows you to take advantage of single instruction, multiple data operations in hardware to efficiently process multiple values in parallel. SIMD values of a numeric DType support all arithmetic operators except for matrix multiplication (@), though the left shift (<<) and right shift (>>) operators support only integral types. Additionally, SIMD values of an integral or boolean type support all bitwise operators. SIMD values apply the operators in an elementwise fashion, as shown in the following example:
simd1 = SIMD[DType.int32, 4](2, 3, 4, 5)
simd2 = SIMD[DType.int32, 4](-1, 2, -3, 4)
simd3 = simd1 * simd2
print(simd3)
[-2, 6, -12, 20]
Scalar values are simply aliases for single-element SIMD vectors, so Float16 is just an alias for SIMD[DType.float16, 1]. Therefore Scalar values support the same set of arithmetic and bitwise operators.
var f1: Float16 = 2.5
var f2: Float16 = -4.0
var f3 = f1 * f2  # Implicitly of type Float16
print(f3)
-10.0
When using these operators on SIMD values, Mojo requires both to have the same size and DType, and the result is a SIMD of the same size and DType. The operators do not automatically widen lower precision SIMD values to higher precision. This means that the DType of each value must be the same or else the result is a compilation error.
var i8: Int8 = 8
var f64: Float64 = 64.0
result = i8 * f64
error: invalid call to '__mul__': failed to infer parameter 'type' of parent struct 'SIMD'
    result = i8 * f64
             ~~~^~~~~
If you need to perform an arithmetic or bitwise operator on two SIMD values of different types, you can explicitly convert a value to the desired type either by invoking its cast() method or by passing it as an argument to the constructor of the target type.
For example, to fix the previous example, add an explicit conversion:
var i8: Int8 = 8
var f64: Float64 = 64.0
result = Float64(i8) * f64
Here are some more examples of converting SIMD values using both constructors and the cast() method:
simd4 = SIMD[DType.float32, 4](2.2, 3.3, 4.4, 5.5)
simd5 = SIMD[DType.int16, 4](-1, 2, -3, 4)
simd6 = simd4 * simd5.cast[DType.float32]()  # Convert with cast() method
print("simd6:", simd6)
simd7 = simd5 + SIMD[DType.int16, 4](simd4)  # Convert with SIMD constructor
print("simd7:", simd7)
simd6: [-2.2, 6.6, -13.200001, 22.0]
simd7: [1, 5, 1, 9]
One exception is that the exponentiation operator, **, is overloaded so that you can specify an Int type exponent. All values in the SIMD are exponentiated to the same power.
base_simd = SIMD[DType.float64, 4](1.1, 2.2, 3.3, 4.4)
var power: Int = 2
pow_simd = base_simd ** power  # Result is SIMD[DType.float64, 4]
print(pow_simd)
[1.2100000000000002, 4.8400000000000007, 10.889999999999999, 19.360000000000003]
There are three operators related to division:
* /, the "true division" operator, performs floating point division for SIMD values with a floating point DType. For SIMD values with an integral DType, true division truncates the quotient to an integral result.
num_float16 = SIMD[DType.float16, 4](3.5, -3.5, 3.5, -3.5)
denom_float16 = SIMD[DType.float16, 4](2.5, 2.5, -2.5, -2.5)

num_int32 = SIMD[DType.int32, 4](5, -6, 7, -8)
denom_int32 = SIMD[DType.int32, 4](2, 3, -4, -5)

# Result is SIMD[DType.float16, 4]
true_quotient_float16 = num_float16 / denom_float16
print("True float16 division:", true_quotient_float16)

# Result is SIMD[DType.int32, 4]
true_quotient_int32 = num_int32 / denom_int32
print("True int32 division:", true_quotient_int32)
True float16 division: [1.4003906, -1.4003906, -1.4003906, 1.4003906]
True int32 division: [2, -2, -1, 1]
* //, the "floor division" operator, performs division and rounds down the result to the nearest integer. The resulting SIMD is still the same type as the original operands. For example:
# Result is SIMD[DType.float16, 4]
var floor_quotient_float16 = num_float16 // denom_float16
print("Floor float16 division:", floor_quotient_float16)

# Result is SIMD[DType.int32, 4]
var floor_quotient_int32 = num_int32 // denom_int32
print("Floor int32 division:", floor_quotient_int32)
Floor float16 division: [1.0, -2.0, -2.0, 1.0]
Floor int32 division: [2, -2, -2, 1]
* %, the modulo operator, returns the remainder after dividing the numerator by the denominator an integral number of times. The relationship between the // and % operators can be defined as num == denom * (num // denom) + (num % denom). For example:
# Result is SIMD[DType.float16, 4]
var remainder_float16 = num_float16 % denom_float16
print("Modulo float16:", remainder_float16)

# Result is SIMD[DType.int32, 4]
var remainder_int32 = num_int32 % denom_int32
print("Modulo int32:", remainder_int32)

print()

# Result is SIMD[DType.float16, 4]
var result_float16 = denom_float16 * floor_quotient_float16 + remainder_float16
print("Result float16:", result_float16)

# Result is SIMD[DType.int32, 4]
var result_int32 = denom_int32 * floor_quotient_int32 + remainder_int32
print("Result int32:", result_int32)
Modulo float16: [1.0, 1.5, -1.5, -1.0]
Modulo int32: [1, 0, -1, -3]

Result float16: [3.5, -3.5, 3.5, -3.5]
Result int32: [5, -6, 7, -8]
IntLiteral and FloatLiteral values​
IntLiteral and FloatLiteral are compile-time, numeric values. When they are used in a compile-time context, they are arbitrary-precision values. When they are used in a run-time context, they are materialized as Int and Float64 type values, respectively.
As an example, the following code causes a compile-time error because the calculated IntLiteral value is too large to store in an Int variable:
alias big_int = (1 << 65) + 123456789  # IntLiteral
var too_big_int: Int = big_int
print("Result:", too_big_int)
note: integer value 36893488147542560021 requires 67 bits to store, but the destination bit width is only 64 bits wide
However in the following example, taking that same IntLiteral value, dividing by the IntLiteral 10 and then assigning the result to an Int variable compiles and runs successfully, because the final IntLiteral quotient can fit in a 64-bit Int.
alias big_int = (1 << 65) + 123456789  # IntLiteral
var not_too_big_int: Int = big_int // 10
print("Result:", not_too_big_int)
Result: 3689348814754256002
In a compile-time context, IntLiteral and FloatLiteral values support all arithmetic operators except exponentiation (**), and IntLiteral values support all bitwise and shift operators. In a run-time context, materialized IntLiteral values are Int values and therefore support the same operators as Int, and materialized FloatLiteral values are Float64 values and therefore support the same operators as Float64.
Comparison operators​
Mojo supports a standard set of comparison operators: ==, !=, <, <=, >, and >=. However their behavior depends on the type of values being compared.
The remainder of this section describes numerical comparison operators. String comparisons are discussed in the String operators. Several other types in the Mojo standard library support various comparison operators, in particular the "equal" and "not equal" comparisons. Consult the API documentation for a type to determine whether any comparison operators are supported.
Bool-returning comparisons​
These comparisons return a single Bool value:
* Int, UInt, IntLiteral, and any type that can be implicitly converted to Int or UInt do standard numerical comparison with a Bool result.
* Equality operators (== and !=) with multi-element SIMD values return a Bool result using reduction semantics. The comparison is True only if it's true for all corresponding elements. For example:
simd8 = SIMD[DType.int32, 4](1, 2, 3, 2)
simd9 = SIMD[DType.int32, 4](1, 2, 4, 2)
print("simd8 == simd9:", simd8 == simd9)  # False (element 2 differs)
print("simd8 != simd9:", simd8 != simd9)  # True (not all elements equal)
simd8 == simd9: False
simd8 != simd9: True
* Inequality operators (<, <=, >, >=) with multi-element SIMD values are not supported. These operators only work with scalar (single-element) SIMD values.
* Scalar values are simply aliases for single-element SIMD vectors and support all comparison operators with Bool results:
var float1: Float16 = 12.345         # SIMD[DType.float16, 1]
var float2: Float32 = 0.5            # SIMD[DType.float32, 1]
result = Float32(float1) > float2    # Result is Bool
print(result)
True
Elementwise comparisons​
For elementwise comparisons that return a SIMD[DType.bool] result, use the comparison methods: eq(), ne(), lt(), le(), gt(), and ge(). These methods work with both SIMD-to-SIMD and SIMD-to-scalar comparisons. Here are examples showing all six elementwise comparison methods:
simd8 = SIMD[DType.int32, 4](1, 2, 3, 2)
simd9 = SIMD[DType.int32, 4](1, 2, 4, 2)

print("simd8.eq(simd9):", simd8.eq(simd9))    # Equal
print("simd8.ne(simd9):", simd8.ne(simd9))    # Not equal
print("simd8.lt(simd9):", simd8.lt(simd9))    # Less than
print("simd8.le(simd9):", simd8.le(simd9))    # Less than or equal
print("simd8.gt(simd9):", simd8.gt(simd9))    # Greater than
print("simd8.ge(simd9):", simd8.ge(simd9))    # Greater than or equal
simd8.eq(simd9): [True, True, False, True]
simd8.ne(simd9): [False, False, True, False]
simd8.lt(simd9): [False, False, True, False]
simd8.le(simd9): [True, True, True, True]
simd8.gt(simd9): [False, False, False, False]
simd8.ge(simd9): [True, True, False, True]
You can also use these methods for SIMD-to-scalar comparisons:
simd4 = SIMD[DType.int16, 4](-1, 2, -3, 4)
simd5 = simd4.gt(2)  # SIMD[DType.bool, 4]
print("simd4.gt(2):", simd5)

simd6 = SIMD[DType.float32, 4](1.1, -2.2, 3.3, -4.4)
simd7 = simd6.gt(0.5)  # SIMD[DType.bool, 4]
print("simd6.gt(0.5):", simd7)
simd4.gt(2): [False, False, False, True]
simd6.gt(0.5): [True, False, True, False]
Use elementwise comparison methods when you need to compare each element individually and work with the resulting boolean mask for further processing.
String operators​
As discussed in Strings, the String type represents a mutable string value. In contrast, the StringLiteral type represents a literal string that is embedded into your compiled program, but at run-time it materializes to a String, allowing you to mutate it:
message = "Hello"       # type = String
alias name = " Pat"       # type = StringLiteral
greeting = " good Day!"  # type = String

# Mutate the original `message` String
message += name
message += greeting
print(message)
Hello Pat good day!
This means that StringLiteral values can be intermixed with String values in any runtime expression without having to convert between types.
String concatenation​
The + operator performs string concatenation. The StringLiteral type supports compile-time string concatenation.
alias last_name = "Curie"

# Compile-time StringLiteral alias
alias marie = "Marie " + last_name
print(marie)

# Compile-time concatenation before materializing to a run-time `String`
pierre = "Pierre " + last_name
print(pierre)
When concatenating multiple values together to form a String, using the multi-argument String() constructor is more performant than using multiple + concatenation operators and can improve code readability. For example, instead of writing this:
result = "The point at (" + String(x) + ", " + String(y) + ")"
you can write:
result = String("The point at (", x, ", ", y, ")")
This will write the underlying data using a stack buffer, and will only allocate and memcpy to the heap once.
String replication​
The * operator replicates a String a specified number of times. For example:
var str1: String = "la"
str2 = str1 * 5
print(str2)
lalalalala
StringLiteral supports the * operator for both compile-time and run-time string replication. The following examples perform compile-time string replication resulting in StringLiteral values:
alias divider1 = "=" * 40
alias symbol = "#"
alias divider2 = symbol * 40

# You must define the following function using `fn` because an alias
# initializer cannot call a function that can potentially raise an error.
fn generate_divider(char: String, repeat: Int) -> String:
    return char * repeat

alias divider3 = generate_divider("~", 40)  # Evaluated at compile-time

print(divider1)
print(divider2)
print(divider3)
========================================
########################################
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In contrast, the following examples perform run-time string replication resulting in String values:
repeat = 40
div1 = "^" * repeat
print(div1)
print("_" * repeat)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
________________________________________
String comparison​
String and StringLiteral values can be compared using standard lexicographical ordering, producing a Bool. For example, "Zebra" is treated as less than "ant" because upper case letters occur before lower case letters in the character encoding.
var animal: String = "bird"

is_cat_eq = "cat" == animal
print('Is "cat" equal to "{}"?'.format(animal), is_cat_eq)

is_cat_ne = "cat" != animal
print('Is "cat" not equal to "{}"?'.format(animal), is_cat_ne)

is_bird_eq = "bird" == animal
print('Is "bird" equal to "{}"?'.format(animal), is_bird_eq)

is_cat_gt = "CAT" > animal
print('Is "CAT" greater than "{}"?'.format(animal), is_cat_gt)

is_ge_cat = animal >= "CAT"
print('Is "{}" greater than or equal to "CAT"?'.format(animal), is_ge_cat)
Is "cat" equal to "bird"? False
Is "cat" not equal to "bird"? True
Is "bird" equal to "bird"? True
Is "CAT" greater than "bird"? False
Is "bird" greater than or equal to "CAT"? True
Substring testing​
String, StringLiteral, and StringSlice support using the in operator to produce a Bool result indicating whether a given substring appears within another string. The operator is overloaded so that you can use any combination of String and StringLiteral for both the substring and the string to test.
var food: String = "peanut butter"

if "nut" in food:
    print("It contains a nut")
else:
    print("It doesn't contain a nut")
It contains a nut
String indexing and slicing​
String, StringLiteral, and StringSlice allow you to use indexing to return a single character. Character positions are identified with a zero-based index starting from the first character. You can also specify a negative index to count backwards from the end of the string, with the last character identified by index -1. Specifying an index beyond the bounds of the string results in a run-time error.
var alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"  # String type value
print(alphabet[0], alphabet[-1])

# The following would produce a run-time error
# print(alphabet[45])
A Z
The String and StringSlice types-but not the StringLiteral type-also support slices to return a substring from the original String. Providing a slice in the form [start:end] returns a substring starting with the character index specified by start and continuing up to but not including the character at index end. You can use positive or negative indexing for both the start and end values. Omitting start is the same as specifying 0, and omitting end is the same as specifying the length of the string.
var alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ" # String type value
print(alphabet[1:4])  # The 2nd through 4th characters
print(alphabet[:6])   # The first 6 characters
print(alphabet[-6:])  # The last 6 characters
BCD
ABCDEF
UVWXYZ
You can also specify a slice with a step value, as in [start:end:step] indicating the increment between subsequent indices of the slide. (This is also sometimes referred to as a "stride.") If you provide a negative value for step, characters are selected in reverse order starting with start but then with decreasing index values up to but not including end.
print(alphabet[1:6:2])     # The 2nd, 4th, and 6th characters
print(alphabet[-1:-4:-1])  # The last 3 characters in reverse order
print(alphabet[::-1])      # The entire string reversed
BDF
ZYX
ZYXWVUTSRQPONMLKJIHGFEDCBA
In-place assignment operators​
Mutable types that support binary arithmetic, bitwise, and shift operators typically support equivalent in-place assignment operators. That means that for a type that supports the + operator, the following two statements are essentially equivalent:
a = a + b
a += b
However there is a subtle difference between the two. In the first example, the expression a + b produces a new value, which is then assigned to a. In contrast, the second example does an in-place modification of the value currently assigned to a. For register-passable types, the compiled results might be equivalent at run-time. But for a memory-only type, the first example allocates storage for the result of a + b and then assigns the value to the variable, whereas the second example can do an in-place modification of the existing value.
A type must explicitly implement in-place assignment methods, so you might encounter some types where in-place equivalents are not supported.
Assignment expressions​
The "walrus" operator, :=, allows you to assign a value to a variable within an expression. The value provided is both assigned to the variable and becomes the result of the expression. This often can simplify conditional or looping logic. For example, consider the following prompting loop:
while True:
    name = input("Enter a name or 'quit' to exit: ")
    if name == "quit":
        break
    print("Hello,", name)
Enter a name or 'quit' to exit: Coco
Hello, Coco
Enter a name or 'quit' to exit: Vivienne
Hello, Vivienne
Enter a name or 'quit' to exit: quit
Using the walrus operator, you can implement the same behavior like this:
while (name := input("Enter a name or 'quit' to exit: ")) != "quit":
    print("Hello,", name)
Enter a name or 'quit' to exit: Donna
Hello, Donna
Enter a name or 'quit' to exit: Vera
Hello, Vera
Enter a name or 'quit' to exit: quit
Type merging​
When an expression involves values of different types, Mojo needs to statically determine the return type of the expression. This process is called type merging. By default, Mojo determines type merging based on implicit conversions. Individual structs can also define custom type merging behavior.
The following code demonstrates type merging based on implicit conversions:
list = [0.5, 1, 2]
for value in list:
    print(value)
0.5
1.0
2.0
Here, the list literal includes both float and integer literals, which materialize as Float64 and Int, respectively. Since Int can be implicitly converted to Float64, the result is a List[Float64].
Here's an example of where type merging fails:
a: Int = 0
b: String = "Hello"
c = a if a > 0 else b   # Error: value of type 'Int' is not compatible with
                        # value of type 'String'mojo
In this case, Int can't be implicitly converted to a String, and String can't be implicitly converted to an Int, so type merging fails. This is the correct result: there's no way for Mojo to know what type you want c to take. You can fix this by adding an explicit conversion:
c = String(a) if a > 0 else b
Individual structs can define custom type merging logic by defining a __merge_with__() dunder method. For example:
@fieldwise_init
struct MyType(Movable, Copyable):
    var val: Int

    def __bool__(self) -> Bool:
        return self.val > 0

    def __merge_with__[other_type: type_of(Int)](self) -> Int:
        return Int(self.val)

def main():
    i = 0
    m = MyType(9)
    print(i if i > 0 else m)  # prints "9"
If either type in the expression defines a custom __merge_with__() dunder for merging with the other type, this type takes precedence over any implicit conversions. (Note that the result type doesn't have to be either of the input types, it could be a third type.)
A type can declare multiple __merge_with__() overrides for different types.
At a high level, the logic for merging two types goes like this:
* Does either type define a __merge_with__() method for the other type? If so, the returned value determines the target type. 
o If both types define a __merge_with__() method for the other type, the two methods must both return the same type, or the conversion fails.
o Both types must be implicitly convertible to the target type (a type is always implicitly convertible to itself).
* Is either type implicitly convertible to the other type? 
o If only one type is implicitly convertible to the other type, convert it.
o If both types are convertible to the other type, the conversion is ambiguous, and it fails.
For more background on type merging and the __merge_with__() dunder, see the proposal, Customizable Type Merging in Mojo.
Implement operators for custom types​
When you create a custom struct, Mojo allows you to define the behavior of many of the built-in operators for that type by implementing special dunder (double underscore) methods. This section lists the dunder methods associated with the operators and briefly describes the requirements for implementing them.
Currently, Mojo doesn't support defining arbitrary custom operators (for example, -^-). You can define behaviors for only the operators listed in the following subsections.
Unary operator dunder methods​
A unary operator invokes an associated dunder method on the value to which it applies. The supported unary operators and their corresponding methods are shown in the table below.
Operator
Dunder method
+ positive
__pos__()
- negative
__neg__()
~ bitwise NOT
__invert__()
For each of these methods that you decide to implement, you should return either the original value if unchanged, or a new value representing the result of the operator. For example, you could implement the - negative operator for a MyInt struct like this:
@fieldwise_init
struct MyInt:
    var value: Int

    def __neg__(self) -> Self:
        return Self(-self.value)
Binary arithmetic, shift, and bitwise operator dunder methods​
When you have a binary expression like a + b, there are two possible dunder methods that could be invoked.
Mojo first determines whether the left-hand side value (a in this example) has a "normal" version of the + operator's dunder method defined that accepts a value of the right-hand side's type. If so, it then invokes that method on the left-hand side value and passes the right-hand side value as an argument.
If Mojo doesn't find a matching "normal" dunder method on the left-hand side value, it then checks whether the right-hand side value has a "reflected" (sometimes referred to as "reversed") version of the + operator's dunder method defined that accepts a value of the left-hand side's type. If so, it then invokes that method on the right-hand side value and passes the left-hand side value as an argument.
For both the normal and the reflected versions, the dunder method should return a new value representing the result of the operator.
Additionally, there are dunder methods corresponding to the in-place assignment versions of the operators. These methods receive the right-hand side value as an argument and the methods should modify the existing left-hand side value to reflect the result of the operator.
The table below lists the various binary arithmetic, shift, and bitwise operators and their corresponding normal, reflected, and in-place dunder methods.
Operator
Normal
Reflected
In-place
+ addition
__add__()
__radd__()
__iadd__()
- subtraction
__sub__()
__rsub__()
__isub__()
* multiplication
__mul__()
__rmul__()
__imul__()
/ division
__truediv__()
__rtruediv__()
__itruediv__()
// floor division
__floordiv__()
__rfloordiv__()
__ifloordiv__()
% modulus/remainder
__mod__()
__rmod__()
__imod__()
** exponentiation
__pow__()
__rpow__()
__ipow__()
@ matrix multiplication
__matmul__()
__rmatmul__()
__imatmul__()
<< left shift
__lshift__()
__rlshift__()
__ilshift__()
>> right shift
__rshift__()
__rrshift__()
__irshift__()
& bitwise AND
__and__()
__rand__()
__iand__()
| bitwise OR
__or__()
__ror__()
__ior__()
^ bitwise XOR
__xor__()
__rxor__()
__ixor__()
As an example, consider implementing support for all of the + operator dunder methods for a custom MyInt struct. This shows supporting adding two MyInt instances as well as adding a MyInt and an Int. We can support the case of having the Int as the right-hand side argument by overloaded the definition of __add__(). But to support the case of having the Int as the left-hand side argument, we need to implement an __radd__() method, because the built-in Int type doesn't have an __add__() method that supports our custom MyInt type.
@fieldwise_init
struct MyInt:
    var value: Int

    def __add__(self, rhs: MyInt) -> Self:
        return MyInt(self.value + rhs.value)

    def __add__(self, rhs: Int) -> Self:
        return MyInt(self.value + rhs)

    def __radd__(self, lhs: Int) -> Self:
        return MyInt(self.value + lhs)

    def __iadd__(mut self, rhs: MyInt) -> None:
        self.value += rhs.value

    def __iadd__(mut self, rhs: Int) -> None:
        self.value += rhs
Comparison operator dunder methods​
When you have a comparison expression like a < b, Mojo invokes as associated dunder method on the left-hand side value and passes the right-hand side value as an argument. Mojo doesn't support "reflected" versions of these dunder methods because you should only compare values of the same type. The comparison dunder methods must return a Bool result representing the result of the comparison.
There are two traits associated with the comparison dunder methods. A type that implements the Comparable trait defines all of the comparison methods, and authors are required to implement at least the "less-than" and "equal" methods, since the trait provides defaults for the rest. However, some types don't have a natural ordering (for example, complex numbers). For those types you can decide to implement the Equatable trait, which defines only the "equal" and "not equal" comparison methods, with "equal" being required to implement by conforming structs.
The supported comparison operators and their corresponding methods are shown in the table below.
Operator
Dunder method
== equal
__eq__()
!= not equal
__ne__()
< less than
__lt__()
<= less than or equal
__le__()
> greater than
__gt__()
>= greater than or equal
__ge__()
The Comparable and Equatable traits don't allow the comparison dunder methods to raise errors. Because using def to define a method implies that it can raise an error, you must use fn to implement the comparison methods declared by these traits. See Functions for more information on the differences between defining functions with def and fn.
As an example, consider implementing support for all of the comparison operator dunder methods for a custom MyInt struct by relying on the default implementations provided by the Comparable (and transitively the Equatable) traits.
@fieldwise_init
struct MyInt(Comparable):
    var value: Int

    fn __eq__(self, rhs: MyInt) -> Bool:
        return self.value == rhs.value

    fn __lt__(self, rhs: MyInt) -> Bool:
        return self.value < rhs.value

    # `__ne__`, `__le__`, `__gt__`, and `__ge__` have default implementations.
Membership operator dunder methods​
The in and not in operators depend on a type implementing the __contains__() dunder method. Typically only collection types (such as List, Dict, and Set) implement this method. It should accept the right-hand side value as an argument and return a Bool indicating whether the value is present in the collection or not.
Subscript and slicing dunder methods​
Subscripting and slicing typically apply only to sequential collection types, like List and String. Subscripting references a single element of a collection or a dimension of a multi-dimensional container, whereas slicing refers to a range of values. A type supports both subscripting and slicing by implementing the __getitem__() method for retrieving values and the __setitem__() method for setting values.
Subscripting​
In the simple case of a one-dimensional sequence, the __getitem__() and __setitem__() methods should have signatures similar to this:
struct MySeq[type: Copyable & Movable]:
    fn __getitem__(self, idx: Int) -> type:
        # Return element at the given index
        ...
    fn __setitem__(mut self, idx: Int, value: type):
        # Assign the element at the given index the provided value
It's also possible to support multi-dimensional collections, in which case you can implement both __getitem__() and __setitem__() methods to accept multiple index arguments-or even variadic index arguments for arbitrary-dimension collections.
struct MySeq[type: Copyable & Movable]:
    # 2-dimension support
    fn __getitem__(self, x_idx: Int, y_idx: Int) -> type:
        ...
    # Arbitrary-dimension support
    fn __getitem__(self, *indices: Int) -> type:
        ...
Slicing​
You provide slicing support for a collection type also by implementing __getitem__() and __setitem__() methods. But for slicing, instead of accepting an Int index (or indices, in the case of a multi-dimensional collection) you implement to methods to accept a Slice (or multiple Slices in the case of a multi-dimensional collection).
struct MySeq[type: Copyable & Movable]:
    # Return a new MySeq with a subset of elements
    fn __getitem__(self, span: Slice) -> Self:
        ...

A Slice contains three fields:
* start (Optional[Int]): The starting index of the slice
* end (Optional[Int]): The ending index of the slice
* step (Optional[Int]): The step increment value of the slice.
Because the start, end, and step values are all optional when using slice syntax, they are represented as Optional[Int] values in the Slice. And if present, the index values might be negative representing a relative position from the end of the sequence. As a convenience, Slice provides an indices() method that accepts a length value and returns a 3-tuple of "normalized" start, end, and step values for the given length, all represented as non-negative values. You can then use these normalized values to determine the corresponding elements of your collection being referenced.
struct MySeq[type: Copyable & Movable]:
    var size: Int

    # Return a new MySeq with a subset of elements
    fn __getitem__(self, span: Slice) -> Self:
        var start: Int
        var end: Int
        var step: Int
        start, end, step = span.indices(self.size)
        ...

An example of implementing operators for a custom type​
As an example of implementing operators for a custom Mojo type, let's create a Complex struct to represent a single complex number, with both the real and imaginary components stored as Float64 values. We'll implement most of the arithmetic operators, the associated in-place assignment operators, the equality comparison operators, and a few additional convenience methods to support operations like printing complex values. We'll also allow mixing Complex and Float64 values in arithmetic expressions to produce a Complex result.
This example builds our Complex struct incrementally. You can also find the complete example in the public GitHub repo.
Note that the Mojo standard library implements a parameterized ComplexSIMD struct that provides support for a basic set of arithmetic operators. However, our Complex type will not be based on the ComplexSIMD struct or be compatible with it.
Implement lifecycle methods​
Our Complex struct is an example of a simple value type consisting of trivial numeric fields and requiring no special constructor or destructor behaviors. This means we can use the @register_passable("trivial") decorator, which declares that the type can be trivially copied, moved, and destroyed-and doesn't need a copy constructor, move constructor, or destructor.
For the time being, we'll also use the @fieldwise_init decorator to automatically implement a field-wise initializer (a constructor with arguments for each field).
@fieldwise_init
@register_passable("trivial")
struct Complex:
    var re: Float64
    var im: Float64
This definition is enough for us to create Complex instances and access their real and imaginary fields.
c1 = Complex(-1.2, 6.5)
print("c1: Real: {}; Imaginary: {}".format(c1.re, c1.im))
c1: Real: -1.2; Imaginary: 6.5
As a convenience, let's add an explicit constructor to handle the case of creating a Complex instance with an imaginary component of 0.
@register_passable("trivial")
struct Complex():
    var re: Float64
    var im: Float64

    fn __init__(out self, re: Float64, im: Float64 = 0.0):
        self.re = re
        self.im = im
Since this constructor also handles creating a Complex instance with both real and imaginary components, we don't need the @fieldwise_init decorator anymore.
Now we can create a Complex instance and provide just a real component.
c2 = Complex(3.14159)
print("c2: Real: {}; Imaginary: {}".format(c2.re, c2.im))
c2: Real: 3.1415899999999999; Imaginary: 0.0
Implement the Writable and Stringable traits​
To make it simpler to print Complex values, let's implement the Writable trait. While we're at it, let's also implement the Stringable trait so that we can use the String() constructor to generate a String representation of a Complex value. You can find out more about these traits and their associated methods in The Stringable, Representable, and Writable traits.
@register_passable("trivial")
struct Complex(
    Writable,
    Stringable,
):
    # ...

    fn __str__(self) -> String:
        return String.write(self)

    fn write_to(self, mut writer: Some[Writer]):
        writer.write("(", self.re)
        if self.im < 0:
            writer.write(" - ", -self.im)
        else:
            writer.write(" + ", self.im)
        writer.write("i)")
The Writable trait doesn't allow the write_to() method to raise an error and the Stringable trait doesn't allow the __str__() method to raise an error. Because defining a method with def implies that it can raise an error, we instead have to define these methods with fn. See Functions for more information on the differences between defining functions with def and fn.
Now we can print a Complex value directly, and we can explicitly generate a String representation by passing a Complex value to String() which constructs a new String from all the arguments passed to it.
c3 = Complex(3.14159, -2.71828)
print("c3 =", c3)

var msg = String("The value is: ", c3)
print(msg)
c3 = (3.1415899999999999 - 2.71828i)
The value is: (3.1415899999999999 - 2.71828i)
Implement basic indexing​
Indexing usually is supported only by collection types. But as an example, let's implement support for accessing the real component as index 0 and the imaginary component as index 1. We'll not implement slicing or variadic assignment for this example.
    # ...
    def __getitem__(self, idx: Int) -> Float64:
        if idx == 0:
            return self.re
        elif idx == 1:
            return self.im
        else:
            raise "index out of bounds"

    def __setitem__(mut self, idx: Int, value: Float64) -> None:
        if idx == 0:
            self.re = value
        elif idx == 1:
            self.im = value
        else:
            raise "index out of bounds"
Now let's try getting and setting the real and imaginary components of a Complex value using indexing.
c2 = Complex(3.14159)
print("c2[0]: {}; c2[1]: {}".format(c2[0], c2[1]))
c2[0] = 2.71828
c2[1] = 42
print("c2[0] = 2.71828; c2[1] = 42; c2:", c2)
c2[0]: 3.1415899999999999; c2[1]: 0.0
c2[0] = 2.71828; c2[1] = 42; c2: (2.71828 + 42.0i)
Implement arithmetic operators​
Now let's implement the dunder methods that allow us to perform arithmetic operations on Complex values. (Refer to the Wikipedia page on complex numbers for a more in-depth explanation of the formulas for these operators.)
Implement basic operators for Complex values​
The unary + operator simply returns the original value, whereas the unary - operator returns a new Complex value with the real and imaginary components negated.
    # ...
    def __pos__(self) -> Self:
        return self

    def __neg__(self) -> Self:
        return Self(-self.re, -self.im)
Let's test these out by printing the result of applying each operator.
c1 = Complex(-1.2, 6.5)
print("+c1:", +c1)
print("-c1:", -c1)
+c1: (-1.2 + 6.5i)
-c1: (1.2 - 6.5i)
Next we'll implement the basic binary operators: +, -, *, and /. Dividing complex numbers is a bit tricky, so we'll also define a helper method called norm() to calculate the Euclidean norm of a Complex instance, which can also be useful for other types of analysis with complex numbers.
For all of these dunder methods, the left-hand side operand is self and the right-hand side operand is passed as an argument. We return a new Complex value representing the result.
from math import sqrt

# ...

    def __add__(self, rhs: Self) -> Self:
        return Self(self.re + rhs.re, self.im + rhs.im)

    def __sub__(self, rhs: Self) -> Self:
        return Self(self.re - rhs.re, self.im - rhs.im)

    def __mul__(self, rhs: Self) -> Self:
        return Self(
            self.re * rhs.re - self.im * rhs.im,
            self.re * rhs.im + self.im * rhs.re
        )

    def __truediv__(self, rhs: Self) -> Self:
        denom = rhs.squared_norm()
        return Self(
            (self.re * rhs.re + self.im * rhs.im) / denom,
            (self.im * rhs.re - self.re * rhs.im) / denom
        )

    def squared_norm(self) -> Float64:
        return self.re * self.re + self.im * self.im

    def norm(self) -> Float64:
        return sqrt(self.squared_norm())
Now we can try them out.
c1 = Complex(-1.2, 6.5)
c3 = Complex(3.14159, -2.71828)
print("c1 + c3 =", c1 + c3)
print("c1 - c3 =", c1 - c3)
print("c1 * c3 =", c1 * c3)
print("c1 / c3 =", c1 / c3)
c1 + c3 = (1.9415899999999999 + 3.78172i)
c1 - c3 = (-4.3415900000000001 + 9.21828i)
c1 * c3 = (13.898912000000001 + 23.682270999999997i)
c1 / c3 = (-1.2422030701265261 + 0.99419218883955773i)
Implement overloaded arithmetic operators for Float64 values​
Our initial set of binary arithmetic operators work fine if both operands are Complex instances. But if we have a Float64 value representing just a real value, we'd first need to use it to create a Complex value before we could add, subtract, multiply, or divide it with another Complex value. If we think that this will be a common use case, it makes sense to overload our arithmetic methods to accept a Float64 as the second operand.
For the case where we have complex1 + float1, we can just create an overloaded definition of __add__(). But what about the case of float1 + complex1? By default, when Mojo encounters a + operator it tries to invoke the __add__() method of the left-hand operand, but the built-in Float64 type doesn't implement support for addition with a Complex value. This is an example where we need to implement the __radd__() method on the Complex type. When Mojo can't find an __add__(self, rhs: Complex) -> Complex method defined on Float64, it uses the __radd__(self, lhs: Float64) -> Complex method defined on Complex.
So we can support arithmetic operations on Complex and Float64 values by implementing the following eight methods.
    # ...
    def __add__(self, rhs: Float64) -> Self:
        return Self(self.re + rhs, self.im)

    def __radd__(self, lhs: Float64) -> Self:
        return Self(self.re + lhs, self.im)

    def __sub__(self, rhs: Float64) -> Self:
        return Self(self.re - rhs, self.im)

    def __rsub__(self, lhs: Float64) -> Self:
        return Self(lhs - self.re, -self.im)

    def __mul__(self, rhs: Float64) -> Self:
        return Self(self.re * rhs, self.im * rhs)

    def __rmul__(self, lhs: Float64) -> Self:
        return Self(lhs * self.re, lhs * self.im)

    def __truediv__(self, rhs: Float64) -> Self:
        return Self(self.re / rhs, self.im / rhs)

    def __rtruediv__(self, lhs: Float64) -> Self:
        denom = self.squared_norm()
        return Self(
            (lhs * self.re) / denom,
            (-lhs * self.im) / denom
        )
Let's see them in action.
c1 = Complex(-1.2, 6.5)
f1 = 2.5
print("c1 + f1 =", c1 + f1)
print("f1 + c1 =", f1 + c1)
print("c1 - f1 =", c1 - f1)
print("f1 - c1 =", f1 - c1)
print("c1 * f1 =", c1 * f1)
print("f1 * c1 =", f1 * c1)
print("c1 / f1 =", c1 / f1)
print("f1 / c1 =", f1 / c1)
c1 + f1 = (1.3 + 6.5i)
f1 + c1 = (1.3 + 6.5i)
c1 - f1 = (-3.7000000000000002 + 6.5i)
f1 - c1 = (3.7000000000000002 - 6.5i)
c1 * f1 = (-3.0 + 16.25i)
f1 * c1 = (-3.0 + 16.25i)
c1 / f1 = (-0.47999999999999998 + 2.6000000000000001i)
f1 / c1 = (-0.068665598535133904 - 0.37193865873197529i)
Implement in-place assignment operators​
Now let's implement support for the in-place assignment operators: +=, -=, *=, and /=. These modify the original value, so we need to mark self as being an mut argument and update the re and im fields instead of returning a new Complex instance. And once again, we'll overload the definitions to support both a Complex and a Float64 operand.
    # ...
    def __iadd__(mut self, rhs: Self) -> None:
        self.re += rhs.re
        self.im += rhs.im

    def __iadd__(mut self, rhs: Float64) -> None:
        self.re += rhs

    def __isub__(mut self, rhs: Self) -> None:
        self.re -= rhs.re
        self.im -= rhs.im

    def __isub__(mut self, rhs: Float64) -> None:
        self.re -= rhs

    def __imul__(mut self, rhs: Self) -> None:
        new_re = self.re * rhs.re - self.im * rhs.im
        new_im = self.re * rhs.im + self.im * rhs.re
        self.re = new_re
        self.im = new_im

    def __imul__(mut self, rhs: Float64) -> None:
        self.re *= rhs
        self.im *= rhs

    def __itruediv__(mut self, rhs: Self) -> None:
        denom = rhs.squared_norm()
        new_re = (self.re * rhs.re + self.im * rhs.im) / denom
        new_im = (self.im * rhs.re - self.re * rhs.im) / denom
        self.re = new_re
        self.im = new_im

    def __itruediv__(mut self, rhs: Float64) -> None:
        self.re /= rhs
        self.im /= rhs
And now to try them out.
c4 = Complex(-1, -1)
print("c4 =", c4)
c4 += Complex(0.5, -0.5)
print("c4 += Complex(0.5, -0.5) =>", c4)
c4 += 2.75
print("c4 += 2.75 =>", c4)
c4 -= Complex(0.25, 1.5)
print("c4 -= Complex(0.25, 1.5) =>", c4)
c4 -= 3
print("c4 -= 3 =>", c4)
c4 *= Complex(-3.0, 2.0)
print("c4 *= Complex(-3.0, 2.0) =>", c4)
c4 *= 0.75
print("c4 *= 0.75 =>", c4)
c4 /= Complex(1.25, 2.0)
print("c4 /= Complex(1.25, 2.0) =>", c4)
c4 /= 2.0
print("c4 /= 2.0 =>", c4)
c4 = (-1.0 - 1.0i)
c4 += Complex(0.5, -0.5) => (-0.5 - 1.5i)
c4 += 2.75 => (2.25 - 1.5i)
c4 -= Complex(0.25, 1.5) => (2.0 - 3.0i)
c4 -= 3 => (-1.0 - 3.0i)
c4 *= Complex(-3.0, 2.0) => (9.0 + 7.0i)
c4 *= 0.75 => (6.75 + 5.25i)
c4 /= Complex(1.25, 2.0) => (3.404494382022472 - 1.247191011235955i)
c4 /= 2.0 => (1.702247191011236 - 0.6235955056179775i)
Implement equality operators​
The field of complex numbers is not an ordered field, so it doesn't make sense for us to implement the Comparable trait and the >, >=, <, and <= operators. However, we can implement the Equatable trait and the == and != operators. (Of course, this suffers the same limitation of comparing floating point numbers for equality because of the limited precision of representing floating point numbers when performing arithmetic operations. But we'll go ahead and implement the operators for completeness.)
struct Complex(
    Equatable,
    Formattable,
    Stringable,
):
    # ...
    fn __eq__(self, other: Self) -> Bool:
        return self.re == other.re and self.im == other.im

    fn __ne__(self, other: Self) -> Bool:
        return self.re != other.re or self.im != other.im
The Equatable trait doesn't allow the __eq__() and __ne__() methods to raise errors. Because defining a method with def implies that it can raise an error, we instead have to define these methods with fn. See Functions for more information on the differences between defining functions with def and fn.
And now to try them out.
c1 = Complex(-1.2, 6.5)
c3 = Complex(3.14159, -2.71828)
c5 = Complex(-1.2, 6.5)

if c1 == c5:
    print("c1 is equal to c5")
else:
    print("c1 is not equal to c5")

if c1 != c3:
    print("c1 is not equal to c3")
else:
    print("c1 is equal to c3")
c1 is equal to c5
c1 is not equal to c3
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit



Edit this page


Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit


Control flow
Mojo includes several traditional control flow structures for conditional and repeated execution of code blocks.
The if statement​
Mojo supports the if statement for conditional code execution. With it you can conditionally execute an indented code block if a given boolean expression evaluates to True.
temp_celsius = 25
if temp_celsius > 20:
    print("It is warm.")
    print("The temperature is", temp_celsius * 9 / 5 + 32, "Fahrenheit." )
It is warm.
The temperature is 77.0 Fahrenheit.
You can write the entire if statement as a single line if all you need to execute conditionally is a single, short statement.
temp_celsius = 22
if temp_celsius < 15: print("It is cool.") # Skipped because condition is False
if temp_celsius > 20: print("It is warm.")
It is warm.
Optionally, an if statement can include any number of additional elif clauses, each specifying a boolean condition and associated code block to execute if True. The conditions are tested in the order given. When a condition evaluates to True, the associated code block is executed and no further conditions are tested.
Additionally, an if statement can include an optional else clause providing a code block to execute if all conditions evaluate to False.
temp_celsius = 25
if temp_celsius <= 0:
    print("It is freezing.")
elif temp_celsius < 20:
    print("It is cool.")
elif temp_celsius < 30:
    print("It is warm.")
else:
    print("It is hot.")
It is warm.
TODO
Mojo currently does not support the equivalent of a Python match or C switch statement for pattern matching and conditional execution.
Short-circuit evaluation​
Mojo follows short-circuit evaluation semantics for boolean operators. If the first argument to an or operator evaluates to True, the second argument is not evaluated.
def true_func() -> Bool:
    print("Executing true_func")
    return True

def false_func() -> Bool:
    print("Executing false_func")
    return False

print('Short-circuit "or" evaluation')
if true_func() or false_func():
    print("True result")
Short-circuit "or" evaluation
Executing true_func
True result
If the first argument to an and operator evaluates to False, the second argument is not evaluated.
print('Short-circuit "and" evaluation')
if false_func() and true_func():
    print("True result")
Short-circuit "and" evaluation
Executing false_func
Conditional expressions​
Mojo also supports conditional expressions (or what is sometimes called a ternary conditional operator) using the syntaxtrue_result if boolean_expression else false_result, just as in Python. This is most often used as a concise way to assign one of two different values to a variable, based on a boolean condition.
temp_celsius = 15
forecast = "warm" if temp_celsius > 20 else "cool"
print("The forecast for today is", forecast)
The forecast for today is cool
The alternative, written as a multi-line if statement, is more verbose.
if temp_celsius > 20:
    forecast = "warm"
else:
    forecast = "cool"
print("The forecast for today is", forecast)
The forecast for today is cool
The while statement​
The while loop repeatedly executes a code block while a given boolean expression evaluates to True. For example, the following loop prints values from the Fibonacci series that are less than 50.
fib_prev = 0
fib_curr = 1

print(fib_prev, end="")
while fib_curr < 50:
    print(",", fib_curr, end="")
    fib_prev, fib_curr = fib_curr, fib_prev + fib_curr
0, 1, 1, 2, 3, 5, 8, 13, 21, 34
A continue statement skips execution of the rest of the code block and resumes with the loop test expression.
n = 0
while n < 5:
    n += 1
    if n == 3:
        continue
    print(n, end=", ")
1, 2, 4, 5,
A break statement terminates execution of the loop.
n = 0
while n < 5:
    n += 1
    if n == 3:
        break
    print(n, end=", ")
1, 2,
Optionally, a while loop can include an else clause. The body of the else clause executes when the loop's boolean condition evaluates to False, even if it occurs the first time tested.
n = 5

while n < 4:
    print(n)
    n += 1
else:
    print("Loop completed")

Loop completed
The else clause does not execute if a break or return statement exits the while loop.
n = 0
while n < 5:
    n += 1
    if n == 3:
        break
    print(n)
else:
    print("Executing else clause")
1
2
The for statement​
The for loop iterates over a sequence, executing a code block for each element in the sequence. The Mojo for loop can iterate over any type that implements an __iter__() method that returns a type that defines __next__() and __len__() methods.
Iterating over Mojo collections​
All of the collection types in the collections module support for loop iteration. See the Collection types documentation for more information on Mojo collection types.
The following shows an example of iterating over a Mojo List.
states = ["California", "Hawaii", "Oregon"]
for state in states:
    print(state)
California
Hawaii
Oregon
The same technique works for iterating over a Mojo Set.
from collections import Set

values = {42, 0}
for item in values:
    print(item)
42
0
There are two techniques for iterating over a Mojo Dict. The first is to iterate directly using the Dict, which produces a sequence of the dictionary's keys.
var capitals: Dict[String, String] = {
    "California": "Sacramento",
    "Hawaii": "Honolulu",
    "Oregon": "Salem"
}

for var state in capitals:
    print(capitals[state] + ", " + state)
Sacramento, California
Honolulu, Hawaii
Salem, Oregon
The second approach to iterating over a Mojo Dict is to invoke its items() method, which produces a sequence of DictEntry objects. Within the loop body, you can then access the key and value fields of the entry.
for item in capitals.items():
    print(item.value + ", " + item.key)
Sacramento, California
Honolulu, Hawaii
Salem, Oregon
Iterating using references​
The Mojo collection iterators all return references, which are captured immutably into the loop variable. If you'd like to get a reference to a mutable element, add the ref keyword in front of the loop variable to create a reference binding that matches the element reference.
This can be useful if you want to mutate the value in the collection:
var values = [1, 4, 7, 3, 6, 11]
for ref value in values:
    if value % 2 != 0:
        value -= 1
print(values.__str__())
[0, 4, 6, 2, 6, 10]
Iterating ranges​
Another type of iterable provided by the Mojo standard library is a range, which is a sequence of integers generated by the range() function. It differs from the collection types shown above in that it's implemented as a generator, producing each value as needed rather than materializing the entire sequence in memory. For example:
for i in range(5):
    print(i, end=", ")
0, 1, 2, 3, 4,
for loop control statements​
A continue statement skips execution of the rest of the code block and resumes the loop with the next element of the collection.
for i in range(5):
    if i == 3:
        continue
    print(i, end=", ")
0, 1, 2, 4,
A break statement terminates execution of the loop.
for i in range(5):
    if i == 3:
        break
    print(i, end=", ")
0, 1, 2,
Optionally, a for loop can include an else clause. The body of the else clause executes after iterating over all of the elements in a collection.
for i in range(5):
    print(i, end=", ")
else:
    print("\nFinished executing 'for' loop")
0, 1, 2, 3, 4,
Finished executing 'for' loop
The else clause executes even if the collection is empty.
from collections import List

var empty: List[Int] = []
for i in empty:
    print(i)
else:
    print("Finished executing 'for' loop")
Finished executing 'for' loop
The else clause does not execute if a break or return statement terminates the for loop.
from collections import List

animals = ["cat", "aardvark", "hippopotamus", "dog"]
for animal in animals:
    if animal == "dog":
        print("Found a dog")
        break
else:
    print("No dog found")
Found a dog
Iterating over Python collections​
The Mojo for loop supports iterating over Python collection types. Each item retrieved by the loop is a PythonObject wrapper around the Python object. Refer to the Python types documentation for more information on manipulating Python objects from Mojo.
The following is a simple example of iterating over a mixed-type Python list.
from python import Python

def main():
    # Create a mixed-type Python list
    py_list = Python.list(42, "cat", 3.14159)
    for py_obj in py_list:  # Each element is of type "PythonObject"
        print(py_obj)
42
cat
3.14159
There are two techniques for iterating over a Python dictionary. The first is to iterate directly using the dictionary, which produces a sequence of its keys.
from python import Python

def main():
    # Create a mixed-type Python dictionary
    py_dict = Python.evaluate("{'a': 1, 'b': 2.71828, 'c': 'sushi'}")
    for py_key in py_dict:  # Each key is of type "PythonObject"
        print(py_key, py_dict[py_key])
a 1
b 2.71828
c sushi
The second approach to iterating over a Python dictionary is to invoke its items() method, which produces a sequence of 2-tuple objects. Within the loop body, you can then access the key and value by index.
from python import Python

def main():
    # Create a mixed-type Python dictionary
    py_dict = Python.evaluate("{'a': 1, 'b': 2.71828, 'c': 'sushi'}")
    for py_tuple in py_dict.items():  # Each 2-tuple is of type "PythonObject"
        print(py_tuple[0], py_tuple[1])
a 1
b 2.71828
c sushi
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Edit this page


Edit this page



Errors, error handling, and context managers
This page discusses how to raise errors in Mojo programs and how to detect and handle error conditions. It also discusses how you can use context managers to correctly allocate and release resources such as files, even when error conditions occur. Finally, it shows you how to implement context managers for your own custom resources.
Raise an error​
The raise statement raises an error condition in your program. You provide the raise statement with an Error instance to indicate the type of error that occurred. For example:
raise Error("integer overflow")
As a convenience, you can instead provide an error message in the form of a String or StringLiteral value, and raise automatically uses that to create an Error instance. So you can raise the same error condition as shown above by executing:
raise "integer overflow"
Currently, Mojo does not support typed error conditions. All errors are instances of Error, and the only thing that distinguishes different error conditions is the error message that you provide.
An error interrupts the execution flow of your program. If you provide an error handler (as described in Handle an error) in the current function, execution resumes with that handler. If the error isn't handled in the current function, it propagates to the calling function and so on. If an error isn't caught by any error handler, your program terminates with a non-zero exit code and prints a stack trace, if enabled, followed by the error message. For example:
stack trace was not collected. Enable stack trace collection with environment variable `MOJO_ENABLE_STACK_TRACE_ON_ERROR`
Unhandled exception caught during execution: integer overflow
Enable stack trace generation for errors​
By default, Mojo generates a stack trace when your program hits a segmentation fault. If you don't want this behavior, you can disable it by setting the MOJO_ENABLE_STACK_TRACE_ON_CRASH environment variable to 0 or false (case-insensitive).
However, Mojo doesn't generate a stack trace when your program raises an error-we skip this to avoid the additional run-time overhead. If you want stack traces for raised errors, you can enable them by setting the MOJO_ENABLE_STACK_TRACE_ON_ERROR environment variable to any value other than 0 or false (case-insensitive).
Keep in mind that when you compile your program with mojo build, the compiler optimizes and strips symbols by default, so often your stack trace won't be very useful.
Let's look at this program:
stacktrace_error.mojo
def func2() -> None:
    raise Error("Intentional error")


def func1() -> None:
    func2()


def main():
    func1()
If you compile the program with the default settings and run it with the environment variable set, you'll see a stack trace without symbols:
mojo build stacktrace_error.mojo
MOJO_ENABLE_STACK_TRACE_ON_ERROR=1 stacktrace_error
#0 0x0000000104ef8ecc llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/Users/ken/tmp/stack/.pixi/envs/default/lib/libKGENCompilerRTShared.dylib+0xccecc)
#1 0x0000000104e379e4 KGEN_CompilerRT_GetStackTrace (/Users/ken/tmp/stack/.pixi/envs/default/lib/libKGENCompilerRTShared.dylib+0xb9e4)
#2 0x000000010478868c main (/Users/ken/tmp/stack/stacktrace_error+0x10000068c)

Unhandled exception caught during execution: Intentional error
To generate a more useful stack trace, you need to compile the program with --debug-level full (or -g) to include debug symbols:
mojo build --debug-level full stacktrace_error.mojo
MOJO_ENABLE_STACK_TRACE_ON_ERROR=1 ./stacktrace_error
#0 0x0000000102bf4ecc llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/Users/ken/tmp/stack/.pixi/envs/default/lib/libKGENCompilerRTShared.dylib+0xccecc)
#1 0x0000000102b339e4 KGEN_CompilerRT_GetStackTrace (/Users/ken/tmp/stack/.pixi/envs/default/lib/libKGENCompilerRTShared.dylib+0xb9e4)
#2 0x000000010266468c stdlib::builtin::error::Error::__init__[__mlir_type.!kgen.string](::StringLiteral[$0])_REMOVED_ARG open-source/max/mojo/stdlib/stdlib/builtin/error.mojo:159:38
#3 0x000000010266468c stacktrace_error::func2()_REMOVED_ARG /Users/ken/tmp/stack/stacktrace_error.mojo:14:16
#4 0x000000010266468c stacktrace_error::func1() /Users/ken/tmp/stack/stacktrace_error.mojo:18:10
#5 0x000000010266468c stacktrace_error::main() /Users/ken/tmp/stack/stacktrace_error.mojo:22:10
#6 0x000000010266468c stdlib::builtin::_startup::__wrap_and_execute_raising_main[fn() raises -> None](::SIMD[::DType(int32), ::Int(1)],__mlir_type.!kgen.pointer<pointer<scalar<ui8>>>),main_func="stacktrace_error::main()" open-source/max/mojo/stdlib/stdlib/builtin/_startup.mojo:88:18
#7 0x000000010266468c main open-source/max/mojo/stdlib/stdlib/builtin/_startup.mojo:103:4

Unhandled exception caught during execution: Intentional error
At this time, running your program directly with mojo run or mojo doesn't include debug symbols in the stack trace even with --debug-level full. For example:
MOJO_ENABLE_STACK_TRACE_ON_ERROR=1 mojo --debug-level full stacktrace_error.mojo
#0 0x000000013a9c4ecc llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/Users/ken/tmp/stack/.pixi/envs/default/lib/libKGENCompilerRTShared.dylib+0xccecc)
#1 0x000000013a9039e4 KGEN_CompilerRT_GetStackTrace (/Users/ken/tmp/stack/.pixi/envs/default/lib/libKGENCompilerRTShared.dylib+0xb9e4)
#2 0x000000032002807c
#3 0x000000010488a0f4 M::KGEN::ExecutionEngine::runProgram(llvm::StringRef, llvm::StringRef, llvm::function_ref<M::ErrorOrSuccess (void*)>) (/Users/ken/tmp/stack/.pixi/envs/default/bin/mojo+0x10039e0f4)
#4 0x0000000104512000 executeMain(M::KGEN::ExecutionEngine&, M::AsyncRT::Runtime&, llvm::ArrayRef<char const*>) (/Users/ken/tmp/stack/.pixi/envs/default/bin/mojo+0x100026000)
#5 0x00000001045118e8 run(M::State const&) (/Users/ken/tmp/stack/.pixi/envs/default/bin/mojo+0x1000258e8)
#6 0x000000010451a240 main (/Users/ken/tmp/stack/.pixi/envs/default/bin/mojo+0x10002e240)
#7 0x0000000186d7ab98

Unhandled exception caught during execution: Intentional error
mojo: error: execution exited with a non-zero result: 1
As described in Handle an error, you can bind the Error instance to a variable in the except clause. If you do, you can invoke its get_stack_trace() method to get a StackTrace instance. StackTrace implements the Stringable trait, so you can construct a String with String(stack_trace) if you want to extract the stack trace as a String for further processing. For example:
def func2() -> None:
    raise Error("Intentional error")


def func1() -> None:
    func2()


def main():
    try:
        func1()
    except e:
        print(e)
        print("-" * 20)
        print(String(e.get_stack_trace()))
When you compile this program with symbols and run it with stack trace generation enabled, you'll see the following output:
mojo build --debug-level full stacktrace_error_capture.mojo
MOJO_ENABLE_STACK_TRACE_ON_ERROR=1 ./stacktrace_error_capture
Intentional error
--------------------
#0 0x0000000102d24ecc llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/Users/ken/tmp/stack/.pixi/envs/default/lib/libKGENCompilerRTShared.dylib+0xccecc)
#1 0x0000000102c639e4 KGEN_CompilerRT_GetStackTrace (/Users/ken/tmp/stack/.pixi/envs/default/lib/libKGENCompilerRTShared.dylib+0xb9e4)
#2 0x00000001026d4694 stdlib::builtin::error::Error::__init__[__mlir_type.!kgen.string](::StringLiteral[$0])_REMOVED_ARG open-source/max/mojo/stdlib/stdlib/builtin/error.mojo:159:38
#3 0x00000001026d4694 stacktrace_error_capture::func2()_REMOVED_ARG /Users/ken/tmp/stack/stacktrace_error_capture.mojo:14:16
#4 0x00000001026d4694 stacktrace_error_capture::func1() /Users/ken/tmp/stack/stacktrace_error_capture.mojo:18:10
#5 0x00000001026d4694 stacktrace_error_capture::main() /Users/ken/tmp/stack/stacktrace_error_capture.mojo:23:14
#6 0x00000001026d4694 stdlib::builtin::_startup::__wrap_and_execute_raising_main[fn() raises -> None](::SIMD[::DType(int32), ::Int(1)],__mlir_type.!kgen.pointer<pointer<scalar<ui8>>>),main_func="stacktrace_error_capture::main()" open-source/max/mojo/stdlib/stdlib/builtin/_startup.mojo:88:18
#7 0x00000001026d4694 main open-source/max/mojo/stdlib/stdlib/builtin/_startup.mojo:103:4
Without enabling stack trace generation, the output is:
Intentional error
--------------------
stack trace was not collected. Enable stack trace collection with environment variable `MOJO_ENABLE_STACK_TRACE_ON_ERROR`
Declare a raising function​
A function defined using the fn keyword is non-raising by default. So if it can raise an error, you must include the raises keyword in the function definition. For example:
fn incr(n: Int) raises -> Int:
    if n == Int.MAX:
        raise "inc: integer overflow"
    else:
        return n + 1
If you don't include the raises keyword on an fn function, then the function must explicitly handle any errors that might occur in the code it executes. For example:
# This function doesn't compile because of the unhandled error
fn unhandled_error(n: Int):
    print(n, "+ 1 =", incr(n))

# This function compiles because it handles the possible error
fn handled_error(n: Int):
    try:
        print(n, "+ 1 =", incr(n))
    except e:
        print("Handled an error:", e)
In contrast, a def function is raising by default. So the following incr() function is equivalent to the incr() function defined above with fn:
def incr(n: Int) -> Int:
    if n == Int.MAX:
        raise "inc: integer overflow"
    else:
        return n + 1
Handle an error​
Mojo allows you to detect and handle error conditions using the try-except control flow structure. The full syntax is:
try:
    # Code block to execute that might raise an error
except <optional_variable_name>:
    # Code block to execute if an error occurs
else:
    # Code block to execute if no error occurs
finally:
    # Final code block to execute in all circumstances
You must include one or both of the except and finally clauses. The else clause is optional.
The try clause contains a code block to execute that might raise an error. If no error occurs, the entire code block executes. If an error occurs, execution of the code block stops at the point that the error is raised. Your program then continues with the execution of the except clause, if provided, or the finally clause.
If the except clause is present, its code block executes only if an error occurred in the try clause. The except clause "consumes" the error that occurred in the try clause. You can then implement any error handling or recovery that's appropriate for your application.
If you provide the name of a variable after the except keyword, then the Error instance is bound to the variable if an error occurs. The Error type implements the Writable trait, so you can pass it as an argument to the print() function if you'd like to print its error message to the console. It also implements the Stringable trait, so you can construct a String with String(error) if you want to extract the error message as a String for further processing.
If desired, you can re-raise an error condition from your except clause simply by executing a raise statement from within its code block. This can be either a new Error instance or, if you provided a variable name to capture the Error that occurred originally, you can re-raise that error.
Because Mojo does not currently support typed errors, a try-except control structure can include at most one except clause, which catches any Error raised.
If the else clause is present, its code block executes only if an error does not occur in the try clause. Note that the else clause is skipped if the try clause executes a continue, break, or return that exits from the try block.
If the finally clause is present, its code block executes after the try clause and the except or else clause, if applicable. The finally clause executes even if one of the other code blocks exits by executing a continue, break, or return statement or by raising an error. The finally clause is often used to release resources used by the try clause (such as a file handle) regardless of whether an error occurred.
As an example, consider the following program:
handle_error.mojo
def incr(n: Int) -> Int:
    if n == Int.MAX:
        raise "inc: integer overflow"
    else:
        return n + 1

def main():
    for value in [0, 1, Int.MAX]:
        try:
            print()
            print("try     =>", value)
            if value == 1:
                continue
            result = "{} incremented is {}".format(value, incr(value))
        except e:
            print("except  =>", e)
        else:
            print("else    =>", result)
        finally:
            print("finally => ====================")
Running this program generates the following output:
try     => 0
else    => 0 incremented is 1
finally => ====================

try     => 1
finally => ====================

try     => 9223372036854775807
except  => inc: integer overflow
finally => ====================
Use a context manager​
A context manager is an object that manages resources such as files, network connections, and database connections. It provides a way to allocate resources and release them automatically when they are no longer needed, ensuring proper cleanup and preventing resource leaks even in the case of error conditions.
As an example, consider reading data from a file. A naive approach might look like this:
# Obtain a file handle to read from storage
f = open(input_file, "r")
content = f.read()
# Process the content as needed
# Close the file handle
f.close()
Calling close() releases the memory and other operating system resources associated with the opened file. If your program were to open many files without closing them, you could exhaust the resources available to your program and cause errors. The problem is even worse if you were writing to a file instead of reading from it, because the operating system might buffer the output in memory until the file is closed. If your program were to crash instead of exiting normally, that buffered data could be lost instead of being written to storage.
The example above actually includes the call to close(), but it ignores the possibility that read() could raise an error, which would prevent the close() from executing. To handle this scenario, you could rewrite the code to use try like this:
# Obtain a file handle to read from storage
f = open(input_file, "r")

try:
    content = f.read()
    # Process the content as needed
finally:
    # Ensure that the file handle is closed even if read() raises an error
    f.close()
However, the FileHandle struct returned by open() is a context manager. When used with Mojo's with statement, a context manager ensures that the resources it manages are properly released at the end of the block, even if an error occurs. In the case of a FileHandle, that means the call to close() takes place automatically. So you could rewrite the example above to take advantage of the context manager (and omit the explicit call to close()) like this:
with open(input_file, "r") as f:
    content = f.read()
    # Process the content as needed
The with statement also allows you to use multiple context managers within the same code block. As an example, the following code opens one text file, reads its entire content, converts it to upper case, and then writes the result to a different file:
with open(input_file, "r") as f_in, open(output_file, "w") as f_out:
    input_text = f_in.read()
    output_text = input_text.upper()
    f_out.write(output_text)
FileHandle is perhaps the most commonly used context manager. Other examples of context managers in the Mojo standard library are NamedTemporaryFile, TemporaryDirectory, BlockingScopedLock, and assert_raises. You can also create your own custom context managers, as described in Write a custom context manager below.
Write a custom context manager​
Writing a custom context manager is a matter of defining a struct that implements two special dunder methods ("double underscore" methods): __enter__() and __exit__():
* __enter__() is called by the with statement to enter the runtime context. The __enter__() method should initialize any state necessary for the context and return the context manager.
* __exit__() is called when the with code block completes execution, even if the with code block terminates with a call to continue, break, or return. The __exit__() method should release any resources associated with the context. After the __exit__() method returns, the context manager is destroyed.
If the with code block raises an error, then the __exit__() method runs before any error processing occurs (that is, before it is caught by a try-except structure or your program terminates). If you'd like to define conditional processing for error conditions in a with code block, you can implement an overloaded version of __exit__() that takes an Error argument. For more information, see Define a conditional __exit__() method below.
For context managers that don't need to release resources or perform other actions on termination, you are not required to implement an __exit__() method. In that case the context manager is destroyed automatically after the with code block completes execution.
Here is an example of implementing a Timer context manager, which prints the amount of time spent executing the with code block:
context_mgr.mojo
import sys
import time

@fieldwise_init
struct Timer(ImplicitlyCopyable, Movable):
    var start_time: Int

    fn __init__(out self):
        self.start_time = 0

    fn __enter__(mut self) -> Self:
        self.start_time = Int(time.perf_counter_ns())
        return self

    fn __exit__(mut self):
        end_time = time.perf_counter_ns()
        elapsed_time_ms = round(((end_time - UInt(self.start_time)) / 1e6), 3)
        print("Elapsed time:", elapsed_time_ms, "milliseconds")

def main():
    with Timer():
        print("Beginning execution")
        time.sleep(1.0)
        if len(sys.argv()) > 1:
            raise "simulated error"
        time.sleep(1.0)
        print("Ending execution")
Running this example produces output like this:
mojo context_mgr.mojo
Beginning execution
Ending execution
Elapsed time: 2010.0 milliseconds
mojo context_mgr.mojo fail
Beginning execution
Elapsed time: 1002.0 milliseconds
Unhandled exception caught during execution: simulated error
Define a conditional __exit__() method​
When creating a context manager, you can implement the __exit__(self) form of the __exit__() method to handle completion of the with statement under all circumstances including errors. However, you have the option of additionally implementing an overloaded version that is invoked instead when an error occurs in the with code block:
fn __exit__(self, error: Error) raises -> Bool
Given the Error that occurred as an argument, the method can do any of the following actions:
* Return True to suppress the error
* Return False to re-raise the error
* Raise a new error
The following is an example of a context manager that suppresses only a certain type of error condition and propagates all others:
conditional_context_mgr.mojo
import time

@fieldwise_init
struct ConditionalTimer(ImplicitlyCopyable, Movable):
    var start_time: Int

    fn __init__(out self):
        self.start_time = 0

    fn __enter__(mut self) -> Self:
        self.start_time = Int(time.perf_counter_ns())
        return self

    fn __exit__(mut self):
        end_time = time.perf_counter_ns()
        elapsed_time_ms = round(((end_time - UInt(self.start_time)) / 1e6), 3)
        print("Elapsed time:", elapsed_time_ms, "milliseconds")

    fn __exit__(mut self, e: Error) raises -> Bool:
        if String(e) == "just a warning":
            print("Suppressing error:", e)
            self.__exit__()
            return True
        else:
            print("Propagating error")
            self.__exit__()
            return False

def flaky_identity(n: Int) -> Int:
    if (n % 4) == 0:
        raise "really bad"
    elif (n % 2) == 0:
        raise "just a warning"
    else:
        return n

def main():
    for i in range(1, 9):
        with ConditionalTimer():
            print("\nBeginning execution")

            print("i =", i)
            time.sleep(0.1)

            if i == 3:
                print("continue executed")
                continue

            j = flaky_identity(i)
            print("j =", j)

            print("Ending execution")
Running this example produces this output:
Beginning execution
i = 1
j = 1
Ending execution
Elapsed time: 105.0 milliseconds

Beginning execution
i = 2
Suppressing error: just a warning
Elapsed time: 106.0 milliseconds

Beginning execution
i = 3
continue executed
Elapsed time: 106.0 milliseconds

Beginning execution
i = 4
Propagating error
Elapsed time: 106.0 milliseconds
Unhandled exception caught during execution: really bad
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Edit this page

Edit this page


Structs
A Mojo struct is a data structure that allows you to encapsulate fields and methods that operate on an abstraction, such as a data type or an object. Fields are variables that hold data relevant to the struct, and methods are functions inside a struct that generally act upon the field data.
For example, if you're building a graphics program, you can use a struct to define an Image that has fields to store information about each image (such as the pixels) and methods that perform actions on it (such as rotate it).
For the most part, Mojo's struct format is designed to provide a static, memory-safe data structure for high-level data types used in programs. For example, all the data types in Mojo's standard library (such as Int, Bool, String, and Tuple) are defined as structs.
If you understand how functions and variables work in Mojo, you probably noticed that Mojo is designed to provide dynamic programming features in a def function while enforcing stronger code safety in fn functions. When it comes to structs, Mojo leans toward the safe side: You can still choose whether to use either def or fn declarations for methods, but all fields must be declared with var.
Struct definition​
You can define a simple struct called MyPair with two fields like this:
struct MyPair:
    var first: Int
    var second: Int
However, you can't instantiate this struct because it has no constructor method. So here it is with a constructor to initialize the two fields:
struct MyPair:
    var first: Int
    var second: Int

    fn __init__(out self, first: Int, second: Int):
        self.first = first
        self.second = second
Notice that the first argument in the __init__() method is out self. You'll have a self argument as the first argument on all struct methods. It references the current struct instance (it allows code in the method to refer to "itself"). When you call the constructor, you never pass a value for self-Mojo passes it in automatically.
The out portion of out self is an argument convention that declares self as a mutable reference that starts out as uninitialized and must be initialized before the function returns.
Many types use a field-wise constructor like the one shown for MyPair above: it takes an argument for each field, and initializes the fields directly from the arguments. To save typing, Mojo provides a @fieldwise_init decorator, which generates a field-wise constructor for the struct. So you can rewrite the MyPair example above like this:
@fieldwise_init
struct MyPair:
    var first: Int
    var second: Int
The __init__() method is one of many special methods (also known as "dunder methods" because they have double underscores) with pre-determined names.
You can't assign values when you declare fields. You must initialize all of the struct's fields in the constructor. (If you try to leave a field uninitialized, the code won't compile.)
Once you have a constructor, you can create an instance of MyPair and set the fields:
var mine = MyPair(2, 4)
print(mine.first)
2
Making a struct copyable and movable​
Mojo structs are not copyable or movable by default.
For example, the following code produces errors:
var a = MyPair(1, 2)

# Implicit copy
var b = a  # value of type 'MyPair' is not implicitly copyable, it does not
           # conform to 'ImplicitlyCopyable'

# Explicit copy
var c = a.copy()  # 'MyPair' value has no attribute 'copy'

# Move
var d = a^  #  value of type 'MyPair' cannot be copied or moved; consider
            # conforming it to 'Movable'

In most cases, you can make a struct copyable and movable just by adding the appropriate traits.
Copyability​
To make a struct copyable, add the Copyable trait:
struct MyPair(Copyable):
  ...
In most cases, that's all you need to do. Mojo will generate a copy constructor ( __copyinit__() method) for you. You don't need to write your own unless you need custom logic in the copy constructor-for example, if your struct dynamically allocates memory. For more information, see the section on copy constructors.
The Copyable trait also supplies the copy() method, which provides a more user-friendly way to copy a value than invoking the copy constructor directly.
Implicit copyability​
To make a struct implicitly copyable, add the ImplicitlyCopyable trait:
struct MyPair(ImplicitlyCopyable):
    ...
ImplicitlyCopyable automatically implies Copyable, so all the notes related to copyability apply here. A type should only be implicitly copyable if copying the type is inexpensive and has no side effects. Unnecessary copies can be a big drain on memory and performance, so use this trait with caution.
Movability​
To make a struct movable, add the Movable trait:
struct MyPair(Copyable, Movable):
...
Mojo will generate a move constructor for you. You have rarely need to write your own custom move constructor. For more information, see the section on move constructors.
A movable, copyable MyPair​
Here's a movable and implicitly copyable version of MyPair:
@fieldwise_init
struct MyPair(ImplicitlyCopyable, Movable):
    var first: Int
    var second: Int
Since the MyPair struct is a simple data structure that holds two Int values, there's no harm in making it implicitly copyable.
And here's how to exercise the copy and move constructors:
var original_pair = MyPair(2, 6)
var copied_pair = original_pair  # implicit copy
var another_pair = original_pair.copy()  # explicit copy
var moved_pair = original_pair^  # move
Methods​
In addition to special methods like __init__(), you can add any other method you want to your struct. For example:
@fieldwise_init
struct MyPair:
    var first: Int
    var second: Int

    fn get_sum(self) -> Int:
        return self.first + self.second
var mine = MyPair(6, 8)
print(mine.get_sum())
14
Notice that get_sum() also uses the self argument, because this is the only way you can access the struct's fields in a method. The name self is just a convention, and you can use any name you want to refer to the struct instance that is always passed as the first argument.
Methods that take the implicit self argument are called instance methods because they act on an instance of the struct.
The self argument in a struct method is the only argument in an fn function that does not require a type. You can include it if you want, but you can elide it because Mojo already knows its type (MyPair in this case).
fn versus def in struct methods​
Struct methods can be declared with either the def or fn keywords. One important difference is that an fn function without the raises keyword can't raise an error. When you call a function that can raise an error from inside a method that can't raise an error, Mojo requires you to handle any errors, as described in Errors, error handling, and context managers.
If you're writing code that you expect to use widely or distribute as a package, you may want to use fn functions for APIs that can't raise an error to limit the number of places users need to add error handling code.
A struct's __del__() method, or destructor, must be a non-raising method, so it's always declared with fn (and without the raises keyword).
Static methods​
A struct can also have static methods. A static method can be called without creating an instance of the struct. Unlike instance methods, a static method doesn't receive the implicit self argument, so it can't access any fields on the struct.
To declare a static method, use the @staticmethod decorator and don't include a self argument:
struct Logger:

    fn __init__(out self):
        pass

    @staticmethod
    fn log_info(message: String):
        print("Info: ", message)
You can invoke a static method by calling it on the type (in this case, Logger). You can also call it on an instance of the type. Both forms are shown below:
Logger.log_info("Static method called.")
var l = Logger()
l.log_info("Static method called from instance.")
Info:  Static method called.
Info:  Static method called from instance.
Structs compared to classes​
If you're familiar with other object-oriented languages, then structs might sound a lot like classes, and there are some similarities, but also some important differences. Eventually, Mojo will also support classes to match the behavior of Python classes.
So, let's compare Mojo structs to Python classes. They both support methods, fields, operator overloading, decorators for metaprogramming, and more, but their key differences are as follows:
* Python classes are dynamic: they allow for dynamic dispatch, monkey-patching (or "swizzling"), and dynamically binding instance fields at runtime.
* Mojo structs are static: they are bound at compile-time (you cannot add methods at runtime). Structs allow you to trade flexibility for performance while being safe and easy to use.
* Mojo structs do not support inheritance ("sub-classing"), but a struct can implement traits.
* Python classes support class attributes-values that are shared by all instances of the class, equivalent to class variables or static data members in other languages.
* Mojo structs don't support static data members.
Syntactically, the biggest difference compared to a Python class is that all fields in a struct must be explicitly declared with var.
In Mojo, the structure and contents of a struct are set at compile time and can't be changed while the program is running. Unlike in Python, where you can add, remove, or change attributes of an object on the fly, Mojo doesn't allow that for structs.
However, the static nature of structs helps Mojo run your code faster. The program knows exactly where to find the struct's information and how to use it without any extra steps or delays at runtime.
Mojo's structs also work really well with features you might already know from Python, like operator overloading (which lets you change how math symbols like + and - work with your own data, using special methods).
As mentioned above, all Mojo's standard types (Int, String, etc.) are made using structs, rather than being hardwired into the language itself. This gives you more flexibility and control when writing your code, and it means you can define your own types with all the same capabilities (there's no special treatment for the standard library types).
Special methods​
Special methods (or "dunder methods") such as __init__() are pre-determined method names that you can define in a struct to perform a special task.
Although it's possible to call special methods with their method names, the point is that you never should, because Mojo automatically invokes them in circumstances where they're needed (which is why they're also called "magic methods"). For example, Mojo calls the __init__() method when you create an instance of the struct; and when Mojo destroys the instance, it calls the __del__() method (if it exists).
Even operator behaviors that appear built-in (+, <, ==, |, and so on) are implemented as special methods that Mojo implicitly calls upon to perform operations or comparisons on the type that the operator is applied to.
Mojo supports a long list of special methods; far too many to discuss here, but they generally match all of Python's special methods and they usually accomplish one of two types of tasks:
* Operator overloading: A lot of special methods are designed to overload operators such as < (less-than), + (add), and | (or) so they work appropriately with each type. For more information, see Implement operators for custom types.
* Lifecycle event handling: These special methods deal with the lifecycle and value ownership of an instance. For example, __init__() and __del__() demarcate the beginning and end of an instance lifetime, and other special methods define the behavior for other lifecycle events such as how to copy or move a value.
You can learn all about the lifecycle special methods in the Value lifecycle section. However, most structs are simple aggregations of other types, so unless your type requires custom behaviors when an instance is created, copied, moved, or destroyed, you can synthesize the essential lifecycle methods you need (and save yourself some time) using the @fieldwise_init decorator (described in Struct definition), and the Copyable and Movable traits (described in Making a struct copyable and movable).
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit



Edit this page

Modules and packages
Mojo provides a packaging system that allows you to organize and compile code libraries into importable files. This page introduces the necessary concepts about how to organize your code into modules and packages (which is a lot like Python), and shows you how to create a packaged binary with the mojo package command.
Mojo modules​
To understand Mojo packages, you first need to understand Mojo modules. A Mojo module is a single Mojo source file that includes code suitable for use by other files that import it. For example, you can create a module to define a struct such as this one:
mymodule.mojo
struct MyPair:
    var first: Int
    var second: Int

    fn __init__(out self, first: Int, second: Int):
        self.first = first
        self.second = second

    fn dump(self):
        print(self.first, self.second)
Notice that this code has no main() function, so you can't execute mymodule.mojo. However, you can import this into another file with a main() function and use it there.
For example, here's how you can import MyPair into a file named main.mojo that's in the same directory as mymodule.mojo:
main.mojo
from mymodule import MyPair

fn main():
    var mine = MyPair(2, 4)
    mine.dump()
Alternatively, you can import the whole module and then access its members through the module name. For example:
main.mojo
import mymodule

fn main():
    var mine = mymodule.MyPair(2, 4)
    mine.dump()
You can also create an alias for an imported member with as, like this:
main.mojo
import mymodule as my

fn main():
    var mine = my.MyPair(2, 4)
    mine.dump()
In this example, it only works when mymodule.mojo is in the same directory as main.mojo. Currently, you can't import .mojo files as modules if they reside in other directories. That is, unless you treat the directory as a Mojo package, as described in the next section.
A Mojo module may include a main() function and may also be executable, but that's generally not the practice and modules typically include APIs to be imported and used in other Mojo programs.
Mojo packages​
A Mojo package is just a collection of Mojo modules in a directory that includes an __init__.mojo file. By organizing modules together in a directory, you can then import all the modules together or individually. Optionally, you can also compile the package into a .mojopkg or .📦 file that's easier to share and still compatible with other system architectures.
You can import a package and its modules either directly from source files or from a compiled .mojopkg/.📦 file. It makes no real difference to Mojo which way you import a package. When importing from source files, the directory name works as the package name, whereas when importing from a compiled package, the filename is the package name (which you specify with the mojo package command-it can differ from the directory name).
For example, consider a project with these files:
main.mojo
mypackage/
    __init__.mojo
    mymodule.mojo
mymodule.mojo is the same code from examples above (with the MyPair struct) and __init__.mojo is empty.
The __init__.mojo file is essential. If you don't have it, Mojo won't recognize the directory as a package and you can't import mymodule.
In this case, the main.mojo file can now import MyPair through the package name like this:
main.mojo
from mypackage.mymodule import MyPair

fn main():
    var mine = MyPair(2, 4)
    mine.dump()
This immediately works:
mojo main.mojo
2 4
However, if you don't want the mypackage source code in the same location as main.mojo, you can compile it into a package file like this:
mojo package mypackage -o mypack.mojopkg
A .mojopkg file contains non-elaborated code, so you can share it across systems. The code becomes an architecture-specific executable only after it's imported into a Mojo program that's then compiled with mojo build.
Now, you can move the mypackage source somewhere else, and the project files now look like this:
main.mojo
mypack.mojopkg
Because we named the package mypack, we need to fix the import statement:
main.mojo
from mypack.mymodule import MyPair
And the code works the same:
mojo main.mojo
2 4
If you want to rename your package, you cannot simply edit the .mojopkg or .📦 filename, because the package name is encoded in the file. You must instead run mojo package again to specify a new name.
The __init__ file​
As mentioned above, the __init__.mojo file is required to indicate that a directory should be treated as a Mojo package, and it can be empty.
Currently, top-level code is not supported in .mojo files, so unlike Python, you can't write code in __init__.mojo that executes upon import. You can, however, add structs and functions, which you can then import from the package name.
However, instead of adding APIs in the __init__.mojo file, you can import module members, which has the same effect by making your APIs accessible from the package name, instead of requiring the <package_name>.<module_name> notation.
For example, again let's say you have these files:
main.mojo
mypackage/
    __init__.mojo
    mymodule.mojo
Let's now add the following line in __init__.mojo:
__init__.mojo
from .mymodule import MyPair
That's all that's in there. Now, we can simplify the import statement in main.mojo like this:
main.mojo
from mypackage import MyPair
This feature explains why some members in the Mojo standard library can be imported from their package name, while others required the <package_name>.<module_name> notation. For example, the functional module resides in the algorithm package, so you can import members of that module (such as the map() function) like this:
from algorithm.functional import map
However, the algorithm/__init__.mojo file also includes these lines:
algorithm/__init__.mojo
from .functional import *
from .reduction import *
So you can actually import anything from functional or reduction simply by naming the package. That is, you can drop the functional name from the import statement, and it also works:
from algorithm import map
Which modules in the standard library are imported to the package scope varies, and is subject to change. Refer to the documentation for each module to see how you can import its members.Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit


Intro to value ownership
A program is nothing without data, and all modern programming languages store data in one of two places: the call stack and the heap (also sometimes in CPU registers, but we won't get into that here). However, each language reads and writes data a bit differently-sometimes very differently. So in the following sections, we'll explain how Mojo manages memory in your programs and how this affects the way you write Mojo code.
For an alternate introduction to ownership in Mojo, check out our two-part blog post: What ownership is really about: a mental model approach, and Deep dive into ownership in Mojo.
Stack and heap overview​
In general, all modern programming languages divide a running program's memory into four segments:
* Text. The compiled program.
* Data. Global data, either initialized or uninitialized.
* Stack. Local data, automatically managed during the program's runtime.
* Heap. Dynamically-allocated data, managed by the programmer.
The text and data segments are statically sized, but the stack and heap change size as the program runs.
The stack stores data local to the current function. When a function is called, the program allocates a block of memory-a stack frame-that is exactly the size required to store the function's data, including any fixed-size local variables. When another function is called, a new stack frame is pushed onto the top of the stack. When a function is done, its stack frame is popped off the stack.
Notice that we said only "fixed-size local values" are stored in the stack. Dynamically-sized values that can change in size at runtime are instead stored in the heap, which is a much larger region of memory that allows for dynamic memory allocation. Technically, a local variable for such a value is still stored in the call stack, but its value is a fixed-size pointer to the real value on the heap. Consider a Mojo string: it can be any length, and its length can change at runtime. So the Mojo String struct includes some statically-sized fields, plus a pointer to a dynamically-allocated buffer holding the actual string data.
Another important difference between the heap and the stack is that the stack is managed automatically-the code to push and pop stack frames is added by the compiler. Heap memory, on the other hand, is managed by the programmer explicitly allocating and deallocating memory. You may do this indirectly-by using standard library types like List and String-or directly, using the UnsafePointer API.
Values that need to outlive the lifetime of a function (such as an array that's passed between functions and should not be copied) are stored in the heap, because heap memory is accessible from anywhere in the call stack, even after the function that created it is removed from the stack. This sort of situation-in which a heap-allocated value is used by multiple functions-is where most memory errors occur, and it's where memory management strategies vary the most between programming languages.
Memory management strategies​
Because memory is limited, it's important that programs remove unused data from the heap ("free" the memory) as quickly as possible. Figuring out when to free that memory is pretty complicated.
Some programming languages try to hide the complexities of memory management from you by utilizing a "garbage collector" process that tracks all memory usage and deallocates unused heap memory periodically (also known as automatic memory management). A significant benefit of this method is that it relieves developers from the burden of manual memory management, generally avoiding more errors and making developers more productive. However, it incurs a performance cost because the garbage collector interrupts the program's execution, and it might not reclaim memory very quickly.
Other languages require that you manually free data that's allocated on the heap. When done properly, this makes programs execute quickly, because there's no processing time consumed by a garbage collector. However, the challenge with this approach is that programmers make mistakes, especially when multiple parts of the program need access to the same memory-it becomes difficult to know which part of the program "owns" the data and must deallocate it. Programmers might accidentally deallocate data before the program is done with it (causing "use-after-free" errors), or they might deallocate it twice ("double free" errors), or they might never deallocate it ("leaked memory" errors). Mistakes like these and others can have catastrophic results for the program, and these bugs are often hard to track down, making it especially important that they don't occur in the first place.
Mojo uses a third approach called "ownership" that relies on a collection of rules that programmers must follow when passing values. The rules ensure there is only one "owner" for a given value at a time. When a value's lifetime ends, Mojo calls its destructor, which is responsible for deallocating any heap memory that needs to be deallocated.
In this way, Mojo helps ensure memory is freed, but it does so in a way that's deterministic and safe from errors such as use-after-free, double-free and memory leaks. Plus, it does so with a very low performance overhead.
Mojo's value ownership model provides an excellent balance of programming productivity and strong memory safety. It only requires that you learn some new syntax and a few rules about how to share access to memory within your program.
But before we explain the rules and syntax for Mojo's value ownership model, you first need to understand value semantics.Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit


Value semantics
Mojo doesn't enforce value semantics or reference semantics. It supports them both and allows each type to define how it is created, copied, and moved (if at all). So, if you're building your own type, you can implement it to support value semantics, reference semantics, or a bit of both. That said, Mojo is designed with argument behaviors that default to value semantics, and it provides tight controls for reference semantics that avoid memory errors.
The controls over reference semantics are provided by the value ownership model, but before we get into the syntax and rules for that, it's important that you understand the principles of value semantics. Generally, it means that each variable has unique access to a value, and any code outside the scope of that variable cannot modify its value.
Intro to value semantics​
In the most basic situation, sharing a value-semantic type means that you create a copy of the value. This is also known as "pass by value." For example, consider this code:
def main():
    var x = 1
    var y = x
    y += 1

    print("x:", x)
    print("y:", y)
x: 1
y: 2
We assigned the value of x to y, which creates the value for y by making a copy of x. When we increment y, the value of x doesn't change. Each variable has exclusive ownership of a value.
Whereas, if a type instead uses reference semantics, then y would point to the same value as x, and incrementing either one would affect the value for both. Neither x nor y would "own" the value, and any variable would be allowed to reference it and mutate it.
Numeric values in Mojo are value semantic because they're trivial types, which are cheap to copy.
Value semantics in Mojo functions​
Value semantics also apply to function arguments in Mojo by default. However, the way in which they apply differs depending on the argument convention, which is discussed in the Ownership page.
For example, in the following function, the y argument is immutable by default, so if the function wants to modify the value in the local scope, it needs to make a local copy:
fn add_two(y: Int):
    # y += 2  # This would cause a compiler error because `y` is immutable
    # We can instead make an explicit copy:
    var z = y
    z += 2
    print("z:", z)

def main():
    var x = 1
    add_two(x)
    print("x:", x)
z: 3
x: 1
This is all consistent with value semantics because each variable maintains unique ownership of its value.
The way the function receives the y value is a "look but don't touch" approach to value semantics. This is also a more memory-efficient approach when dealing with memory-intensive arguments, because Mojo doesn't make any copies unless we explicitly make the copies ourselves.
Thus, the default behavior for function arguments is fully value semantic: arguments are immutable references, and any living variable from the caller is not affected by the function.
But we must also allow reference semantics (mutable references) because it's how we build performant and memory-efficient programs (making copies of everything gets really expensive). The challenge is to introduce reference semantics in a way that does not disturb the predictability and safety of value semantics.
The way we do that in Mojo is, instead of enforcing that every variable have "exclusive access" to a value, we ensure that every value has an "exclusive owner," and destroy each value when the lifetime of its owner ends.
On the next page about value ownership, you'll learn how to modify the default argument conventions, and safely use reference semantics so every value has only one owner at a time.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Edit this page

Edit this page


Ownership
A challenge you might face when using some programming languages is that you must manually allocate and deallocate memory. When multiple parts of the program need access to the same memory, it becomes difficult to keep track of who "owns" a value and determine when is the right time to deallocate it. If you make a mistake, it can result in a "use-after-free" error, a "double free" error, or a "leaked memory" error, any one of which can be catastrophic.
Mojo helps avoid these errors by ensuring there is only one variable that owns each value at a time, while still allowing you to share references with other functions. When the life span of the owner ends, Mojo destroys the value. Programmers are still responsible for making sure any type that allocates resources (including memory) also deallocates those resources in its destructor. Mojo's ownership system ensures that destructors are called promptly.
On this page, we'll explain the rules that govern this ownership model, and how to specify different argument conventions that define how values are passed into functions.
Ownership summary​
The fundamental rules that make Mojo's ownership model work are the following:
* Every value has only one owner at a time.
* When the lifetime of the owner ends, Mojo destroys the value.
* If there are existing references to a value, Mojo extends the lifetime of the owner.
Variables and references​
A variable owns its value. A struct owns its fields.
A reference allows you to access a value owned by another variable. A reference can have either mutable access or immutable access to that value.
Mojo references are created when you call a function: function arguments can be passed as mutable or immutable references. A function can also return a reference instead of returning a value. To capture a returned reference, you can use a reference binding:
ref value_ref = list[0]
Argument conventions​
In all programming languages, code quality and performance is heavily dependent upon how functions treat argument values. That is, whether a value received by a function is a unique value or a reference, and whether it's mutable or immutable, has a series of consequences that define the readability, performance, and safety of the language.
In Mojo, we want to provide full value semantics by default, which provides consistent and predictable behavior. But as a systems programming language, we also need to offer full control over memory optimizations, which generally requires reference semantics. The trick is to introduce reference semantics in a way that ensures all code is memory safe by tracking the lifetime of every value and destroying each one at the right time (and only once). All of this is made possible in Mojo through the use of argument conventions that ensure every value has only one owner at a time.
An argument convention specifies whether an argument is mutable or immutable, and whether the function owns the value. Each convention is defined by a keyword at the beginning of an argument declaration:
* read: The function receives an immutable reference. This means the function can read the original value (it is not a copy), but it cannot mutate (modify) it.
* mut: The function receives a mutable reference. This means the function can read and mutate the original value (it is not a copy).
* var: The function takes ownership of a value. This means the function has exclusive ownership of the argument. The caller might choose to transfer ownership of an existing value to this function, but that's not always what happens. The callee might receive a newly-created value, or a copy of an existing value.
* ref: The function gets a reference with a parametric mutability: that is, the reference can be either mutable or immutable. You can think of ref arguments as a generalization of the read and mut conventions. ref arguments are an advanced topic, and they're described in more detail in Lifetimes, origins, and references.
* out: A special convention used for the self argument in constructors and for named results. An out argument is uninitialized at the beginning of the function, and must be initialized before the function returns. Although out arguments show up in the argument list, they're never passed in by the caller.
* deinit: A special convention used in the destructor and consuming-move lifecycle methods. A deinit argument is initialized at the beginning of the function, and uninitialized when the function returns.
For example, this function has one argument that's a mutable reference and one that's immutable:
fn add(mut x: Int, read y: Int):
    x += y

fn main():
    var a = 1
    var b = 2
    add(a, b)
    print(a)
3
You've probably already seen some function arguments that don't declare a convention. By default, all arguments are read. In the following sections, we'll explain each of these argument conventions in more detail.
Immutable arguments (read)​
The read convention is the default for all arguments. The callee receives an immutable reference to the argument value.
For example:
def print_list(list: List[Int]):
    print(list.__str__())

def main():
    var values = [1, 2, 3, 4]
    print_list(values)
[1, 2, 3, 4]
Here the print_list() function can read from the list argument, but not mutate it. list is a reference to values in the main() function, not a copy.
In general, passing an immutable reference is much more efficient when handling large or expensive-to-copy values, because the copy constructor and destructor are not invoked for a read argument.
Compared to C++ and Rust​
Mojo's read argument convention is similar in some ways to passing an argument by const& in C++, which also avoids a copy of the value and disables mutability in the callee. However, the read convention differs from const& in C++ in two important ways:
* The Mojo compiler implements a lifetime checker that ensures that values are not destroyed when there are outstanding references to those values.
* Small values like Int, Float, and SIMD are always passed in machine registers-instead of through an extra indirection or optimized at every call site-because these types are declared with the @register_passable decorator. This is a significant performance enhancement compared to languages like C++ and Rust.
The major difference between Rust and Mojo is that Mojo does not require a sigil on the caller side to pass by immutable reference. Also, Mojo is more efficient when passing small values, and Rust defaults to moving values instead of passing them around by borrow. These policy and syntax decisions allow Mojo to provide an easier-to-use programming model.
Mutable arguments (mut)​
If you'd like your function to receive a mutable reference, add the mut keyword in front of the argument name. You can think of mut like this: it means any changes to the value inside the function are visible outside the function.
For example, this mutate() function updates the original list value:
def print_list(list: List[Int]):
    print(list.__str__())

def mutate(mut l: List[Int]):
    l.append(5)

def main():
    var values = [1, 2, 3, 4]

    mutate(values)
    print_list(values)
[1, 2, 3, 4, 5]
That behaves like an optimized replacement for this:
def print_list(list: List[Int]):
    print(list.__str__())

def mutate_copy(l: List[Int]) -> List[Int]:
    # def creates an implicit copy of the list because it's mutated
    l.append(5)
    return l

def main():
    var values = [1, 2, 3, 4]

    values = mutate_copy(values)
    print_list(values)
[1, 2, 3, 4, 5]
Although the code using mut isn't that much shorter, it's more memory efficient because it does not make a copy of the value.
However, remember that the values passed as mut must already be mutable. For example, if you try to take a read value and pass it to another function as mut, you'll get a compiler error because Mojo can't form a mutable reference from an immutable reference.
You cannot define default values for mut arguments.
Argument exclusivity​
Mojo enforces argument exclusivity for mutable references. This means that if a function receives a mutable reference to a value (such as an mut argument), it can't receive any other references to the same value-mutable or immutable. That is, a mutable reference can't have any other references that alias it.
For example, consider the following code example:
fn append_twice(mut s: String, other: String):
   # Mojo knows 's' and 'other' cannot be the same string.
   s += other
   s += other

fn invalid_access():
  var my_string = "o"  # Create a run-time String value

  # error: passing `my_string` mut is invalid since it is also passed
  # read.
  append_twice(my_string, my_string)
  print(my_string)
This code is confusing because the user might expect the output to be ooo, but since the first addition mutates both s and other, the actual output would be oooo. Enforcing exclusivity of mutable references not only prevents coding errors, it also allows the Mojo compiler to optimize code in some cases.
One way to avoid this issue when you do need both a mutable and an immutable reference (or need to pass the same value to two arguments) is to make a copy:
fn valid_access():
  var my_string = "o"           # Create a run-time String value
  var other_string = my_string  # Create a copy of the String value
  append_twice(my_string, other_string)
  print(my_string)
Note that argument exclusivity isn't enforced for register-passable trivial types (like Int and Bool), because they are always passed by copy. When passing the same value into two Int arguments, the callee will receive two copies of the value.
Transfer arguments (var and ^)​
And finally, if you'd like your function to receive value ownership, add the var keyword in front of the argument name.
This convention is often combined with use of the postfixed ^ "transfer" sigil on the variable that is passed into the function, which ends the lifetime of that variable.
Technically, the var keyword does not guarantee that the received value is the original value-it guarantees only that the function gets unique ownership of a value. This happens in one of three ways:
* The caller passes the argument with the ^ transfer sigil, which ends the lifetime of that variable (the variable becomes uninitialized) and ownership is transferred into the function.
* The caller does not use the ^ transfer sigil, in which case, Mojo copies the value. If the type isn't copyable, this is a compile-time error.
* The caller passes in a newly-created "owned" value, such as a value returned from a function. In this case, no variable owns the value and it can be transferred directly to the callee. For example:
def take(var s: String):
    pass

def main():
    take("A brand-new String!")
The following code works by making a copy of the string, because take_text() uses the var convention, and the caller does not include the transfer sigil:
fn take_text(var text: String):
    text += "!"
    print(text)

fn main():
    var message = "Hello"  # Create a run-time String value
    take_text(message)
    print(message)
Hello!
Hello
However, if you add the ^ transfer sigil when calling take_text(), the compiler complains about print(message), because at that point, the message variable is no longer initialized. That is, this version does not compile:
fn main():
    var message = "Hello"  # Create a run-time String value
    take_text(message^)
    print(message)  # error: use of uninitialized value 'message'
This is a critical feature of Mojo's lifetime checker, because it ensures that no two variables can have ownership of the same value. To fix the error, you must not use the message variable after you end its lifetime with the ^ transfer sigil. So here is the corrected code:
fn take_text(var text: String):
    text += "!"
    print(text)

fn main():
    var message = "Hello"  # Create a run-time String value
    take_text(message^)
Hello!
Regardless of how it receives the value, when the function declares an argument as var, it can be certain that it has unique mutable access to that value. Because the value is owned, the value is destroyed when the function exits-unless the function transfers the value elsewhere.
For example, in the following example, add_to_list() takes a string and appends it to the list. Ownership of the string is transferred to the list, so it's not destroyed when the function exits. On the other hand, consume_string() doesn't transfer its var value out, so the value is destroyed at the end of the function.
def add_to_list(var name: String, mut list: List[String]):
    list.append(name^)
    # name is uninitialized, nothing to destroy

def consume_string(var s: String):
    print(s)
    # s is destroyed here
Transfer implementation details​
In Mojo, you shouldn't conflate "ownership transfer" with a "move operation"-these are not strictly the same thing.
There are multiple ways that Mojo can transfer ownership of a value:
* If a type implements the move constructor, __moveinit__(), Mojo may invoke this method if a value of that type is transferred into a function as a var argument, and the original variable's lifetime ends at the same point (with or without use of the ^ transfer sigil).
* If a type implements the copy constructor, __copyinit__() and not __moveinit__(), Mojo may copy the value and destroy the old value.
* In some cases, Mojo can optimize away the move operation entirely, leaving the value in the same memory location but updating its ownership. In these cases, a value can be transferred without invoking either the __copyinit__() or __moveinit__() constructors.
In order for the var convention to work without the transfer sigil, the value type must be copyable (via __copyinit__()).
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit
Edit this page

Edit this page


Lifetimes, origins, and references
The Mojo compiler includes a lifetime checker, a compiler pass that analyzes dataflow through your program. It identifies when variables are valid and inserts destructor calls when a variable's lifetime ends.
The Mojo compiler uses a special value called an origin to track the lifetime of variables and the validity of references.
Specifically, an origin answers two questions:
* What variable "owns" this value?
* Can the value be mutated using this reference?
For example, consider the following code:
def print_str(s: String):
    print(s)

def main():
    name: String = "Joan"
    print_str(name)
Joan
The line name = "Joan" declares a variable with an identifier (name) and logical storage space for a String value. When you pass name into the print_str() function, the function gets an immutable reference to the value. So both name and s refer to the same logical storage space, and have associated origin values that lets the Mojo compiler reason about them.
Origin tracking and lifetime checking is done at compile time, so origins don't track the actual storage space allocated for the name variable, for example. Instead, origins track variables symbolically, so the compiler tracks that print_str() is called with a value owned by name in the caller's scope. By tracking how owned data flows through the program, the compiler can identify the lifetimes of values.
Most of the time, origins are handled automatically by the compiler. However, in some cases you'll need to interact with origins directly:
* When working with references-specifically ref arguments and ref return values.
* When working with types like Pointer or Span which are parameterized on the origin of the data they refer to.
This section also covers ref arguments and ref return values, which let functions take arguments and provide return values as references with parametric origins.
Working with origins​
Mojo's origin values are mostly created by the compiler, so you can't just create your own origin value-you usually need to derive an origin from an existing value.
Among other things, Mojo uses origins to extend the lifetimes of referenced values, so values aren't destroyed prematurely.
Origin types​
Mojo supplies a struct and a set of aliases that you can use to specify origin types. As the names suggest, the ImmutOrigin and MutOrigin aliases represent immutable and mutable origins, respectively:
struct ImmutRef[origin: ImmutOrigin]:
    pass
Or you can use the Origin struct to specify an origin with parametric mutability:
struct ParametricRef[
    is_mutable: Bool,
    //,
    origin: Origin[is_mutable]
]:
    pass
Origin types carry the mutability of a reference as a boolean parameter value, indicating whether the origin is mutable, immutable, or even with mutability depending on a parameter specified by the enclosing API.
The is_mutable parameter here is an infer-only parameter. The origin value is often inferred, as well. For example, the following code creates a Pointer to an existing value, but doesn't need to specify an origin-the origin is inferred from the existing value.
from memory import Pointer

def use_pointer():
    a = 10
    ptr = Pointer(to=a)
Origin sets​
An OriginSet is not a type of origin, it represents a group of origins. Origin sets are used for tracking the lifetimes of values captured in closures.
Origin values​
Most origin values are created by the compiler. As a developer, there are a few ways to specify origin values:
* Static origin. The StaticConstantOrigin alias is an origin value representing immutable values that last for the duration of the program. String literal values have a StaticConstantOrigin.
* Derived origin. The origin_of() magic function returns the origin associated with the value (or values) passed in.
* Inferred origin. You can use inferred parameters to capture the origin of a value passed in to a function.
* External origins. The external origins, MutOrigin.external and ImmutOrigin.external represent values that are not tracked by the lifetime checker, such as dynamically-allocated memory.
* Wildcard origins. The ImmutAnyOrigin and MutAnyOrigin aliases are special cases indicating a reference that might access any live value.
Static origins​
You can use the static origin StaticConstantOrigin when you have a value that exists for the entire duration of the program.
For example, the StringLiteral method as_string_slice() returns a StringSlice pointing to the original string literal. String literals are static-they're allocated at compile time and never destroyed-so the slice is created with an immutable, static origin.
Derived origins​
Use the origin_of(value) operator to obtain a value's origin. An argument to origin_of() can take an arbitrary expression that yields one of the following:
* An origin value.
* A value with a memory location.
For example:
origin_of(self)
origin_of(x.y)
origin_of(foo())
The origin_of() operator is analyzed statically at compile time; The expressions passed to origin_of() are never evaluated. (For example, when the compiler analyzes origin_of(foo()), it doesn't run the foo() function.)
The following struct stores a string value using a OwnedPointer: a smart pointer that holds an owned value. The as_ptr() method returns a Pointer to the stored string, using the same origin as the original OwnedPointer.
from memory import OwnedPointer, Pointer

struct BoxedString:
    var o_ptr: OwnedPointer[String]

    fn __init__(out self, value: String):
        self.o_ptr = OwnedPointer(value)

    fn as_ptr(mut self) -> Pointer[String, origin_of(self.o_ptr)]:
        return Pointer(to=self.o_ptr[])
Note that the as_ptr() method takes its self argument as mut self. If it used the default read argument convention, it would be immutable, and the derived origin (origin_of(self.o_ptr)) would also be immutable.
You can also pass multiple expressions to origin_of() to express the union of two or more origins:
origin_of(a, b)
Origin unions​
The union of two or more origins creates a new origin that references all of the original origins for the purposes of lifetime extension (so a union of the origins of a and b extends both lifetimes).
An origin union is mutable if and only if all of its constituent origins are mutable. For an example, see Return values with union origins.
Inferred origins​
Since origins are parameters, the compiler can infer an origin value from an the argument passed to a function or method, as described in Parameter inference. This allows a function to return a value that has the same origin as the argument passed to it.
See the section on ref arguments for an example using an inferred origin.
External origins​
The external origins, MutOrigin.external and ImmutOrigin.external represent values that do not alias any existing value. That is, they point to memory that is not owned by any other variable, and are therefore not tracked by the lifetime checker. For example, the alloc() function returns an UnsafePointer to a new dynamically-allocated block of memory, with the origin MutOrigin.external. The origin indicates that the memory is not managed by the Mojo ownership system. When you use an unsafe API like this, you're responsible for managing the lifetime yourself: for example, a struct that allocates memory should generally free that memory in its destructor.
Wildcard origins​
The wildcard origins, ImmutAnyOrigin and MutAnyOrigin, are special cases indicating a reference that might access any live value. These were previously widely used for unsafe pointers. Using a pointer with a wildcard origin into a scope effectively disables Mojo's ASAP destruction for any values in that scope, as long as the pointer is live. Accordingly, the use of wildcard origins is discouraged, and should be used as a last resort.
Working with references​
You can use the ref keyword with arguments and return values to specify a reference with parametric mutability. That is, they can be either mutable or immutable.
From inside the called function, a ref argument looks like a read or mut argument.
A ref return value looks like any other return value to the calling function, but it is a reference to an existing value, not a copy.
ref arguments​
The ref argument convention lets you specify an argument of parametric mutability: that is, you don't need to know in advance whether the passed argument will be mutable or immutable. There are several reasons you might want to use a ref argument:
* You want to accept an argument with parametric mutability.
* You want to tie the lifetime of one argument to the lifetime of another argument.
* When you want an argument that is guaranteed to be passed in memory: this can be important and useful for generic arguments that need an identity, irrespective of whether the concrete type is register passable.
The syntax for a ref argument is:
ref arg_name: arg_type 
Or:
ref [origin_specifier(s)] arg_name: arg_type 
In the first form, the origin and mutability of the ref argument is inferred from the value passed in. The second form includes an origin clause, consisting of one or more origin specifiers inside square brackets. An origin specifier can be either:
* An origin value.
* An arbitrary expression, which is treated as shorthand for origin_of(expression). In other words, the following declarations are equivalent:
ref [origin_of(self)]
ref [self]
* An AddressSpace value.
* An underscore character (_) to indicate that the origin is unbound. This is equivalent to omitting the origin specifier.
def add_ref(ref a: Int, b: Int) -> Int:
  return a+b
You can also name the origin explicitly. This is useful if you want to restrict the argument to either a ImmutOrigin or MutOrigin, or if you want to bind a function's return value to the origin of an argument.
For example, the Span type is a non-owning view of contiguous data (like a substring of a string, or a subset of a list). Because it points to data that it doesn't own, it is parameterized on an origin value that represents the lifetime and ownership of the data it points to.
In the following example, the to_byte_span() function takes a List[Byte] and returns a Span[Byte] with the same origin as the list:
from collections import List
from memory import Span

def to_byte_span[
    is_mutable: Bool,
    //,
    origin: Origin[is_mutable],
](ref [origin]list: List[Byte]) -> Span[Byte, origin]:
    return Span(list)

def main():
    list: List[Byte] = [77, 111, 106, 111]
    span = to_byte_span(list)
In this example, the origin parameter is inferred from the list argument, and then used as the origin for the returned Span.
Since the Span takes on the origin of the list argument, the Mojo compiler can identify the span's data as owned by the list. The span will have the same lifetime as the list, and the span will be mutable if the list is mutable.
ref return values​
Like ref arguments, ref return values allow a function to return a mutable or immutable reference to a value. The syntax for a ref return value is:
-> ref [origin_specifier(s)] arg_type 
Note that you must provide an origin specifier for a ref return value. The values allowed for origin specifiers are the same as the ones listed for ref arguments.
ref return values can be an efficient way to handle updating items in a collection. The standard way to do this is by implementing the __getitem__() and __setitem__() dunder methods. These are invoked to read from and write to a subscripted item in a collection:
value = list[a]
list[b] += 10
With a ref argument, __getitem__() can return a mutable reference that can be modified directly. This has pros and cons compared to using a __setitem__() method:
* The mutable reference is more efficient-a single update isn't broken up across two methods. However, the referenced value must be in memory.
* A __getitem__()/__setitem__() pair allows for arbitrary code to be run when values are retrieved and set. For example, __setitem__() can validate or constrain input values.
For example, in the following example, NameList has a __getitem__() method that returns a reference:
struct NameList:
    var names: List[String]

    def __init__(out self, *names: String):
        self.names = []
        for name in names:
            self.names.append(name)

    def __getitem__(ref self, index: Int) ->
        ref [self.names] String:
        if (index >=0 and index < len(self.names)):
            return self.names[index]
        else:
            raise Error("index out of bounds")

def main():
    list = NameList("Thor", "Athena", "Dana", "Vrinda")
    ref name = list[2]
    print(name)
    name += "?"
    print(list[2])
Dana
Dana?
Note the use of the ref name syntax to create a reference binding.
If you assign a ref return value to a variable, the variable receives a copy of the referenced item. Use a reference binding if you need to capture the reference for future use:
var name_copy = list[2]  # owned copy of list[2]
ref name_ref = list[2]  # reference to list[2]
Parametric mutability of return values​
Another advantage of ref return arguments is the ability to support parametric mutability. For example, recall the signature of the __getitem__() method above:
def __getitem__(ref self, index: Int) ->
    ref [self] String:
Since the origin of the return value is tied to the origin of self, the returned reference will be mutable if the method was called using a mutable reference. The method still works if you have an immutable reference to the NameList, but it returns an immutable reference:
fn pass_immutable_list(list: NameList) raises:
    print(list[2])
    # list[2] += "?" # Error, this list is immutable

def main():
    list = NameList("Sophie", "Jack", "Diana")
    pass_immutable_list(list)
Diana
Without parametric mutability, you'd need to write two versions of __getitem__(), one that accepts an immutable self and another that accepts a mutable self.
Return values with union origins​
A ref return value can include multiple values in its origin specifier, which yields the union of the origins. For example, the following pick_one() function returns a reference to one of the two input strings, with an origin that's a union of both origins.
def pick_one(cond: Bool, ref a: String, ref b: String) -> ref [a, b] String:
    return a if cond else b
Because the compiler can't statically determine which branch will be picked, this function must use the union origin [a, b]. This ensures that the compiler extends the lifetime of both values as long as the returned reference is live.
The returned reference is mutable if both a and b are mutable.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Intro to value lifecycle
So far, we've explained how Mojo allows you to build high-performance code that is memory safe without manually managing memory, using Mojo's ownership model. However, Mojo is designed for systems programming, which often requires manual memory management for custom data types. So, Mojo lets you do that as you see fit. To be clear, Mojo has no reference counter and no garbage collector.
Mojo also has no built-in data types with special privileges. All data types in the standard library (such as Bool, Int, and String) are implemented as structs.
What's great about the Mojo language is that it provides you these low-level tools for systems programming, but within a framework that helps you build things that are safe and easy to use from higher-level programs. That is, you can get under the hood and write all the "unsafe" code you want, but as long as you do so in accordance with Mojo's value semantics, the programmer instantiating your type/object doesn't need to think about memory management at all, and the behavior will be safe and predictable, thanks to value ownership.
In summary, it's the responsibility of the type author to manage the memory and resources for each value type, by implementing specific lifecycle methods, such as the constructor, copy constructor, move constructor, and destructor, as necessary. Mojo doesn't create any constructors by default, although it does add a trivial, no-op destructor for types that don't define their own.
In the following pages, we'll explain exactly how to define these lifecycle methods in accordance with value semantics so your types play nicely with value ownership.
Lifecycles and lifetimes​
First, let's clarify some terminology:
* The "lifecycle" of a value is defined by various dunder methods in a struct. Each lifecycle event is handled by a different method, such as the constructor (__init__()), the destructor (__del__()), the copy constructor (__copyinit__()), and the move constructor (__moveinit__()). All values that are declared with the same type have the same lifecycle.
* The "lifetime" of a variable is defined by the span of time during program execution in which the variable is considered valid. The life of a variable begins when its value is initialized (via __init__(), __copyinit__() or __moveinit__()) and ends when the value is destroyed (__del__()), or consumed in some other way (for example, as part of a __moveinit__() call).
No two values have the exact same lifetime, because every value is created and destroyed at a different point in time (even if the difference is imperceptible).
Origin type
The concept of lifetimes is related to the origin type, a Mojo primitive used to track ownership. For most Mojo programming, you won't need to work with origin values directly. For information, see Lifetimes, origins and references.
The life of a value in Mojo begins when a variable is initialized and continues up until the value is last used, at which point Mojo destroys it. Mojo destroys every value/object as soon as it's no longer used, using an "as soon as possible" (ASAP) destruction policy that runs after every sub-expression. The Mojo compiler takes care of releasing resources after last use when needed.
As you might imagine, keeping track of a value's life can be difficult if a value is shared across functions many times during the life of a program. However, Mojo makes this predictable partly through its value semantics and value ownership (both prerequisite readings for the following sections). The final piece of the puzzle for lifetime management is the value lifecycle: every value (defined in a struct) needs to implement key lifecycle methods that define how a value is created and destroyed.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Life of a value
The life of a value in Mojo begins when a variable is initialized and continues up until the value is last used, at which point Mojo destroys it. This page describes how every value in Mojo is created, copied, and moved. (The next page describes how values are destroyed.)
All data types in Mojo-including basic types in the standard library such as Bool, Int, and String-are defined as structs. This means the creation and destruction of any piece of data follows the same lifecycle rules, and you can define your own data types that work exactly the same way.
Mojo structs don't get any default lifecycle methods, such as a constructor, copy constructor, or move constructor. That means you can define a struct without a constructor, but then you can't instantiate it, and it would be useful only as a sort of namespace for static methods. For example:
struct NoInstances:
    var state: Int

    @staticmethod
    fn print_hello():
        print("Hello world!")
Without a constructor, this cannot be instantiated, so it has no lifecycle. The state field is also useless because it cannot be initialized (Mojo structs do not support default field values-you must initialize them in a constructor).
So the only thing you can do is call the static method:
NoInstances.print_hello()
Hello world!
Constructor​
To create an instance of a Mojo type, it needs the __init__() constructor method. The main responsibility of the constructor is to initialize all fields. For example:
struct MyPet:
    var name: String
    var age: Int

    fn __init__(out self, name: String, age: Int):
        self.name = name
        self.age = age
Now we can create an instance:
var mine = MyPet("Loki", 4)
An instance of MyPet can also be read and destroyed, but it currently can't be copied or moved.
We believe this is a good default starting point, because there are no built-in lifecycle events and no surprise behaviors. You-the type author-must explicitly decide whether and how the type can be copied or moved, by implementing the copy and move constructors.
The pattern shown above-a constructor that takes an argument for each of the struct's fields and initializes the fields directly from the arguments-is called a field-wise constructor. It's a common enough pattern that Mojo includes a @fieldwise_init decorator to synthesize the field-wise constructor. So you can rewrite the previous example like this:
@fieldwise_init
struct MyPet:
    var name: String
    var age: Int
Mojo does not require a destructor to destroy an instance. But in some cases you may need to define a custom destructor to release resources (for example, if a struct dynamically allocates memory using UnsafePointer). We'll discuss that more in Death of a value.
The "constructor" name
In a Python class, object construction happens across both the __new__() and __init__() methods, so the __init__() method is technically just the initializer for attributes (but it's often still called the constructor). However, in a Mojo struct, there is no __new__() method, so we prefer to always call __init__() the constructor.
Overloading the constructor​
Like any other function/method, you can overload the __init__() constructor to initialize the object with different arguments. For example, you might want a default constructor that sets some default values and takes no arguments, and then additional constructors that accept more arguments.
Just be aware that, in order to modify any fields, each constructor must declare the self argument with the out convention. If you want to call one constructor from another, you simply call upon that constructor as you would externally (you don't need to pass self).
For example, here's how you can delegate work from an overloaded constructor:
struct MyPet:
    var name: String
    var age: Int

    fn __init__(out self):
        self.name = ""
        self.age = 0

    fn __init__(out self, name: String):
        self = MyPet()
        self.name = name
Field initialization​
Notice in the previous example that, by the end of each constructor, all fields must be initialized. That's the only requirement in the constructor.
In fact, the __init__() constructor is smart enough to treat the self object as fully initialized even before the constructor is finished, as long as all fields are initialized. For example, this constructor can pass around self as soon as all fields are initialized:
fn use(arg: MyPet):
    pass

struct MyPet:
    var name: String
    var age: Int

    fn __init__(out self, name: String, age: Int, cond: Bool):
        self.name = name
        if cond:
            self.age = age
            use(self)  # Safe to use immediately!

        self.age = age
        use(self)  # Safe to use immediately!
Constructors and implicit conversion​
Mojo supports implicit conversion from one type to another. Implicit conversion can happen when one of the following occurs:
* You assign a value of one type to a variable with a different type.
* You pass a value of one type to a function that requires a different type.
* You return a value of one type from a function that specifies a different return type.
In all cases, implicit conversion is supported when the target type defines a constructor that meets the following criteria:
* Is declared with the @implicit decorator.
* Has a single required, non-keyword argument of the source type.
For example:
var a = Source()
var b: Target = a
Mojo implicitly converts the Source value in a to a Target value if Target defines a matching constructor like this:
struct Target:

    @implicit
    fn __init__(out self, s: Source): ...
With implicit conversion, the assignment above is essentially identical to:
var b = Target(a)
In general, types should only support implicit conversions when the conversion is lossless, and ideally inexpensive. For example, converting an integer to a floating-point number is usually lossless (except for very large positive and negative integers, where the conversion may be approximate), but converting a floating-point number to an integer is very likely to lose information. So Mojo supports implicit conversion from Int to Float64, but not the reverse.
The constructor used for implicit conversion can take optional arguments, so the following constructor would also support implicit conversion from Source to Target:
struct Target:

    @implicit
    fn __init__(out self, s: Source, reverse: Bool = False): ...
Implicit conversion can fail if Mojo can't unambiguously match the conversion to a constructor. For example, if the target type has two overloaded constructors that take different types, and each of those types supports an implicit conversion from the source type, the compiler has two equally-valid paths to convert the values:
struct A:
    @implicit
    fn __init__(out self, s: Source): ...

struct B:
    @implicit
    fn __init__(out self, s: Source): ...

struct OverloadedTarget:
    @implicit
    fn __init__(out self, a: A): ...
    @implicit
    fn __init__(out self, b: B): ...

var t = OverloadedTarget(Source()) # Error: ambiguous call to '__init__': each
                                   # candidate requires 1 implicit conversion
In this case, you can fix the issue by explicitly casting to one of the intermediate types. For example:
var t = OverloadedTarget(A(Source())) # OK
Mojo applies at most one implicit conversion to a variable. For example:
var t: OverloadedTarget = Source() # Error: can't implicitly convert Source
                                   # to Target
Would fail because there's no direct conversion from Source to OverloadedTarget.
For structs with a single field, you can generate an implicit constructor with the @fieldwise_init("implicit") decorator.
@fieldwise_init("implicit")
struct Counter:
    var count: Int

def main():
    var c: Counter = 5  # implicitly converts from Int
Copy constructor​
In Mojo, a value can be copied either explicitly or implicitly:
# Explicit copy
var s = "Test string"
var s2 = s.copy()

# Implicit copy
var i = 15
var i2 = i
To make a struct explicitly copyable, you need to:
* Add the Copyable trait.
* (Optionally) define a custom __copyinit__() method if needed.
If you simply add the Copyable trait, Mojo will generate a default __copyinit__() method for you, which copies each field of the existing value into the new value. The Copyable trait also defines a default copy() method, which provides a more user-friendly way to copy a value than invoking the copy constructor directly.
@fieldwise_init
struct MyPet(Copyable):
    var name: String
    var age: Int
Now this code works to make a copy:
var mine = MyPet("Loki", 4)
var yours = mine.copy()
Technically, you could make a struct with a copy constructor and not add the Copyable trait, but this is not recommended. Mojo would be able to copy the value, but you couldn't use the struct with any generic containers or functions that require the Copyable trait.
The generated copy constructor simply copies each field from the existing value into the new value. For example, if you wrote the __copyinit__() method for MyPet, it would look like this:
fn __copyinit__(out self, existing: Self):
    self.name = existing.name
    self.age = existing.age
This default copy constructor works in most cases, but there are a few cases where you need to define a custom copy constructor:
* One or more of the struct's fields is not Copyable.
* The struct includes a non-owning type (like UnsafePointer), and you want to make a deep copy of the data.
* The struct holds other resources (like file descriptors or network sockets) that need to be managed.
Custom copy constructor​
What makes Mojo's copy behavior different, compared to other languages, is that __copyinit__() is designed to perform a deep copy of all fields in the type (as per value semantics). That is, it copies heap-allocated values, rather than just copying the pointer.
However, the Mojo compiler doesn't enforce this, so it's the type author's responsibility to implement __copyinit__() with value semantics.
For example, here's a new HeapArray type with a custom copy constructor that performs a deep copy:
struct HeapArray(Copyable):
    var data: UnsafePointer[Int, MutOrigin.external]
    var size: Int
    var cap: Int

    fn __init__(out self, size: Int, val: Int):
        self.size = size
        self.cap = size * 2
        self.data = alloc[Int](self.cap)
        for i in range(self.size):
            (self.data + i).init_pointee_copy(val)

    fn __copyinit__(out self, existing: Self):
        # Deep-copy the existing value
        self.size = existing.size
        self.cap = existing.cap
        self.data = alloc[Int](self.cap)
        for i in range(self.size):
            (self.data + i).init_pointee_copy(existing.data[i])
        # The lifetime of `existing` continues unchanged

    fn __del__(deinit self):
        # We must free the heap-allocated data, but
        # Mojo knows how to destroy the other fields
        for i in range(self.size):
            (self.data + i).destroy_pointee()
        self.data.free()

    fn append(mut self, val: Int):
        # Update the array for demo purposes
        if self.size < self.cap:
            (self.data + self.size).init_pointee_copy(val)
            self.size += 1
        else:
            print("Out of bounds")

    fn dump(self):
        # Print the array contents for demo purposes
        print("[", end="")
        for i in range(self.size):
            if i > 0:
                print(", ", end="")
            print(self.data[i], end="")
        print("]")
Notice that __copyinit__() does not copy the UnsafePointer value (doing so would make the copied value refer to the same data memory address as the original value, which is a shallow copy). Instead, we initialize a new UnsafePointer to allocate a new block of memory, and then copy over all the heap-allocated values (this is a deep copy).
Thus, when we copy an instance of HeapArray, each copy has its own set of values on the heap, so changes to one array do not affect the other, as shown here:
fn copies():
    var a = HeapArray(2, 1)
    var b = a.copy()  # Calls the copy constructor
    a.dump()  # Prints [1, 1]
    b.dump()  # Prints [1, 1]

    b.append(2)  # Changes the copied data
    b.dump()  # Prints [1, 1, 2]
    a.dump()  # Prints [1, 1] (the original did not change)
Two other things to note from the __copyinit__() method:
* The existing argument is type Self (capital "S"). Self is an alias for the current type name (HeapArray, in this example). Using this alias is a best practice to avoid any mistakes when referring to the current struct name.
* The existing argument is immutable because the default argument convention is read-this is a good thing because this function should not modify the contents of the value being copied.
In HeapArray, we must use the __del__() destructor to free the heap-allocated data when the HeapArray lifetime ends, but Mojo automatically destroys all other fields when their respective lifetimes end. We'll discuss this destructor more in Death of a value.
If your type doesn't use any pointers for heap-allocated data, then writing the constructor and copy constructor is all boilerplate code that you shouldn't have to write. For most structs that don't manage memory explicitly, you can just add the Copyable trait to your struct definition and Mojo will synthesize the __copyinit__() method.
Mojo also calls upon the copy constructor when a value is passed to a function that takes the argument as var and when the lifetime of the given value does not end at that point. If the lifetime of the value does end there (usually indicated with the transfer sigil ^), then Mojo instead invokes the move constructor.
Implicitly-copyable types​
To make a type implicitly copyable, add the ImplicitlyCopyable trait:
@fieldwise_init
struct MyPair(ImplicitlyCopyable, Movable):
    var first: Int
    var second: Int

def main():
    pair = MyPair(3, 4)
    copy = pair
    print(pair.first, copy.second)
The ImplicitlyCopyable trait inherits from Copyable, so adding ImplicitlyCopyable to your type gives you a copy constructor and a default copy() method. The trait doesn't define any methods of its own: it serves as a signal to the compiler that it can insert calls to __copyinit__() as needed. For example, in the previous example, the compiler might generate code equivalent to:
pair = MyPair(3, 4)
copy = MyPair.__copyinit__(pair)
A type should be implicitly copyable only if copying the type is inexpensive and has no side effects. Unnecessary copies can be a big drain on memory and performance, so use this trait with caution.
In particular, any type that dynamically allocates memory or manages other resources probably shouldn't be implicitly copyable.
Move constructor​
Although copying values provides predictable behavior that matches Mojo's value semantics, copying some data types can be a significant hit on performance.
Mojo uses the move constructor to transfer ownership of a value from one variable to another, without copying its fields. A value that's copyable but doesn't have a move constructor can still be transferred by making a copy and then discarding the original. So in this case, the move constructor serves as an optimization.
To make a type movable:
* Add the Movable trait.
* (Optionally) implement a custom __moveinit__() method.
Note that register-passable values types are automatically movable, and cannot define a custom __moveinit__() method.
Here's a movable version of the MyPet struct:
@fieldwise_init
struct MyPet(Copyable, Movable):
    var name: String
    var age: Int
Here's an example showing how to invoke the move constructor for MyPet:
fn moves():
    var a = MyPet("Bobo", 2)

    print(a.name) # Prints "bobo"

    var b = a^ # the lifetime of `a` ends here

    print(b.name) # prints "bobo"
    # print(a.name)  # ERROR: use of uninitialized value 'a'
If you include the Movable trait and don't define a move constructor, Mojo generates a default move constructor for you. This move constructor simply moves each of the fields to the new instance.
The generated move constructor for MyPet would look like this if you wrote it yourself:
fn __moveinit__(out self, deinit existing: Self):
    self.name = existing.name^
    self.age = existing.age
The move constructor takes its existing argument using the deinit argument convention, which grants exclusive ownership of the value and marks it as destroyed at the end of the function. (For more information on the deinit convention, see Field lifetimes during destruct and move).
The move constructor uses the transfer sigil (^) to indicate that ownership of the name value is being transferred from existing to self. For register-passable types like Int, the transfer sigil is omitted: register-passable types are always treated as movable, but they can't define custom move constructors or destructors, so there's no special logic to run for a move.
At the end of the __moveinit__() method, Mojo immediately invalidates the original variable, preventing any access to it. It does not call the destructor, since that would destroy resources that have been transferred to the new instance. Invalidating the original variable is important to avoid memory errors on heap-allocated data, such as use-after-free and double-free errors.
A move constructor is not required to transfer ownership of a value. If a value is copyable but not movable, Mojo can copy the value and invalidate the original instance. You can learn more in the section about ownership transfer.
If copying a type is expensive, moving it with __moveinit__() is much more efficient. For example, if a type has heap-allocated data, __copyinit__() typically needs to allocate new storage to make a deep copy of the data.
For types without heap-allocated fields, you get no real benefit from the move constructor. Making copies of simple data types on the stack, like integers, floats, and booleans, is very cheap. Yet, if you allow your type to be copied, then there's generally no reason to disallow moves.
Custom move constructor​
In practice, structs very rarely require a custom __moveinit__(). A type might require a custom __moveinit__() if it has a pointer to itself or one of its fields, for example, since the struct's location in memory changes when it's moved.
The __moveinit__() method performs a consuming move: it transfers ownership of a value from one variable to another when the original variable's lifetime ends (also called a "destructive move").
A critical feature of __moveinit__() is that it takes the incoming value as a deinit argument, meaning this method gets unique ownership of the value. Moreover, because this is a dunder method that Mojo calls only when performing a move (during ownership transfer), the existing argument is guaranteed to be a mutable reference to the original value, not a copy (unlike other methods that may declare an argument as var, but might receive the value as a copy if the method is called without the ^ transfer sigil). That is, Mojo calls this move constructor only when the original variable's lifetime actually ends at the point of transfer.
Move-only and immovable types​
To ensure your type can't be implicitly copied, you can make it "move-only" by making it Movable but not Copyable. A move-only type can be passed to other variables and passed into functions with any argument convention (read, mut, and var)-the only catch is that you must use the ^ transfer sigil to end the lifetime of a move-only type when assigning it to a new variable or when passing it as a var argument. The OwnedPointer is an example of a move-only type: because it is designed to provide clear single ownership of a stored value, the OwnedPointer can be moved, but not copied.
In some (rare) cases, you may not want a type to be copyable or movable. The Atomic type is an example of a type that's neither copyable or movable.
Trivial types​
So far, we've talked about values that live in memory, which means they have an identity (an address) that can be passed around among functions (passed "by reference"). This is great for most types, and it's a safe default for large objects with expensive copy operations. However, it's inefficient for tiny things like a single integer or floating point number. We call these types "trivial" because they are just "bags of bits" that should be copied, moved, and destroyed without invoking any custom lifecycle methods.
Trivial types are the most common types that surround us, and from a language perspective, Mojo doesn't need special support for these written in a struct. Usually, these values are so tiny that they should be passed around in CPU registers, not indirectly through memory.
As such, Mojo provides a struct decorator to declare these types of values: @register_passable("trivial"). This decorator tells Mojo that the type should be copyable and movable but that it has no user-defined logic for this (no custom copy constructor or move constructor). It also tells Mojo to pass the value in CPU registers whenever possible, which has clear performance benefits.
You'll see this decorator on types like Int in the standard library:
@register_passable("trivial")
struct Int:
    ...
We expect to use this decorator pervasively on Mojo standard library types, but it is safe to ignore for general application-level code.
For more information, see the @register_passable documentation.
Trivial lifecycle methods​
The use of a decorator to identify trivial types will be phased out in favor of a more granular set of aliases, which are boolean flags set by the compiler:
* The Copyable trait defines the alias __copyinit__is_trivial, which is an optimization hint that guarantees that the value can be copied by its bits from one location to another without causing any side effects.
* The Movable trait defines the alias __moveinit__is_trivial, which is an optimization hint that guarantees that the value can be moved by its bits from one location to another without causing any side effects.
* The AnyType trait defines the alias __del__is_trivial, which is a hint that the __del__() method is a no-op.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Death of a value
As soon as a value/object is no longer used, Mojo destroys it. Mojo does not wait until the end of a code block-or even until the end of an expression-to destroy an unused value. It destroys values using an "as soon as possible" (ASAP) destruction policy that runs after every sub-expression. Even within an expression like a+b+c+d, Mojo destroys the intermediate values as soon as they're no longer needed.
Mojo uses static compiler analysis to find the point where a value is last used. Then, Mojo immediately ends the value's lifetime and calls the __del__() destructor to perform any necessary cleanup for the type.
For example, notice when the __del__() destructor is called for each instance of Balloon:
@fieldwise_init
struct Balloon(Writable):
    var color: String

    fn write_to(self, mut writer: Some[Writer]):
        writer.write(String("a ", self.color, " balloon"))

    fn __del__(deinit self):
        print("Destroyed", String(self))


def main():
    var a = Balloon("red")
    var b = Balloon("blue")
    print(a)
    # a.__del__() runs here for "red" Balloon

    a = Balloon("green")
    # a.__del__() runs immediately because "green" Balloon is never used

    print(b)
    # b.__del__() runs here

a red balloon
Destroyed a red balloon
Destroyed a green balloon
a blue balloon
Destroyed a blue balloon
Notice that each initialization of a value is matched with a call to the destructor, and a is actually destroyed multiple times-once for each time it receives a new value.
Also notice that this __del__() implementation doesn't actually do anything. Most structs don't require a custom destructor, and Mojo automatically adds a no-op destructor if you don't define one.
The __del__() method takes its argument using the deinit argument convention, which indicates that the value is being deinitialized.
Default destruction behavior​
You may be wondering how Mojo can destroy a type without a custom destructor, or why a no-op destructor is useful. If a type is simply a collection of fields, like the Balloon example, Mojo only needs to destroy the fields: Balloon doesn't dynamically allocate memory or use any long-lived resources (like file handles). There's no special action to take when a Balloon value is destroyed.
When a Balloon value is destroyed, the String value in its color field is no longer used, and it is also immediately destroyed.
The String value is a little more complicated. Mojo strings are mutable. The String object has an internal buffer-a List field, which holds the characters that make up the string. A List stores its contents in dynamically allocated memory on the heap, so the string can grow or shrink. The string itself doesn't have any special destructor logic, but when Mojo destroys a string, it calls the destructor for the List field, which de-allocates the memory.
Since String doesn't require any custom destructor logic, it has a no-op destructor: literally, a __del__() method that doesn't do anything. This may seem pointless, but it means that Mojo can call the destructor on any value when its lifetime ends. This makes it easier to write generic containers and algorithms.
Benefits of ASAP destruction​
Similar to other languages, Mojo follows the principle that objects/values acquire resources in a constructor (__init__()) and release resources in a destructor (__del__()). However, Mojo's ASAP destruction has some advantages over scope-based destruction (such as the C++ RAII pattern, which waits until the end of the code scope to destroy values):
* Destroying values immediately at last-use composes nicely with the "move" optimization, which transforms a "copy+del" pair into a "move" operation.
* Destroying values at end-of-scope in C++ is problematic for some common patterns like tail recursion, because the destructor call happens after the tail call. This can be a significant performance and memory problem for certain functional programming patterns, which is not a problem in Mojo, because the destructor call always happens before the tail call.
The Mojo destruction policy is more similar to how Rust and Swift work, because they both have strong value ownership tracking and provide memory safety. One difference is that Rust and Swift require the use of a dynamic "drop flag"-they maintain hidden shadow variables to keep track of the state of your values to provide safety. These are often optimized away, but the Mojo approach eliminates this overhead entirely, making the generated code faster and avoiding ambiguity.
Destructor​
Mojo calls a value's destructor (__del__() method) when the value's lifetime ends (typically the point at which the value is last used). As we mentioned earlier, Mojo provides a default, no-op destructor for all types, so in most cases you don't need to define the __del__() method.
You should define the __del__() method to perform any kind of cleanup the type requires. Usually, that includes freeing memory for any fields where you dynamically allocated memory (for example, via UnsafePointer) and closing any long-lived resources such as file handles.
However, any struct that is just a simple collection of other types does not need to implement the destructor.
For example, consider this simple struct:
@fieldwise_init
struct Balloons:
    var color: String
    var count: Int
There's no need to define the __del__() destructor for this, because it's a simple collection of other types (String and Int), and it doesn't dynamically allocate memory.
Whereas, the following struct must define the __del__() method to free the memory allocated by its UnsafePointer:
struct HeapArray(Writable):
    var data: UnsafePointer[Int, MutOrigin.external]
    var size: Int

    fn __init__(out self, *values: Int):
        self.size = len(values)
        self.data = alloc[Int](self.size)
        for i in range(self.size):
            (self.data + i).init_pointee_copy(values[i])

    fn write_to(self, mut writer: Some[Writer]):
        writer.write("[")
        for i in range(self.size):
            writer.write(self.data[i])
            if i < self.size - 1:
                writer.write(", ")
        writer.write("]")

    fn __del__(deinit self):
        print("Destroying", self.size, "elements")
        for i in range(self.size):
            (self.data + i).destroy_pointee()
        self.data.free()


def main():
    var a = HeapArray(10, 1, 3, 9)
    print(a)
[10, 1, 3, 9]
Destroying 4 elements
The destructor takes its self argument using the deinit argument convention, which grants exclusive ownership of the value and marks it as destroyed at the end of the function. (For more information on the deinit convention, see Field lifetimes during destruct and move).
Note that a pointer doesn't own any values in the memory it points to, so when a pointer is destroyed, Mojo doesn't call the destructors on those values.
So in the HeapArray example above, calling free() on the pointer releases the memory, but doesn't call the destructors on the stored values. To invoke the destructors, use the destroy_pointee() method provided by the UnsafePointer type.
You shouldn't call the destructor explicitly. If you need to ensure that a destructor is called at a specific point, use the discard pattern described in explicit lifetime extension.
It's important to notice that the __del__() method is an "extra" cleanup event, and your implementation does not override any default destruction behaviors. For example, Mojo still destroys all the fields in Balloons even if you add a __del__() method that to do nothing:
@fieldwise_init
struct Balloons:
    var color: String
    var count: Int

    fn __del__(deinit self):
        # Mojo destroys all the fields when they're last used
        pass
However, the self value inside the __del__() destructor is still whole (so all fields are still usable) until the destructor returns, as we'll discuss more in the following section.
Destructors cannot raise errors
Currently a Mojo destructor isn't allowed to raise an error. This means that the destructor must be defined as an fn function without the raises keyword. Mojo won't allow you to define a destructor using fn raises or def.
Field lifetimes​
In addition to tracking the lifetime of all objects in a program, Mojo also tracks each field of a structure independently. That is, Mojo keeps track of whether a "whole object" is fully or partially initialized/destroyed, and it destroys each field independently with its ASAP destruction policy.
For example, consider this code that changes the value of a field:
@fieldwise_init
struct Balloons:
    var color: String
    var count: Int


def main():
    var balloons = Balloons("red", 5)
    print(balloons.color)
    # balloons.color.__del__() runs here, because this instance is
    # no longer used; it's replaced below

    balloons.color = "blue"  # Overwrite balloons.color
    print(balloons.color)
    # balloons.__del__() runs here
The balloons.color field is destroyed after the first print(), because Mojo knows that it will be overwritten below. You can also see this behavior when using the transfer sigil:
fn consume(var arg: String):
    pass


fn use(arg: Balloons):
    print(arg.count, arg.color, "balloons.")


fn consume_and_use():
    var balloons = Balloons("blue", 8)
    consume(balloons.color^)
    # String.__moveinit__() runs here, which invalidates balloons.color
    # Now balloons is only partially initialized

    # use(balloons)  # This fails because balloons.color is uninitialized

    balloons.color = String("orange")  # All together now
    use(balloons)  # This is ok
    # balloons.__del__() runs here (and only if the object is whole)
Notice that the code transfers ownership of the name field to consume(). For a period of time after that, the name field is uninitialized. Then name is reinitialized before it is passed to the use() function. If you try calling use() before name is re-initialized, Mojo rejects the code with an uninitialized field error.
Also, if you don't re-initialize the name by the end of the pet lifetime, the compiler complains because it's unable to destroy a partially initialized object.
Mojo's policy here is powerful and intentionally straight-forward: fields can be temporarily transferred, but the "whole object" must be constructed with the aggregate type's initializer and destroyed with the aggregate destructor. This means it's impossible to create an object by initializing only its fields, and it's likewise impossible to destroy an object by destroying only its fields.
Field lifetimes during destruct and move​
Both the consuming move constructor and the destructor take their operand with the deinit argument convention. This grants exclusive ownership of the value and marks it as destroyed at the end of the function. Within the function body, Mojo's ASAP policy still applies to fields: each field is destroyed immediately after its last use.
Just to recap, the move constructor and destructor method signatures look like this:
struct TwoStrings:
    fn __moveinit__(out self, deinit existing: Self):
        # Initializes a new `self` by consuming the contents of `existing`
    fn __del__(deinit self):
        # Destroys all resources in `self`
Like the var argument convention, the deinit convention gives the argument exclusive ownership of a value. But unlike a var argument, Mojo doesn't insert a destructor call for the argument at the end of the function-because the deinit convention already marks it as a value that's in the process of being destroyed.
For example, the following code shows how fields are destroyed inside a destructor.
fn consume(var str: String):
    print("Consumed", str)

@fieldwise_init
struct TwoStrings(Copyable, Movable):
    var str1: String
    var str2: String

    fn __del__(deinit self):
        # self value is whole at the beginning of the function
        self.dump()
        # After dump(): str2 is never used again, so str2.__del__() runs now

        consume(self.str1^)
        # self.str1 has been transferred so str1 becomes uninitialized, and
        # no destructor is called for str1.
        # self.__del__() is not called (avoiding an infinite loop).

    fn dump(mut self):
        print("str1:", self.str1)
        print("str2:", self.str2)


def main():
    var two_strings = TwoStrings("foo", "bar")
Explicit lifetime extension​
Most of the time, Mojo's ASAP destruction "just works." Very rarely, you may need to explicitly mark the last use of a value to control when its destructor runs. Think of this as an explicit last-use marker for the lifetime checker, not a general-purpose pattern.
You might do this:
* When writing tests that intentionally create otherwise-unused values (to avoid warnings or dead-code elimination).
* When writing unsafe/advanced code (for example, code that manipulates a value's origin).
* When you need deterministic destructor timing relative to specific side effects (such as logging or profiling).
Mark the last use by assigning the value to the _ discard pattern at the point where it is okay to destroy it. This sets the last use at that line, so the destructor runs immediately after the statement:
# Without explicit extension: s is last used in the print() call, so it is
# destroyed immediately afterwards.
var s = "abc"
print(s)  # s.__del__() runs after this line

# With explicit extension: push last-use to the discard line.
var t = "xyz"
print(t)

# ... some time later
_ = t  # t.__del__() runs after this line
Previous versions of Mojo required the transfer sigil (^) when discarding a move-only type. This is no longer required, since the compiler doesn't actually move the discarded value. For more information on the transfer sigil, see the section on ownership transfer.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Edit this page

Edit this page


Traits
A trait is a set of requirements that a type must implement. You can think of it as a contract: a type that conforms to a trait guarantees that it implements all of the features of the trait.
Traits are similar to Java interfaces, C++ concepts, Swift protocols, and Rust traits. If you're familiar with any of those features, Mojo traits solve the same basic problem.
You've probably already seen some traits, like Copyable and Movable, used in example code. This section describes how traits work, how to use existing traits, and how to define your own traits.
Background​
In dynamically-typed languages like Python, you don't need to explicitly declare that two classes are similar. This is easiest to show by example:
🐍 Python
class Duck:
    def quack(self):
        print("Quack.")

class StealthCow:
    def quack(self):
        print("Moo!")

def make_it_quack(maybe_a_duck):
    try:
        maybe_a_duck.quack()
    except:
        print("Not a duck.")

make_it_quack(Duck())
make_it_quack(StealthCow())
The Duck and StealthCow classes aren't related in any way, but they both define a quack() method, so they work the same in the make_it_quack() function. This works because Python uses dynamic dispatch-it identifies the methods to call at runtime. So make_it_quack() doesn't care what types you're passing it, only the fact that they implement the quack() method.
In a statically-typed environment, this approach doesn't work: Mojo functions require you to specify the type of each argument. If you wanted to write this example in Mojo without traits, you'd need to write a function overload for each input type.
🔥 Mojo
@fieldwise_init
struct Duck(Copyable, Movable):
    fn quack(self):
        print("Quack")

@fieldwise_init
struct StealthCow(Copyable, Movable):
    fn quack(self):
        print("Moo!")

fn make_it_quack(definitely_a_duck: Duck):
    definitely_a_duck.quack()

fn make_it_quack(not_a_duck: StealthCow):
    not_a_duck.quack()

make_it_quack(Duck())
make_it_quack(StealthCow())
Quack
Moo!
This isn't too bad with only two types. But the more types you want to support, the less practical this approach is.
You might notice that the Mojo versions of make_it_quack() don't include the try/except statement. We don't need it because Mojo's static type checking ensures that you can only pass instances of Duck or StealthCow into the make_it_quack()function.
Using traits​
Traits solve this problem by letting you define a shared set of behaviors that types can implement. Then you can write a function that depends on the trait, rather than individual types. As an example, let's update the make_it_quack() example using traits. This will involve three steps:
1. Defining a new Quackable trait.
2. Adding the trait to the Duck and StealthCow structs.
3. Updating the make_it_quack() function to depend on the trait.
Defining a trait​
The first step is defining a trait that requires a quack() method:
trait Quackable:
    fn quack(self):
        ...
A trait looks a lot like a struct, except it's introduced by the trait keyword. Note that the quack() method signature is followed three dots (...), which indicates it isn't implemented. This is a required method that must be implemented by any conforming struct.
A trait can also supply a default implementation for a required method, so that conforming structs don't need to implement it. For more information, see Default method implementations.
A trait can also include associated aliases-compile-time constant values that must be defined by conforming structs. Associated aliases are useful for writing traits that describe generic types. For more information, see Associated aliases for generics.
Currently, Mojo treats a trait method as not implemented if the function body starts with ... or pass. In the future only ... will mark a trait method as not implemented.
Adding traits to structs​
Next we need some structs that conform to the Quackable trait. Since the Duck and StealthCow structs above already implement the quack() method, all we need to do is add the Quackable trait to the traits it conforms to (in parenthesis, after the struct name).
(If you're familiar with Python, this looks just like Python's inheritance syntax.)
@fieldwise_init
struct Duck(Copyable, Movable, Quackable):
    fn quack(self):
        print("Quack")

@fieldwise_init
struct StealthCow(Copyable, Movable, Quackable):
    fn quack(self):
        print("Moo!")
The struct needs to implement any methods that are declared in the trait. The compiler enforces conformance: if a struct says it conforms to a trait, it must implement everything required by the trait or the code won't compile.
Using a trait as a type bound​
Finally, you can define a function that takes a Quackable like this:
fn make_it_quack[DuckType: Quackable](maybe_a_duck: DuckType):
    maybe_a_duck.quack()
Or using the shorthand form:
fn make_it_quack2(maybe_a_duck: Some[Quackable]):
    maybe_a_duck.quack()
This syntax may look a little unfamiliar if you haven't dealt with Mojo parameters before. What the first signature means is that maybe_a_duck is an argument of type DuckType, where DuckType is a type that must conform to the Quackable trait. Quackable is called the type bound for DuckType.
The Some[Quackable] form expresses the same idea: the type of maybe_a_duck is some concrete type that conforms to the trait Quackable.
Both forms work the same, except that the first form explicitly names the type value. This can be useful, for example, if you want to take two values of the same type:
fn take_two_quackers[DuckType: Quackable](quacker1: DuckType, quacker2: DuckType):
    pass
Putting it all together​
Using the function is simple enough:
make_it_quack(Duck())
make_it_quack(StealthCow())
Quack
Moo!
Note that you don't need the square brackets when you call make_it_quack(): the compiler infers the type of the argument, and ensures the type has the required trait.
One limitation of traits is that you can't add traits to existing types. For example, if you define a new Numeric trait, you can't add it to the standard library Float64 and Int types. However, the standard library already includes quite a few traits, and we'll be adding more over time.
Traits can require static methods​
In addition to regular instance methods, traits can specify required static methods.
trait HasStaticMethod:
    @staticmethod
    fn do_stuff(): ...

fn fun_with_traits[type: HasStaticMethod]():
    type.do_stuff()
Default method implementations​
Often, some or all of the structs that conform to a given trait can use the same implementation for a given required method. In this case, the trait can include a default implementation. A conforming struct can still define its own version of the method, overriding the default implementation. But if the struct doesn't define its own version, it automatically inherits the default implementation.
Defining a default implementation for a trait looks the same as writing a method for a struct:
trait DefaultQuackable:
    fn quack(self):
        print("Quack")


@fieldwise_init
struct DefaultDuck(Copyable, DefaultQuackable, Movable):
    pass
When looking at the API doc for a standard library trait, you'll see methods that you must implement listed as required methods, and methods that have default implementations listed as provided methods.
The Equatable trait is a good example of the use case for default implementations. The trait includes two methods: __eq__() (corresponding to the == operator) and __ne__() (corresponding to the != operator). Every type that conforms to Equatable needs to define the __eq__() method for itself, but the trait supplies a default implementation for __ne__(). Given an __eq__() method, the definition of __ne__() is trivial for most types:
fn __ne__(self, other: Self) -> Bool:
    return not self.__eq__(other)
Trait compositions​
You can compose traits using the & sigil. This lets you define new traits that are simple combinations of other traits. You can use a trait composition anywhere that you'd use a single trait:
trait Flyable:
    fn fly(self): ...

fn quack_and_go[type: Quackable & Flyable](quacker: type):
    quacker.quack()
    quacker.fly()

@fieldwise_init
struct FlyingDuck(Copyable, Movable, Quackable, Flyable):
    fn quack(self):
        print("Quack")

    fn fly(self):
        print("Whoosh!")
You can also use the alias keyword to create a shorthand name for a trait composition:
alias DuckLike = Quackable & Flyable

struct ToyDuck(DuckLike):
    # ... implementation omitted
You can also compose traits using inheritance, by defining a new, empty trait like this:
trait DuckTrait(Quackable, Flyable):
    pass
However, this is less flexible than using a trait composition and not recommended. The difference is that using the trait keyword defines a new, named trait. For a struct to conform to this trait, you need to explicitly include it in the struct's signature. On the other hand, the DuckLike alias represents a composition of two separate traits, Quackable and Flyable, and anything that conforms to those two traits conforms to DuckLike. For example, consider the FlyingDuck type shown above:
struct FlyingDuck(Copyable, Movable, Quackable, Flyable):
    # ... etc
Because FlyingDuck conforms to both Quackable and Flyable, it also conforms to the DuckLike trait composition. But it doesn't conform to DuckTrait, since it doesn't include DuckTrait in its list of traits.
Trait inheritance​
Traits can inherit from other traits. A trait that inherits from another trait includes all of the requirements declared by the parent trait. For example:
trait Animal:
    fn make_sound(self):
        ...

# Bird inherits from Animal
trait Bird(Animal):
    fn fly(self):
        ...
Since Bird inherits from Animal, a struct that conforms to the Bird trait needs to implement both make_sound() and fly(). And since every Bird conforms to Animal, a struct that conforms to Bird can be passed to any function that requires an Animal.
To inherit from multiple traits, add a comma-separated list of traits or trait compositions inside the parenthesis. For example, you could define a NamedAnimal trait that combines the requirements of the Animal trait and a new Named trait:
trait Named:
    fn get_name(self) -> String:
        ...

trait NamedAnimal(Animal, Named):
    fn emit_name_and_sound(self):
        ...
Inheritance is useful when you're creating a new trait that adds its own requirements. If you simply want to express the union of two or more traits, you should use a simple trait composition instead:
alias NamedAnimal = Animal & Named
Traits and lifecycle methods​
Traits can specify required lifecycle methods, including constructors, copy constructors and move constructors.
For example, the following code creates a MassProducible trait. A MassProducible type has a default (no-argument) constructor and can be moved. It uses two built-in traits: Defaultable, which requires a default (no-argument) constructor, and Movable, which requires the type to have a move constructor.
The factory[]() function returns a newly-constructed instance of a MassProducible type. The following example shows the definitions of the Defaultable and Movable traits in comments for reference:
# trait Defaultable
#     fn __init__(out self): ...

# trait Movable
#     fn __moveinit__(out self, deinit existing: Self): ...

alias MassProducible = Defaultable & Movable

fn factory[type: MassProducible]() -> type:
    return type()

struct Thing(MassProducible):
    var id: Int

    fn __init__(out self):
        self.id = 0

    fn __moveinit__(out self, deinit existing: Self):
        self.id = existing.id

var thing = factory[Thing]()
Register-passable traits​
A trait can be declared with either the @register_passable decorator or the @register_passable("trivial") decorator. These decorators add requirements for conforming structs:
* If the trait is declared as @register_passable, every struct that conforms to the trait must be either @register_passable or @register_passable("trivial").
* If the trait is declared as @register_passable("trivial"), every struct that conforms to the trait must be struct must be @register_passable("trivial"), too.
For the purpose of trait conformance, a trait or struct that's defined with @register_passable should automatically conform to the Movable trait, and a trait or struct that's defined with @register_passable("trivial") should automatically conform to the Copyable and Movable traits.
In some cases, the compiler may not track these automatic conformances correctly. If you run into an issue, add the traits to your struct explicitly.
Built-in traits​
The Mojo standard library includes many traits. They're implemented by a number of standard library types, and you can also implement these on your own types. These standard library traits include:
* Absable
* AnyType
* Boolable
* Comparable
* Copyable
* Defaultable
* Hashable
* Indexer
* Intable
* IntableRaising
* KeyElement
* Movable
* PathLike
* Powable
* Representable
* Sized
* Stringable
* StringableRaising
* Roundable
* Writable
* Writer
The API reference docs linked above include usage examples for each trait. The following sections discuss a few of these traits.
The Sized trait​
The Sized trait identifies types that have a measurable length, like strings and arrays.
Specifically, Sized requires a type to implement the __len__() method. This trait is used by the built-in len() function. For example, if you're writing a custom list type, you could implement this trait so your type works with len():
struct MyList(Copyable, Movable, Sized):
    var size: Int
    # ...

    fn __init__(out self):
        self.size = 0

    fn __len__(self) -> Int:
        return self.size

print(len(MyList()))
0
The Intable and IntableRaising traits​
The Intable trait identifies a type that can be converted to Int. The IntableRaising trait describes a type can be converted to an Int, but the conversion might raise an error.
Both of these traits require the type to implement the __int__() method. For example:
@fieldwise_init
struct IntLike(Intable):
    var i: Int

    fn __int__(self) -> Int:
        return self.i

value = IntLike(42)
print(Int(value) == 42)
True
The Stringable, Representable, and Writable traits​
The Stringable trait identifies a type that can be explicitly converted to String. The StringableRaising trait describes a type that can be converted to a String, but the conversion might raise an error. These traits also mean that the type can support both the {!s} and {} format specifiers of the String and StringSlice class's format() method. These traits require the type to define the __str__() method.
In contrast, the Representable trait defines a type that can be used with the built-in repr() function, as well as the {!r} format specifier of the format() method. This trait requires the type to define the __repr__() method, which should compute the "official" string representation of a type. If at all possible, this should look like a valid Mojo expression that could be used to recreate a struct instance with the same value.
The Writable trait describes a type that can be converted to a stream of UTF-8 encoded data by writing to a Writer object. The print() function requires that its arguments conform to the Writable trait. This enables efficient stream-based writing by default, avoiding unnecessary intermediate String heap allocations.
The Writable trait requires a type to implement a write_to() method, which is provided with an object that conforms to the Writer as an argument. You then invoke the Writer instance's write() method to write a sequence of Writable arguments constituting the String representation of your type.
While this might sound complex at first, in practice you can minimize boilerplate and duplicated code by using the String.write() static function to implement the type's Stringable implementation in terms of its Writable implementation. Here is a simple example of a type that implements all of the Stringable, Representable, and Writable traits:
@fieldwise_init
struct Dog(Copyable, Stringable, Representable, Writable):
    var name: String
    var age: Int

    # Allows the type to be written into any `Writer`
    fn write_to(self, mut writer: Some[Writer]):
        writer.write("Dog(", self.name, ", ", self.age, ")")

    # Construct and return a `String` using the previous method
    fn __str__(self) -> String:
        return String.write(self)

    # Alternative full representation when calling `repr`
    fn __repr__(self) -> String:
        return String(
            "Dog(name=", repr(self.name), ", age=", repr(self.age), ")"
        )

dog = Dog("Rex", 5)
print(repr(dog))
print(dog)

var dog_info = "String: {!s}\nRepresentation: {!r}".format(dog, dog)
print(dog_info)
Dog(name='Rex', age=5)
Dog(Rex, 5)
String: Dog(Rex, 5)
Representation: Dog(name='Rex', age=5)
The AnyType trait​
When building a generic container type, one challenge is knowing how to dispose of the contained items when the container is destroyed. Any type that dynamically allocates memory needs to supply a destructor (__del__() method) that must be called to free the allocated memory. But not all types have a destructor.
The AnyType trait (also provided as the ImplicitlyDestructible alias) represents a type with a destructor. Almost all traits inherit from AnyType, and all structs conform to AnyType by default. For any type that conforms to AnyType and doesn't define a destructor, Mojo generates a no-op destructor. This means you can call the destructor on any type that inherits from AnyType/ImplicitlyDestructible.
TODO
In the Mojo standard library docs you will also see a trait called UnknownDestructability, which represents a type that may or may not have a destructor. All structs implicitly conform to this trait.
This trait exists to support a planned future feature called linear or explicitly-destroyed types.
Generic structs with traits​
You can also use traits when defining a generic container. A generic container is a container (for example, an array or hashmap) that can hold different data types. In a dynamic language like Python it's easy to add different types of items to a container. But in a statically-typed environment the compiler needs to be able to identify the types at compile time. For example, if the container needs to copy a value, the compiler needs to verify that the type can be copied.
The List type is an example of a generic container. A single List can only hold a single type of data. The list elements must conform to the Copyable and Movable traits:
struct List[T: Copyable & Movable]:
For example, you can create a list of integer values like this:
var list: List[Int]
list = [1, 2, 3, 4]
for i in range(len(list)):
    print(list[i], end=" ")
1 2 3 4
You can use traits to define requirements for elements that are stored in a container. For example, List requires elements that can be moved and copied. To store a struct in a List, the struct needs to conform to the Copyable and Movable traits, which require a copy constructor and a move constructor.
Building generic containers is an advanced topic. For an introduction, see the section on parameterized structs.
Associated aliases for generics​
In addition to methods, a trait can include associated aliases, which must be defined by any conforming struct. For example:
trait Repeater:
    alias count: Int
An implementing struct must define a concrete constant value for the alias, using any compile-time parameter value. For example, it can use a literal constant or a compile-time expression, including one that uses the struct's parameters.
struct Doublespeak(Repeater):
    alias count: Int = 2

struct Multispeak[verbosity: Int](Repeater):
    alias count: Int = Self.verbosity * 2 + 1
The Doublespeak struct has a constant value for the alias, but the Multispeak struct lets the user set the value using a parameter:
repeater = Multispeak[12]()
Note that the alias is named count, and the Multispeak parameter is named verbosity. Parameters and aliases are in the same namespace, so the parameter can't have the same name as the associated alias.
Associated aliases are most useful for writing traits for generic types. For example, imagine that you want to write a trait that describes a generic stack data structure that stores elements that conform to the Copyable and Movable traits.
By adding the element type as an associated alias to the trait, you can specify generic methods on the trait:
trait Stacklike:
    alias EltType: Copyable & Movable

    fn push(mut self, var item: Self.EltType):
        ...

    fn pop(mut self) -> Self.EltType:
        ...
The following struct implements the Stacklike trait using a List as the underlying storage:
struct MyStack[type: Copyable & Movable](Stacklike):
    """A simple Stack built using a List."""
    alias EltType = Self.type
    alias list_type = List[Self.EltType]

    var list: Self.list_type

    fn __init__(out self):
        self.list = Self.list_type()

    fn push(mut self, var item: Self.EltType):
        self.list.append(item^)

    fn pop(mut self) -> Self.EltType:
        return self.list.pop()

    fn dump[
        WritableEltType: Writable & Copyable & Movable
    ](self: MyStack[WritableEltType]):
        print("[", end="")
        for item in self.list:
            print(item, end=", ")
        print("]")
The MyStack type adds a dump() method that prints the contents of the stack. Because a struct that conforms to Copyable and Movable is not necessarily printable, MyStack uses conditional conformance to define a dump() method that works as long as the element type is writable.
The following code exercises this new trait by defining a generic method, add_to_stack() that adds an item to any Stacklike type.
def add_to_stack[S: Stacklike](mut stack: S, var item: S.EltType):
    stack.push(item^)

def main():
    s = MyStack[Int]()
    add_to_stack(s, 12)
    add_to_stack(s, 33)
    s.dump()             # [12, 33, ]
    print(s.pop())       # 33
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit


Parameterization: compile-time metaprogramming
Many languages have facilities for metaprogramming: that is, for writing code that generates or modifies code. Python has facilities for dynamic metaprogramming: features like decorators, metaclasses, and many more. These features make Python very flexible and productive, but since they're dynamic, they come with runtime overhead. Other languages have static or compile-time metaprogramming features, like C preprocessor macros and C++ templates. These can be limiting and hard to use.
To support Modular's work in AI, Mojo aims to provide powerful, easy-to-use metaprogramming with zero runtime cost. This compile-time metaprogramming uses the same language as runtime programs, so you don't have to learn a new language-just a few new features.
The main new feature is parameters. You can think of a parameter as a compile-time variable that becomes a runtime constant. This usage of "parameter" is probably different from what you're used to from other languages, where "parameter" and "argument" are often used interchangeably. In Mojo, "parameter" and "parameter expression" refer to compile-time values, and "argument" and "expression" refer to runtime values.
In Mojo, you can add parameters to a struct or function. You can also define named parameter expressions-aliases-that you can use as runtime constants.
Parameterized functions​
To define a parameterized function, add parameters in square brackets ahead of the argument list. Each parameter is formatted just like an argument: a parameter name, followed by a colon and a type (which is required). In the following example, the function has a single parameter, count of type Int.
fn repeat[count: Int](msg: String):
    @parameter
    for i in range(count):
        print(msg)
The @parameter decorator shown here causes the for loop to be evaluated at compile time. The decorator only works if the loop limits are compile-time constants. Since count is a parameter, range(count) can be calculated at compile time.
Calling a parameterized function, you provide values for the parameters, just like function arguments:
repeat[3]("Hello")
Hello
Hello
Hello
The compiler resolves the parameter values during compilation, and creates a concrete version of the repeat[]() function for each unique parameter value. After resolving the parameter values and unrolling the loop, the repeat[3]() function would be roughly equivalent to this:
fn repeat_3(msg: String):
    print(msg)
    print(msg)
    print(msg)
This doesn't represent actual code generated by the compiler. By the time parameters are resolved, Mojo code has already been transformed to an intermediate representation in MLIR.
If the compiler can't resolve all parameter values to constant values, compilation fails.
Anatomy of a parameter list​
Parameters to a function or struct appear in square brackets after a function or struct name. Parameters always require type annotations.
When you're looking at a function or struct definition, you may see some special characters such as / and * in the parameter list. Here's an example:
def my_sort[
    # infer-only parameters
    dtype: DType,
    width: Int,
    //,
    # positional-only parameter
    values: SIMD[dtype, width],
    /,
    # positional-or-keyword parameter
    compare: fn (Scalar[dtype], Scalar[dtype]) -> Int,
    *,
    # keyword-only parameter
    reverse: Bool = False,
]() -> SIMD[dtype, width]:
Here's a quick overview of the special characters in the parameter list:
* Double slash (//): parameters declared before the double slash are infer-only parameters.
* Slash (/): parameters declared before a slash are positional-only parameters. Positional-only and keyword-only parameters follow the same rules as positional-only and keyword-only arguments.
* A parameter name prefixed with a star, like *Types identifies a variadic parameter (not shown in the example above). Any parameters following the variadic parameter are keyword-only.
* Star (*): in a parameter list with no variadic parameter, a star by itself indicates that the following parameters are keyword-only parameters.
* An equals sign (=) introduces a default value for an optional parameter.
Parameters and generics​
"Generics" refers to functions that can act on multiple types of values, or containers that can hold multiple types of values. For example, List, can hold different types of values, so you can have a list of Int values, or a list of String values).
In Mojo, generics use parameters to specify types. For example, List takes a type parameter, so a vector of integers is written List[Int]. So all generics use parameters, but not everything that uses parameters is a generic.
For example, the repeat[]() function in the previous section includes parameter of type Int, and an argument of type String. It's parameterized, but not generic. A generic function or struct is parameterized on type. For example, we could rewrite repeat[]() to take any type of argument that conforms to the Stringable trait:
fn repeat[MsgType: Stringable, //, count: Int](msg: MsgType):
    @parameter
    for i in range(count):
        print(String(msg))


def main():
    # MsgType is always inferred, so first positional keyword `2` is
    # passed to `count`
    repeat[2](42)
42
42
This updated function takes any Stringable type, so you can pass it an Int, String, or Bool value.
Note that there's a double-slash (//) in the parameter list after MsgType, to specify that it's an infer-only parameter, so you don't need to specify it explicitly. Instead, the compiler sees that the msg argument is an Int and infers the type from the value.
Mojo's support for generics is still early. You can write generic functions like this using traits and parameters. You can also write generic collections like List and Dict. If you're interested in learning how these types work, you can find the source code for the standard library collection types on GitHub.
Parameterized structs​
You can also add parameters to structs. You can use parameterized structs to build generic collections. For example, a generic array type might include code like this:
struct GenericArray[ElementType: Copyable & Movable]:
    var data: UnsafePointer[Self.ElementType, MutOrigin.external]
    var size: Int

    fn __init__(out self, var *elements: Self.ElementType):
        self.size = len(elements)
        self.data = alloc[Self.ElementType](self.size)
        for i in range(self.size):
            (self.data + i).init_pointee_move(elements[i].copy())

    fn __del__(deinit self):
        for i in range(self.size):
            (self.data + i).destroy_pointee()
        self.data.free()

    fn __getitem__(self, i: Int) raises -> ref [self] Self.ElementType:
        if i < self.size:
            return self.data[i]
        else:
            raise Error("Out of bounds")
This struct has a single parameter, ElementType, which is a placeholder for the data type you want to store in the array, sometimes called a type parameter. ElementType conforms to the Copyable and Movable traits.
As with parameterized functions, you need to pass in parameter values when you use a parameterized struct. In this case, when you create an instance of GenericArray, you need to specify the type you want to store, like Int, or Float64. (This is a little confusing, because the parameter value you're passing in this case is a type. That's OK: a Mojo type is a valid compile-time value.)
You'll see that ElementType is used throughout the struct where you'd usually see a type name. For example, as the formal type for the elements in the constructor, and the return type of the __getitem__() method.
Here's an example of using GenericArray:
var array = GenericArray(1, 2, 3, 4)
for i in range(array.size):
    end = ", " if i < array.size - 1 else "\n"
    print(array[i], end=end)
1, 2, 3, 4
A parameterized struct can use the Self type to represent a concrete instance of the struct (that is, with all its parameters specified). For example, you could add a static factory method to GenericArray with the following signature:
struct GenericArray[ElementType: Copyable & Movable]:
    ...

    @staticmethod
    fn splat(count: Int, value: Self.ElementType) -> Self:
        # Create a new array with count instances of the given value
Here, Self is equivalent to writing GenericArray[Self.ElementType]. That is, you can call the splat() method like this:
GenericArray[Float64].splat(8, 0)
The method returns an instance of GenericArray[Float64].
Conditional conformance​
When creating a generic struct, you might want to define some methods that require extra features. For example, consider a collection like GenericArray that holds instances of a type that conforms to the Copyable and Movable traits. This imposes a lot of limitations: you can't implement a sort() method because you can't guarantee that the stored type supports the comparison operators; you can't write a useful __str__() or __repr__() dunder method because you can't guarantee that the stored type supports conversion to a string.
The answer to these issues is conditional conformance, which lets you define a method that requires additional features. You do this by defining the self value that has a more specific bound on one or more of its parameters.
For example, the following code defines a Container type that holds an instance of a type conforming to Movable. It also defines a __str__() method that can only be called if the stored ElementType conforms to Writable as well as Movable:
@fieldwise_init
struct Container[ElementType: Movable]:
    var element: Self.ElementType

    def __str__[
        StrElementType: Writable & Movable, //
    ](self: Container[StrElementType]) -> String:
        return String(self.element)


def main():
    float_container = Container(5.0)
    string_container = Container("Hello")
    print(float_container.__str__())
    print(string_container.__str__())
5.0
Hello
Note the signature of the __str__() method, which declares the self argument with a more specific type. Specifically, it declares that it takes a Container with an ElementType that conforms to the Writable and Movable traits.
def __str__[StrElementType: Writable & Movable, //](
        self: Container[StrElementType]) -> String:
The trait bound on StrElementType must include ElementType's original trait (Movable in this case), either by composition or by inheritance. The trait composition Writable & Movable includes the original trait. You could also use a trait that inherits from Movable.
Note that the main() function calls the __str__() method directly, rather than calling String(float_container). One current limitation of conditional conformance is that Mojo can't recognize the struct Container[Int] as conforming to Stringable, even though the __str__() method is implemented for any ElementType that's also Stringable.
Case study: the SIMD type​
For a real-world example of a parameterized type, let's look at the SIMD type from Mojo's standard library.
Single instruction, multiple data (SIMD) is a parallel processing technology built into many modern CPUs, GPUs, and custom accelerators. SIMD allows you to perform a single operation on multiple pieces of data at once. For example, if you want to take the square root of each element in an array, you can use SIMD to parallelize the work.
Processors implement SIMD using low-level vector registers in hardware that hold multiple instances of a scalar data type. In order to use the SIMD instructions on these processors, the data must be shaped into the proper SIMD width (data type) and length (vector size). Processors may support 512-bit or longer SIMD vectors, and support many data types from 8-bit integers to 64-bit floating point numbers, so it's not practical to define all of the possible SIMD variations.
Mojo's SIMD type (defined as a struct) exposes the common SIMD operations through its methods, and makes the SIMD data type and size values parametric. This allows you to directly map your data to the SIMD vectors on any hardware.
Here's a cut-down (non-functional) version of Mojo's SIMD type definition:
struct SIMD[type: DType, size: Int]:
    var value: ... # Some low-level MLIR stuff here

    # Create a new SIMD from a number of scalars
    fn __init__(out self, *elems: SIMD[Self.type, 1]):  ...

    # Fill a SIMD with a duplicated scalar value.
    @staticmethod
    fn splat(x: SIMD[Self.type, 1]) -> SIMD[Self.type, Self.size]: ...

    # Cast the elements of the SIMD to a different elt type.
    fn cast[target: DType](self) -> SIMD[target, Self.size]: ...

    # Many standard operators are supported.
    fn __add__(self, rhs: Self) -> Self: ...
So you can create and use a SIMD vector like this:
var vector = SIMD[DType.int16, 4](1, 2, 3, 4)
vector = vector * vector
for i in range(4):
    print(vector[i], end=" ")
1 4 9 16
As you can see, a simple arithmetic operator like * applied to a pair of SIMD vector operates on the corresponding elements in each vector.
Defining each SIMD variant with parameters is great for code reuse because the SIMD type can express all the different vector variants statically, instead of requiring the language to pre-define every variant.
Because SIMD is a parameterized type, the self argument in its functions carries those parameters-the full type name is SIMD[type, size]. Although it's valid to write this out (as shown in the return type of splat()), this can be verbose, so we recommend using the Self type (from PEP673) like the __add__ example does.
Overloading on parameters​
Functions and methods can be overloaded on their parameter signatures. For information on overload resolution, see Overloaded functions.
Using parameterized types and functions​
You can use parametric types and functions by passing values to the parameters in square brackets. For example, for the SIMD type above, type specifies the data type and size specifies the length of the SIMD vector (it must be a power of 2):
def main():
    # Make a vector of 4 floats.
    var small_vec = SIMD[DType.float32, 4](1.0, 2.0, 3.0, 4.0)

    # Make a big vector containing 1.0 in float16 format.
    var big_vec = SIMD[DType.float16, 32](1.0)

    # Do some math and convert the elements to float32.
    var bigger_vec = (big_vec + big_vec).cast[DType.float32]()

    # You can write types out explicitly if you want of course.
    var bigger_vec2: SIMD[DType.float32, 32] = bigger_vec

    print("small_vec DType:", small_vec.dtype, "size:", small_vec.size)
    print(
        "bigger_vec2 DType:",
        bigger_vec2.dtype,
        "size:",
        bigger_vec2.size,
    )
small_vec type: float32 length: 4
bigger_vec2 type: float32 length: 32
Note that the cast() method also needs a parameter to specify the type you want from the cast (the method definition above expects a target parametric value). Thus, just as the SIMD struct is a generic type definition, the cast() method is a generic method definition. At compile time, the compiler creates a concrete version of the cast() method with the target parameter bound to DType.float32.
The code above shows the use of concrete types (that is, the parameters are all bound to known values). But the major power of parameters comes from the ability to define parametric algorithms and types (code that uses the parameter values). For example, here's how to define a parametric algorithm with Scalar that is datatype agnostic:
from math import sqrt

fn rsqrt[dt: DType](x: Scalar[dt]) -> Scalar[dt]:
    return 1 / sqrt(x)

def main():
    var v = Scalar[DType.float16](42)
    print(rsqrt(v))
0.154296875
Notice that the x argument is a Scalar type that's parameterized based on the dt function parameter. And note that the actual call to rsqrt() doesn't specify the dt parameter, because it can be inferred from the argument's type (here, Scalar[DType.float16]).
Parameter inference​
The Mojo compiler can often infer parameter values, so you don't always have to specify them. For example, you can call the rsqrt() function defined above without specifying the dt parameter:
from math import sqrt

fn rsqrt[dt: DType](x: Scalar[dt]) -> Scalar[dt]:
    return 1 / sqrt(x)

def main():
    var v = Scalar[DType.float16](33)
    print(rsqrt(v))
0.174072265625
The compiler infers the dt parameter based on the parametric v value passed into it, as if you wrote rsqrt[DType.float16](v) explicitly. Figure 1 shows a mental model for how parameter inference works.

Figure 1. Parameter inference 
Parameter inference can seem a little confusing: it might seem like the compiler is inferring compile-time parameter values from run-time argument values. But in fact it's inferring parameters from the statically-known types of the arguments.
Inference failures
If parameter inference fails, the compiler will report an error, usually "failed to infer parameter 'param_name'". Unfortunately, the compiler also sometimes reports this error incorrectly, for example, when the actual error is a type mismatch. In these cases, specifying the missing parameters explicitly will often allow Mojo to report the correct error.
Mojo can also infer the values of struct parameters from the arguments passed to a constructor or static method.
For example, consider the following struct:
struct One[Type: Writable & Copyable & Movable]:
    var value: Self.Type

    fn __init__(out self, value: Self.Type):
        self.value = value.copy()


def use_one():
    s1 = One(123)  # equivalent to One[Int](123)
    s2 = One("Hello")  # equivalent to One[String]("Hello")
Note that you can create an instance of One without specifying the Type parameter-Mojo can infer it from the value argument.
You can also infer parameters from a parameterized type passed to a constructor or static method:
struct Two[Type: Writable & Copyable & Movable]:
    var val1: Self.Type
    var val2: Self.Type

    fn __init__(out self, one: One[Self.Type], another: One[Self.Type]):
        self.val1 = one.value.copy()
        self.val2 = another.value.copy()
        print(String(self.val1), String(self.val2))

    @staticmethod
    fn fire(thing1: One[Self.Type], thing2: One[Self.Type]):
        print("🔥", String(thing1.value), String(thing2.value))

def use_two():
    s3 = Two(One("infer"), One("me"))
    Two.fire(One(1), One(2))
    # Two.fire(One("mixed"), One(0)) # Error: parameter inferred to two
                                     # different values

use_two()
infer me
🔥 1 2
Two takes a Type parameter, and its constructor takes values of type One[Type]. When constructing an instance of Two, you don't need to specify the Type parameter, since it can be inferred from the arguments.
Similarly, the static fire() method takes values of type One[Type], so Mojo can infer the Type value at compile time. Note that passing two instances of One with different types doesn't work.
If you're familiar with C++, you may recognize this as similar to Class Template Argument Deduction (CTAD).
Optional parameters and keyword parameters​
Just as you can specify optional arguments in function signatures, you can also define an optional parameter by giving it a default value.
You can also pass parameters by keyword, just like you can use keyword arguments. For a function or struct with multiple optional parameters, using keywords allows you to pass only the parameters you want to specify, regardless of their position in the function signature.
For example, here's a function with two parameters, each with a default value:
fn speak[a: Int = 3, msg: String = "woof"]():
    print(msg, a)


fn use_defaults():
    speak()  # prints 'woof 3'
    speak[5]()  # prints 'woof 5'
    speak[7, "meow"]()  # prints 'meow 7'
    speak[msg="baaa"]()  # prints 'baaa 3'
Recall that when a parametric function is called, Mojo can infer the parameter values. That is, it can determine its parameter values from the parameters attached to an argument. If the parametric function also has a default value defined, then the inferred parameter value takes precedence.
For example, in the following code, we update the parametric speak[]() function to take an argument with a parametric type. Although the function has a default parameter value for a, Mojo instead uses the inferred a parameter value from the bar argument (as written, the default a value can never be used, but this is just for demonstration purposes):
@fieldwise_init
struct Bar[v: Int]:
    pass


fn speak[a: Int = 3, msg: String = "woof"](bar: Bar[a]):
    print(msg, a)


fn use_inferred():
    speak(Bar[9]())  # prints 'woof 9'
As mentioned above, you can also use optional parameters and keyword parameters in a struct:
struct KwParamStruct[greeting: String = "Hello", name: String = "🔥mojo🔥"]:
    fn __init__(out self):
        print(Self.greeting, Self.name)

fn use_kw_params():
    var a = KwParamStruct[]()                 # prints 'Hello 🔥mojo🔥'
    var b = KwParamStruct[name="World"]()     # prints 'Hello World'
    var c = KwParamStruct[greeting="Hola"]()  # prints 'Hola 🔥mojo🔥'
Mojo supports positional-only and keyword-only parameters, following the same rules as positional-only and keyword-only arguments.
Infer-only parameters​
Sometimes you need to declare functions where parameters depend on other parameters. Because the signature is processed left to right, a parameter can only depend on a parameter earlier in the parameter list. For example:
fn dependent_type[dtype: DType, value: Scalar[dtype]]():
    print("Value: ", value)
    print("Value is floating-point: ", dtype.is_floating_point())

dependent_type[DType.float64, Float64(2.2)]()
Value:  2.2000000000000002
Value is floating-point:  True
You can't reverse the position of the dtype and value parameters, because value depends on dtype. However, because dtype is a required parameter, you can't leave it out of the parameter list and let Mojo infer it from value:
dependent_type[Float64(2.2)]() # Error!
Infer-only parameters are a special class of parameters that are always either inferred from context or specified by keyword. Infer-only parameters are placed at the beginning of the parameter list, set off from other parameters by the // sigil:
fn example[type: Copyable & Movable, //, list: List[type]]()
Transforming dtype into an infer-only parameter solves this problem:
fn dependent_type[dtype: DType, //, value: Scalar[dtype]]():
    print("Value: ", value)
    print("Value is floating-point: ", dtype.is_floating_point())

dependent_type[Float64(2.2)]()
Value:  2.2000000000000002
Value is floating-point:  True
Because infer-only parameters are declared at the beginning of the parameter list, other parameters can depend on them, and the compiler will always attempt to infer the infer-only values from bound parameters or arguments.
There are sometimes cases where it's useful to specify an infer-only parameter by keyword. For example, the Span type is parametric on origin:
struct Span[mut: Bool, //, T: Copyable & Movable, origin: Origin[mut], ...]:
    ...
Here, the mut parameter is infer-only. The value is usually inferred when you create an instance of Span. Binding the mut parameter by keyword lets you define a Span that requires a mutable origin.
def mutate_span(span: Span[mut=True, Byte]):
    for i in range(0, len(span), 2):
        if i + 1 < len(span):
            span.swap_elements(i, i + 1)
If the compiler can't infer the value of an infer-only parameter, and it's not specified by keyword, compilation fails.
Variadic parameters​
Mojo also supports variadic parameters, similar to Variadic arguments:
struct MyTensor[*dimensions: Int]:
    pass
Variadic parameters currently have some limitations that variadic arguments don't have:
* Variadic parameters must be homogeneous-that is, all the values must be the same type.
* The parameter type must be register-passable.
* The parameter values aren't automatically projected into a VariadicList, so you need to construct the list explicitly:
fn sum_params[*values: Int]() -> Int:
    alias list = VariadicList(values)
    var sum = 0
    for v in list:
        sum += v
    return sum
Variadic keyword parameters (for example, **kwparams) are not supported yet.
Parameter expressions are just Mojo code​
A parameter expression is any code expression (such as a+b) that occurs where a parameter is expected. Parameter expressions support operators and function calls, just like runtime code, and all parameter types use the same type system as the runtime program (such as Int and DType).
Because parameter expressions use the same grammar and types as runtime Mojo code, you can use many "dependent type" features. For example, you might want to define a helper function to concatenate two SIMD vectors:
fn concat[
    dtype: DType, ls_size: Int, rh_size: Int, //
](lhs: SIMD[dtype, ls_size], rhs: SIMD[dtype, rh_size]) -> SIMD[
    dtype, ls_size + rh_size
]:
    var result = SIMD[dtype, ls_size + rh_size]()

    @parameter
    for i in range(ls_size):
        result[i] = lhs[i]

    @parameter
    for j in range(rh_size):
        result[ls_size + j] = rhs[j]
    return result
result type: float32 length: 4
Note that the resulting length is the sum of the input vector lengths, and this is expressed with a simple + operation.
Powerful compile-time programming​
While simple expressions are useful, sometimes you want to write imperative compile-time logic with control flow. You can even do compile-time recursion. For instance, here is an example "tree reduction" algorithm that sums all elements of a vector recursively into a scalar:
fn slice[
    dtype: DType, size: Int, //
](x: SIMD[dtype, size], offset: Int) -> SIMD[dtype, size // 2]:
    alias new_size = size // 2
    var result = SIMD[dtype, new_size]()
    for i in range(new_size):
        result[i] = SIMD[dtype, 1](x[i + offset])
    return result


fn reduce_add(x: SIMD) -> Int:
    @parameter
    if x.size == 1:
        return Int(x[0])
    elif x.size == 2:
        return Int(x[0]) + Int(x[1])

    # Extract the top/bottom halves, add them, sum the elements.
    alias half_size = x.size // 2
    var lhs = slice(x, 0)
    var rhs = slice(x, half_size)
    return reduce_add(lhs + rhs)


def main():
    var x = SIMD[DType.int, 4](1, 2, 3, 4)
    print(x)
    print("Elements sum:", reduce_add(x))
[1, 2, 3, 4]
Elements sum: 10
This makes use of the @parameter decorator to create a parametric if condition, which is an if statement that runs at compile-time. It requires that its condition be a valid parameter expression, and ensures that only the live branch of the if statement is compiled into the program. (This is similar to use of the @parameter decorator with a for loop shown earlier.)
alias: named parameter expressions​
It is very common to want to name compile-time values. Whereas var defines a runtime value, we need a way to define a compile-time temporary value. For this, Mojo uses an alias declaration. At its simplest, alias can be used to define a constant value:
alias rows = 512
Some Mojo types use aliases to express enumerations. For example, the following code defines a Sentiment type that defines aliases for different sentiment values:
@fieldwise_init
struct Sentiment(Equatable, ImplicitlyCopyable):
    var _value: Int

    alias NEGATIVE = Sentiment(0)
    alias NEUTRAL = Sentiment(1)
    alias POSITIVE = Sentiment(2)

    fn __eq__(self, other: Self) -> Bool:
        return self._value == other._value

    fn __ne__(self, other: Self) -> Bool:
        return not (self == other)

fn is_happy(s: Sentiment):
    if s == Sentiment.POSITIVE:
        print("Yes. 😀")
    else:
        print("No. ☹️")
This pattern provides a type-safe enumeration.
The DType struct implements a simple enum using aliases like this. This allows clients to use values like DType.float32 in parameter expressions or runtime expressions.
Types are another common use for aliases. Because types are compile-time expressions, you can use an alias as a shorthand (or "typedef") for a parameterized type:
alias Float16 = SIMD[DType.float16, 1]
alias UInt8 = SIMD[DType.uint8, 1]

var x: Float16 = 0  # Float16 works like a "typedef"
(These aliases and others are actually defined in the simd module.)
Like var variables, aliases obey scope, and you can use local aliases within functions as you'd expect.
Parametric aliases​
A parametric alias is a compile-time expression that takes a list of parameters and returns a compile-time constant value:
alias AddOne[a: Int] : Int = a + 1

alias nine = AddOne(8)
As you can see in the previous example, a parametric alias is a little like a compile-time-only function. A regular function or method can also be invoked at compile time:
fn add_one(a: Int) -> Int:
    return a + 1

alias ten = add_one(9)
A major difference between a function and a parametric alias is that the value of an alias can be a type, while a function can't return a type as a value.
# Does not work--dynamic type values not permitted
fn int_type() -> AnyType:
    return Int

# Works
alias IntType = Int
Because the value of an alias can be a type, you can use parametric aliases to express new types:
alias TwoOfAKind[dt: DType] = SIMD[dt, 2]
twoFloats = TwoOfAKind[DType.float32](1.0, 2.0)

alias StringKeyDict[ValueType: Copyable & Movable] = Dict[String, ValueType]
var b: StringKeyDict[UInt8] = {"answer": 42}
Parametric alias signatures support the same features as parameterized structs or functions: infer-only parameters, keyword-only and optional parameters, automatic parameterization, and so on.
    alias Floats[size: Int, half_width: Bool = False] = SIMD[
        (DType.float16 if half_width else DType.float32), size
    ]
    var floats = Floats[2](6.0, 8.0)
    var half_floats = Floats[2, True](10.0, 12.0)
Fully-bound, partially-bound, and unbound types​
A parametric type with its parameters specified is said to be fully-bound. That is, all of its parameters are bound to values. As mentioned before, you can only instantiate a fully-bound type (sometimes called a concrete type).
However, parametric types can be unbound or partially bound in some contexts. For example, you can alias a partially-bound type to create a new type that requires fewer parameters:
alias StringKeyDict = Dict[String, _]
var b: StringKeyDict[UInt8] = {"answer": 42}
Here, StringKeyDict is a type alias for a Dict that takes String keys. The underscore _ in the parameter list indicates that the second parameter, V (the value type), is unbound. You specify the V parameter later, when you use StringKeyDict.
Partially-bound types versus parametric aliases
You may notice that this example is very similar to an example in the section on parametric aliases. For simple type aliases like this, you can use either a partially-bound type or a parametric alias. Parametric aliases provide a more flexible way to define type aliases, since you can define the order of the parameters, add default values, and so on.
Partially-bound and unbound types can provide a handy shortcut when defining parametric functions and aliases, called automatic parameterization.
You can also use partially-bound types as the type bound for an argument or parameter.
For example, given the following type:
struct MyType[s: String, i: Int, i2: Int, b: Bool = True]:
    pass
It can appear in code in the following forms:
* Fully bound, with all of its parameters specified:
def my_fn1(m1: MyType["Hello", 3, 4, True]):
    pass
* Partially bound, with some but not all of its parameters specified:
def my_fn2(m2: MyType["Hola", _, _, True]):
    pass
* Unbound, with no parameters specified:
def my_fn3(m3: MyType[_, _, _, _]):
    pass
You can also use the star-underscore expression *_ to unbind an arbitrary number of positional parameters at the end of a parameter list:
# These two types are equivalent
MyType["Hello", *_]
MyType["Hello", _, _, _]
The *_ expression specifically matches any parameters that can be specified by position (positional-only or positional-or-keyword). To unbind all parameters that can be specified by keyword (positional-or-keyword or keyword-only), use the double-star-underscore expression, **_.
@fieldwise_init
struct KeyWordStruct[pos_or_kw: Int, *, kw_only: Int = 10]:
    pass

# Unbind both pos_or_kw and kw_only parameters
fn use_kw_struct(k: KeyWordStruct[**_]):
    pass

def main():
    use_kw_struct(KeyWordStruct[10, kw_only=11]())
When a parameter is explicitly unbound with the _, *_, or **_ expressions, you must specify a value for that parameter to use the type. The default values of explicitly unbound parameters are ignored.
Partially-bound and unbound parametric types can be used in some contexts where the missing (unbound) parameters will be supplied later-such as in aliases and automatically parameterized functions.
Omitted parameters​
Mojo also supports an alternate format for unbound parameters where parameters are simply omitted from the expression:
@fieldwise_init
struct MyComplicatedType[a: Int = 7, /, b: Int = 8, *, c: Int, d: Int = 9]:
    pass

# Unbound
fn my_func(t: MyComplicatedType):
    pass
This is equivalent to fn my_func(t: MyComplicatedType[*_, **_]): pass. That is, all parameters (positional-only, positional-or-keyword, keyword-only) are unbound and their default values (if any) ignored.
Note that when an argument type is partially bound, default values will be bound:
# Partially bound
MyComplicatedType[1]
# Equivalent to
MyComplicatedType[1, 8, c=_, d=9]  # Uses default values for `b` and `d`.
This behavior with omitted parameters is currently supported for backwards compatibility. We intend to reconcile the behavior of omitted parameters and explicitly unbound parameters in the future.
Automatic parameterization​
Mojo supports "automatic" parameterization of functions and parametric aliases. If a function argument type or parameter type is partially-bound or unbound, the unbound parameters are automatically added as input parameters on the function. This is easier to understand with an example:
fn print_params(vec: SIMD):
    print(vec.type)
    print(vec.size)

var v = SIMD[DType.float64, 4](1.0, 2.0, 3.0, 4.0)
print_params(v)
float64
4
In the above example, the print_params function is automatically parameterized. The vec argument takes an argument of type SIMD[*_]. This is an unbound parameterized type-that is, it doesn't specify any parameter values for the type. Mojo treats the unbound parameters on vec as infer-only parameters on the function. This is roughly equivalent to the following code:
fn print_params2[t: DType, s: Int, //](vec: SIMD[t, s]):
    print(vec.dtype)
    print(vec.size)
When you call print_params() you must pass it a concrete instance of the SIMD type-that is, one with all of its parameters specified, like SIMD[DType.float64, 4]. The Mojo compiler infers the parameter values from the input argument.
With a manually parameterized function, you can access the input parameters by name (for example, t and s in the previous example). For an automatically parameterized function, you can access the parameters as attributes on the argument (for example, vec.type).
This ability to access a type's input parameters is not specific to automatically parameterized functions, you can use it anywhere. You can access the input parameters of a parameterized type as attributes on the type itself:
fn on_type():
    print(SIMD[DType.float32, 2].size) # prints 2
Or as attributes on an instance of the type:
fn on_instance():
    var x = SIMD[DType.int32, 2](4, 8)
    print(x.dtype)  # prints int32
You can even use this syntax in the function's signature to define a function's arguments and return type based on an argument's parameters. For example, if you want your function to take two SIMD vectors with the same type and size, you can write code like this:
fn interleave(v1: SIMD, v2: type_of(v1)) -> SIMD[v1.type, v1.size*2]:
    var result = SIMD[v1.type, v1.size*2]()
    for i in range(v1.size):
        result[i*2] = SIMD[v1.type, 1](v1[i])
        result[i*2+1] = SIMD[v1.type, 1](v2[i])
    return result

var a = SIMD[DType.int16, 4](1, 2, 3, 4)
var b = SIMD[DType.int16, 4](0, 0, 0, 0)
var c = interleave(a, b)
print(c)
[1, 0, 2, 0, 3, 0, 4, 0]
As shown in the example, you can use the magic type_of(x) call if you just want to match the type of an argument. In this case, it's more convenient and compact that writing the equivalent SIMD[v1.type, v1.size].
Automatic parameterization of parameters​
You can also take advantage of automatic parameterization in the parameter list of a function or parametric alias. For example:
fn foo[value: SIMD]():
    pass

# Equivalent to:
fn foo[dtype: DType, size: Int, //, value: SIMD[dtype, size]]():
    pass
Here's another example using a parametric alias:
alias Foo[S: SIMD] = Bar[S]

# Equivalent to:
alias Foo[dtype: DType, size: Int, //, S: SIMD[dtype, size]] = Bar[S]
Automatic parameterization with partially-bound types​
Mojo also supports automatic parameterization: with partially-bound parameterized types (that is, types with some but not all of the parameters specified).
For example, suppose we have a Fudge struct with three parameters:
@fieldwise_init
struct Fudge[sugar: Int, cream: Int, chocolate: Int = 7](Stringable):
    fn __str__(self) -> String:
        return String.write(
            "Fudge (", Self.sugar, ",", Self.cream, ",", Self.chocolate, ")"
        )
We can write a function that takes a Fudge argument with just one bound parameter (it's partially bound):
fn eat(f: Fudge[5, *_]):
    print("Ate " + String(f))
The eat() function takes a Fudge struct with the first parameter (sugar) bound to the value 5. The second and third parameters, cream and chocolate are unbound.
The unbound cream and chocolate parameters become implicit input parameters on the eat function. In practice, this is roughly equivalent to writing:
fn eat[cr: Int, ch: Int](f: Fudge[5, cr, ch]):
    print("Ate", String(f))
In both cases, we can call the function by passing in an instance with the cream and chocolate parameters bound:
eat(Fudge[5, 5, 7]())
eat(Fudge[5, 8, 9]())
Ate Fudge (5,5,7)
Ate Fudge (5,8,9)
If you try to pass in an argument with a sugar value other than 5, compilation fails, because it doesn't match the argument type:
eat(Fudge[12, 5, 7]())
# ERROR: invalid call to 'eat': argument #0 cannot be converted from 'Fudge[12, 5, 7]' to 'Fudge[5, 5, 7]'
You can also explicitly unbind individual parameters. This gives you more freedom in specifying unbound parameters.
For example, you might want to let the user specify values for sugar and chocolate, and leave cream constant. To do this, replace each unbound parameter value with a single underscore (_):
fn devour(f: Fudge[_, 6, _]):
    print("Devoured",  String(f))
Again, the unbound parameters (sugar and chocolate) are added as implicit input parameters on the function. This version is roughly equivalent to the following code, where these two values are explicitly bound to the input parameters, su and ch:
fn devour[su: Int, ch: Int](f: Fudge[su, 6, ch]):
    print("Devoured", String(f))
You can also specify parameters by keyword, or mix positional and keyword parameters, so the following function is roughly equivalent to the previous one: the first parameter, sugar is explicitly unbound with the underscore character. The chocolate parameter is unbound using the keyword syntax, chocolate=_. And cream is explicitly bound to the value 6:
fn devour(f: Fudge[_, chocolate=_, cream=6]):
    print("Devoured", String(f))
All three versions of the devour() function work with the following calls:
devour(Fudge[3, 6, 9]())
devour(Fudge[4, 6, 8]())
Devoured Fudge (3,6,9)
Devoured Fudge (4,6,8)
Legacy syntax (omitted parameters)​
You can also specify an unbound or partially-bound type by omitting parameters: for example:
fn nibble(f: Fudge[5]):
    print("Ate", String(f))

nibble(Fudge[5, 4, 7]())

Ate Fudge (5,4,7)
Here, Fudge[5] works like Fudge[5, *_] except in the handling of parameters with default values. Instead of discarding the default value of chocolate, Fudge[5] binds the default value immediately, making it equivalent to: Fudge[5, _, 7].
This means that the following code won't compile with the previous definition for the nibble() function, since it doesn't use the default value for chocolate:
nibble(Fudge[5, 5, 9]())
# ERROR: invalid call to 'nibble': argument #0 cannot be converted from 'Fudge[5, 5, 9]' to 'Fudge[5, 5, 7]'
TODO
We intend to reconcile the behavior of omitted parameters and explicitly unbound parameters in the future.
The rebind() builtin​
One of the consequences of Mojo not performing function instantiation in the parser like C++ is that Mojo cannot always figure out whether some parametric types are equal and complain about an invalid conversion. This typically occurs in static dispatch patterns. For example, the following code won't compile:
fn take_simd8(x: SIMD[DType.float32, 8]):
    pass

fn generic_simd[nelts: Int](x: SIMD[DType.float32, nelts]):
    @parameter
    if nelts == 8:
        take_simd8(x)
The parser will complain:
error: invalid call to 'take_simd8': argument #0 cannot be converted from
'SIMD[f32, nelts]' to 'SIMD[f32, 8]'
        take_simd8(x)
        ~~~~~~~~~~^~~
This is because the parser fully type-checks the function without instantiation, and the type of x is still SIMD[f32, nelts], and not SIMD[f32, 8], despite the static conditional. The remedy is to manually "rebind" the type of x, using the rebind builtin, which inserts a compile-time assert that the input and result types resolve to the same type after function instantiation:
fn take_simd8(x: SIMD[DType.float32, 8]):
    pass

fn generic_simd[nelts: Int](x: SIMD[DType.float32, nelts]):
    @parameter
    if nelts == 8:
        take_simd8(rebind[SIMD[DType.float32, 8]](x))
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Intro to pointers
A pointer is an indirect reference to one or more values stored in memory. The pointer is a value that holds an address to memory, and provides APIs to store and retrieve values to that memory. The value pointed to by a pointer is also known as a pointee.
The Mojo standard library includes several types of pointers, which provide different sets of features. All of these pointer types are generic-they can point to any type of value, and the value type is specified as a parameter. For example, the following code creates an OwnedPointer that points to an Int value:
var ptr: OwnedPointer[Int]
ptr = OwnedPointer(100)
The ptr variable has a value of type OwnedPointer[Int]. The pointer points to a value of type Int, as shown in Figure 1.

Figure 1. Pointer and pointee 
Accessing the memory-to retrieve or update a value-is called dereferencing the pointer. You can dereference a pointer by following the variable name with an empty pair of square brackets:
# Update an initialized value
ptr[] += 10
# Access an initialized value
print(ptr[])
Pointer terminology​
Before we jump into the pointer types, here are a few terms you'll run across. Some of them may already be familiar to you.
* Safe pointers: are designed to prevent memory errors. Unless you use one of the APIs that are specially designated as unsafe, you can use these pointers without worrying about memory issues like double-free or use-after-free.
* Nullable pointers: can point to an invalid memory location (typically 0, or a "null pointer"). Safe pointers aren't nullable.
* Smart pointers: own their pointees, which means that the value they point to may be deallocated when the pointer itself is destroyed. Non-owning pointers may point to values owned elsewhere, or may require some manual management of the value lifecycle.
* Memory allocation: some pointer types can allocate memory to store their pointees, while other pointers can only point to pre-existing values. Memory allocation can either be implicit (that is, performed automatically when initializing a pointer with a value) or explicit.
* Uninitialized memory: refers to memory locations that haven't been initialized with a value, which may therefore contain random data. Newly-allocated memory is uninitialized. The safe pointer types don't allow users to access memory that's uninitialized. Unsafe pointers can allocate a block of uninitialized memory locations and then initialize them one at a time. Being able to access uninitialized memory is unsafe by definition.
* Copyable types: can be copied implicitly (for example, by assigning a value to a variable). Also called implicitly copyable types.
copied_ptr = ptr
Explicitly copyable types require the user to request a copy, using a constructor with a keyword argument:
copied_owned_ptr = OwnedPointer(other=owned_ptr)
Pointer types​
The Mojo standard library includes several pointer types with different characteristics:
* Pointer is a safe pointer that points to a single value that it doesn't own.
* OwnedPointer is a smart pointer that points to a single value, and maintains exclusive ownership of that value.
* ArcPointer is a reference-counted smart pointer that points to an owned value with ownership potentially shared with other instances of ArcPointer.
* UnsafePointer points to one or more consecutive memory locations, and can refer to uninitialized memory.
Table 1 summarizes the different types of pointers:

Pointer
OwnedPointer
ArcPointer
UnsafePointer
Safe
Yes
Yes
Yes
No
Allocates memory
No
Implicitly 1
Implicitly 1
Explicitly
Owns pointee(s)
No
Yes
Yes
No 2
Copyable
Yes
No 3
Yes
Yes
Nullable
No
No
No
Yes
Can point to uninitialized memory
No
No
No
Yes
Can point to multiple values (array-like access)
No
No
No
Yes
Table 1. Pointer types 
1 OwnedPointer and ArcPointer implicitly allocate memory when you initialize the pointer with a value.
2 UnsafePointer provides unsafe methods for initializing and destroying instances of the stored type. The user is responsible for managing the lifecycle of stored values.
3 OwnedPointer is explicitly copyable, but explicitly copying an OwnedPointer copies the stored value into a new OwnedPointer.
The following sections provide more details on each pointer type.
Pointer​
The Pointer type is a safe pointer that points to a initialized value that it doesn't own. Some example use cases for a Pointer include:
* Storing a reference to a related type. For example, a list's iterator object might hold a Pointer back to the original list.
* Passing the memory location for a single value to external code via external_call().
* Where you need an API to return a long-lived reference to a value. (Currently the iterators for standard library collection types like List return pointers.)
You can construct a Pointer to an existing value by calling the constructor with the to keyword argument:
ptr = Pointer(to=some_value)
You can also create a Pointer by copying an existing Pointer.
A Pointer carries an origin for the stored value, so Mojo can track the lifetime of the referenced value.
OwnedPointer​
The OwnedPointer type is a smart pointer designed for cases where there is single ownership of the underlying data. An OwnedPointer points to a single item, which is passed in when you initialize the OwnedPointer. The OwnedPointer allocates memory and moves or copies the value into the reserved memory.
o_ptr = OwnedPointer(some_big_struct)
An owned pointer can hold almost any type of item, but when constructing an OwnedPointer, the stored item must be either Movable or Copyable.
Since an OwnedPointer is designed to enforce single ownership, the pointer itself can be moved, but not copied.
Note: Currently, you can't create an Optional[OwnedPointer[T]] because the Optional type only works with types that are both movable and copyable. This restricts some use cases that would otherwise be a natural fit for OwnedPointer, including self-referential data structures like linked lists and trees. (Until this use case is supported for OwnedPointer, it's possible to use ArcPointer where you need a smart pointer that can be Optional.)
ArcPointer​
An ArcPointer is a reference-counted smart pointer, ideal for shared resources where the last owner for a given value may not be clear. Like an OwnedPointer, it points to a single value, and it allocates memory when you initialize the ArcPointer with a value:
attributesDict: Dict[String, String] = {}
attributes = ArcPointer(attributesDict)
Unlike an OwnedPointer, an ArcPointer can be freely copied. All instances of a given ArcPointer share a reference count, which is incremented whenever the ArcPointer is copied and decremented whenever an instance is destroyed. When the reference count reaches zero, the stored value is destroyed and the allocated memory is freed.
You can use ArcPointer to implement safe reference-semantic types. For example, in the following code snippet SharedDict uses an ArcPointer to store a dictionary. Copying an instance of SharedDict only copies the ArcPointer, not the dictionary, which is shared between all of the copies.
from memory import ArcPointer

struct SharedDict:
    var attributes: ArcPointer[Dict[String, String]]

    fn __init__(out self):
        attributesDict: Dict[String, String] = {}
        self.attributes = ArcPointer(attributesDict)

    fn __copyinit__(out self, other: Self):
        self.attributes = other.attributes

    def __setitem__(mut self, key: String, value: String):
        self.attributes[][key] = value

    def __getitem__(self, key: String) -> String:
        return self.attributes[].get(key, default="")

def main():
    thing1 = SharedDict()
    thing2 = thing1
    thing1["Flip"] = "Flop"
    print(thing2["Flip"])
Note: The reference count is stored using an Atomic value to ensure that updates to the reference count are thread-safe. However, Mojo doesn't currently enforce exclusive access across thread boundaries, so it's possible to form race conditions.
UnsafePointer​
UnsafePointer is a low-level pointer that can access a block of contiguous memory locations, which might be uninitialized. It's analogous to a raw pointer in the C and C++ programming languages. UnsafePointer provides unsafe methods for initializing and destroying stored values, as well as for accessing the values once they're initialized.
As the name suggests, UnsafePointer doesn't provide any memory safety guarantees, so you should reserve it for cases when none of the other pointer types will do the job. Here are some use cases where you might want to use an UnsafePointer:
* Building a high-performance array-like structure, such as List or Tensor. A single UnsafePointer can access many values, and gives you a lot of control over how you allocate, use, and deallocate memory. Being able to access uninitialized memory means that you can preallocate a block of memory, and initialize values incrementally as they are added to the collection.
* Interacting with external libraries including C++ and Python. You can useUnsafePointer to pass a buffer full of data to or from an external library.

Unsafe pointers
The UnsafePointer type is one of several pointer types available in the standard library to indirectly reference locations in memory.
You can use an UnsafePointer to dynamically allocate and free memory, or to point to memory allocated by some other piece of code. You can use these pointers to write code that interacts with low-level interfaces, to interface with other programming languages, or to build array-like data structures. But as the name suggests, they're inherently unsafe. For example, when using unsafe pointers, you're responsible for ensuring that memory gets allocated and freed correctly.
In general, you should prefer safe pointer types when possible, reserving UnsafePointer for those use cases where no other pointer type works. For a comparison of standard library pointer types, see Intro to pointers.
LegacyUnsafePointer
The implementation of UnsafePointer documented here replaces an earlier version with a slightly different API. The old version has been renamed to LegacyUnsafePointer and will be deprecated.
The UnsafePointer reference page briefly describes the differences between the new and old versions.
To ease this transition, the library currently supports implicit conversion between LegacyUnsafePointer and UnsafePointer, so you can pass a legacy pointer to an UnsafePointer function, and vice versa.
For more details on migrating to the new UnsafePointer, see the migration guide provided in the UnsafePointer v2 proposal.
Unsafe pointer basics​
An UnsafePointer is a type that holds an address to memory. You can store and retrieve values in that memory. The UnsafePointer type is generic-it can point to any type of value, and the value type is specified as a parameter. The value pointed to by a pointer is sometimes called a pointee.
# Allocate memory to hold a value
var ptr = alloc[Int](1)
# Initialize the allocated memory
ptr.init_pointee_copy(100)

Figure 1. Pointer and pointee 
Accessing the memory-to retrieve or update a value-is called dereferencing the pointer. You can dereference a pointer by following the variable name with an empty pair of square brackets:
# Update an initialized value
ptr[] += 10
# Access an initialized value
print(ptr[])
110
You can also allocate memory to hold multiple values to build array-like structures. For details, see Storing multiple values.
Lifecycle of a pointer​
At any given time, a pointer can be in one of several states:
* Uninitialized. Just like any variable, a variable of type UnsafePointer can be declared but uninitialized.
var ptr: UnsafePointer[Int, MutOrigin.external]
* Null. A null pointer has an address of 0, indicating an invalid pointer.
ptr = {}
* Pointing to allocated, uninitialized memory. The alloc() function returns a pointer to a newly-allocated block of memory with space for the specified number of elements of the pointee's type.
ptr = alloc[Int](1)
Trying to dereference a pointer to uninitialized memory results in undefined behavior.
* Pointing to initialized memory. You can initialize an allocated, uninitialized pointer by moving or copying an existing value into the memory. Or you can get a pointer to an existing value by calling the constructor with the to keyword argument.
ptr.init_pointee_copy(value)
# or
ptr.init_pointee_move(value^)
# or
ptr = UnsafePointer(to=value)
Once the value is initialized, you can read or mutate it using the dereference syntax:
var oldValue = ptr[]
ptr[] = newValue
* Dangling. When you free the pointer's allocated memory, you're left with a dangling pointer. The address still points to its previous location, but the memory is no longer allocated to this pointer. Trying to dereference the pointer, or calling any method that would access the memory location results in undefined behavior.
ptr.free()
The following diagram shows the lifecycle of an UnsafePointer:

Figure 2. Lifecycle of an UnsafePointer 
Allocating memory​
Use the alloc() function to allocate memory. This function returns a new pointer pointing to the requested memory. You can allocate space for one or more values of the pointee's type.
var ptr = alloc[Int](10) # Allocate space for 10 Int values
The allocated space is uninitialized-like a variable that's been declared but not initialized.
Initializing the pointee​
To initialize allocated memory, UnsafePointer provides the init_pointee_copy() and init_pointee_move() methods. For example:
ptr.init_pointee_copy(my_value)
To move a value into the pointer's memory location, use init_pointee_move():
str_ptr.init_pointee_move(my_string^)
Note that to move the value, you usually need to add the transfer sigil (^), unless the value is a trivial type (like Int) or a newly-constructed, "owned" value:
str_ptr.init_pointee_move("Owned string")
Alternately, you can get a pointer to an existing value by calling the UnsafePointer constructor with the keyword to argument. This is useful for getting a pointer to a value on the stack, for example.
var counter: Int = 5
var ptr = UnsafePointer(to=counter)
Note that when calling UnsafePointer(to=value), you don't need to allocate memory, since you're pointing to an existing value.
Dereferencing pointers​
Use the [] dereference operator to access the value stored at a pointer (the "pointee").
# Read from pointee
print(ptr[])
# mutate pointee
ptr[] = 0
5
If you've allocated space for multiple values, you can use subscript syntax to access the values, as if they were an array, like ptr[3]. The empty subscript [] has the same meaning as [0].
Caution
The dereference operator assumes that the memory being dereferenced is initialized. Dereferencing uninitialized memory results in undefined behavior.
You cannot safely use the dereference operator on uninitialized memory, even to initialize a pointee. This is because assigning to a dereferenced pointer calls lifecycle methods on the existing pointee (such as the destructor, move constructor or copy constructor).
var str_ptr = alloc[String](1)
# str_ptr[] = "Testing" # Undefined behavior!
str_ptr.init_pointee_move("Testing")
str_ptr[] += " pointers" # Works now
Destroying or removing values​
The take_pointee() method moves the pointee from the memory location pointed to by ptr. This is a consuming move-it invokes __moveinit__() on the destination value. It leaves the memory location uninitialized.
The destroy_pointee() method calls the destructor on the pointee, and leaves the memory location pointed to by ptr uninitialized.
Both take_pointee() and destroy_pointee() require that the pointer is non-null, and the memory location contains a valid, initialized value of the pointee's type; otherwise the function results in undefined behavior.
The move_pointee_into(self, dst) method moves the pointee from one pointer location to another. Both pointers must be non-null. The source location must contain a valid, initialized value of the pointee's type, and is left uninitialized after the call. The destination location is assumed to be uninitialized-if it contains a valid value, that value's destructor is not run. The value from the source location is moved to the destination location as a consuming move. This function also has undefined behavior if any of its prerequisites is not met.
Freeing memory​
Calling free() on a pointer frees the memory allocated by the pointer. It doesn't call the destructors on any values stored in the memory-you need to do that explicitly (for example, using destroy_pointee() or one of the other functions described in Destroying or removing values).
Disposing of a pointer without freeing the associated memory can result in a memory leak-where your program keeps taking more and more memory, because not all allocated memory is being freed.
On the other hand, if you have multiple copies of a pointer accessing the same memory, you need to make sure you only call free() on one of them. Freeing the same memory twice is also an error.
After freeing a pointer's memory, you're left with a dangling pointer-its address still points to the freed memory. Any attempt to access the memory, like dereferencing the pointer results in undefined behavior.
Storing multiple values​
As mentioned in Allocating memory, you can use an UnsafePointer to allocate memory for multiple values. The memory is allocated as a single, contiguous block. Pointers support arithmetic: adding an integer to a pointer returns a new pointer offset by the specified number of values from the original pointer:
var third_ptr = first_ptr + 2
Pointers also support subtraction, as well as in-place addition and subtraction:
# Advance the pointer one element:
ptr += 1

Figure 3. Pointer arithmetic 
For example, the following example allocates memory to store 6 Float64 values, and initializes them all to zero.
var float_ptr = alloc[Float64](6)
for offset in range(6):
    (float_ptr+offset).init_pointee_copy(0.0)
Once the values are initialized, you can access them using subscript syntax:
float_ptr[2] = 3.0
for offset in range(6):
    print(float_ptr[offset], end=", ")
0.0, 0.0, 3.0, 0.0, 0.0, 0.0,
UnsafePointer and origins​
The UnsafePointer struct has an origin parameter to track the origin of the memory it points to.
For pointers initialized with the to keyword argument, the origin is set to the origin of the pointee. For example, in the following code, s_ptr.origin is the same as the origin of s:
s = "Testing"
s_ptr = UnsafePointer(to=s)
When allocating memory with the alloc() function, the returned pointer has an origin value of MutOrigin.external. This value represents an origin that is mutable and does not alias any existing value. (For example, it does not point to the memory allocated for any other variable.) This memory isn't tracked by Mojo's lifetime checker, so you're responsible for freeing it.
If you're using a pointer in the implementation of a struct, you usually don't have to worry about the origin, as long as the pointer isn't exposed outside of of the struct. For example, if you implement a static array type that allocates memory in its constructor, deallocates it in its destructor, and doesn't expose the pointer outside of the struct, the default origin is fine.
But if the struct exposes a pointer to that memory, you need to set the origin appropriately. For example, the List type has an unsafe_ptr() method that returns an UnsafePointer to the underlying storage. In this case, the returned pointer should share the origin of the list, since the list is the logical owner of the storage.
That method looks something like this:
fn unsafe_ptr(
    ref self,
) -> UnsafePointer[T, origin_of(self)]:

    return self.data.unsafe_origin_cast[origin_of(self)]()
This returns a copy of the original pointer, with the origin set to match the origin and mutability of the self value.
A method like this is unsafe, but setting the correct origin makes it safer, since the compiler knows that the pointer is referring to data owned by the list.
Working with foreign pointers​
When exchanging data with other programming languages, you may need to construct an UnsafePointer from a foreign pointer. Mojo restricts creating UnsafePointer instances from arbitrary addresses, to avoid users accidentally creating pointers that alias each other (that is, two pointers that refer to the same location). However, there are specific methods you can use to get an UnsafePointer from a Python or C/C++ pointer.
When dealing with memory allocated elsewhere, you need to be aware of who's responsible for freeing the memory. Freeing memory allocated elsewhere can result in undefined behavior.
When working with some foreign functions, you may need to supply a pointer with no specific type (a type-erased pointer, or "void pointer" in C/C++). This is equivalent to a Mojo OpaquePointer.
You also need to be aware of the format of the data stored in memory, including data types and byte order. For more information, see Converting data: bitcasting and byte order.
Creating a Mojo pointer from a raw memory address​
You can create a UnsafePointer from a raw memory address using the unsafe_from_address initializer.
fn write_to_address(mmio_address: Int, value: Int32):
    var ptr = UnsafePointer[Int32, MutOrigin.external](
      unsafe_from_address=mmio_address
    )

     # Writing to a raw memory address may require a volatile load/store as the
     # operation may have side effects not visible to the compiler.
     # You can specify this using the `volatile` parameter.
    ptr.store[volatile = True](value)
This is unsafe, as the caller must ensure the address is valid before writing to it, and that the memory is initialized before reading from it. The caller must also ensure the pointer's origin and mutability is valid for the address, failure to to do may result in undefined behavior.
Creating a Mojo pointer from a Python pointer​
The PythonObject type defines an unsafe_get_as_pointer() method to construct an UnsafePointer from a Python address.
For example, the following code creates a NumPy array and then accesses the data using a Mojo pointer:
from python import Python

def share_array():
    np = Python.import_module("numpy")
    arr = np.array(Python.list(1, 2, 3, 4, 5, 6, 7, 8, 9))
    ptr = arr.ctypes.data.unsafe_get_as_pointer[DType.int64]()
    for i in range(9):
        print(ptr[i], end=", ")
    print()

def main():
    share_array()
1, 2, 3, 4, 5, 6, 7, 8, 9,
This example uses the NumPy ndarray.ctype attribute to access the raw pointer to the underlying storage (ndarray.ctype.data). The unsafe_get_as_pointer() method constructs an UnsafePointer to this address.
Working with C/C++ pointers​
If you call a C/C++ function that returns a pointer using the external_call function, you can specify the return type as an UnsafePointer, and Mojo will handle the type conversion for you.
Notably, the origin parameter when working across FFI boundaries should often be set to (MutOrigin/ImmutOrigin).external, since the pointer points to memory external to the mojo program.
from sys.ffi import external_call

def get_foreign_pointer() -> UnsafePointer[Int, MutOrigin.external]:
    var ptr = external_call[
        "my_c_function", # external function name
        UnsafePointer[Int, MutOrigin.external] # return type
    ]()
    return ptr
Opaque pointers​
The OpaquePointer type is a pointer that does not have a specific type. In other languages, this is usually called a type-erased pointer or a void pointer. Opaque pointers are usually used when interfacing with non-Mojo code, such as a C library function that takes a void pointer.
OpaquePointer is actually a type alias for UnsafePointer[NoneType], so it has the same API as any other UnsafePointer.
You can't dereference an opaque pointer, but you can cast it to a specific type using the bitcast() method. Similarly, you can create an opaque pointer from an existing pointer by bitcasting to NoneType. For example:
var str = "Hello, world!"
var str_ptr = UnsafePointer(to=str)
var opaque_ptr = str_ptr.bitcast[NoneType]()
# ... call some foreign function that takes a void pointer
Converting data: bitcasting and byte order​
Bitcasting a pointer returns a new pointer that has the same memory location, but a new data type. This can be useful if you need to access different types of data from a single area of memory. This can happen when you're reading binary files, like image files, or receiving data over the network.
The following sample processes a format that consists of chunks of data, where each chunk contains a variable number of 32-bit integers. Each chunk begins with an 8-bit integer that identifies the number of values in the chunk.
def read_chunks(
  var ptr: UnsafePointer[mut=False, UInt8],
) -> List[List[UInt32]]:
    chunks = List[List[UInt32]]()
    # A chunk size of 0 indicates the end of the data
    chunk_size = Int(ptr[])
    while (chunk_size > 0):
        # Skip the 1 byte chunk_size and get a pointer to the first
        # UInt32 in the chunk
        ui32_ptr = (ptr + 1).bitcast[UInt32]()
        chunk = List[UInt32](capacity=chunk_size)
        for i in range(chunk_size):
            chunk.append(ui32_ptr[i])
        chunks.append(chunk)
        # Move our pointer to the next byte after the current chunk
        ptr += (1 + 4 * chunk_size)
        # Read the size of the next chunk
        chunk_size = Int(ptr[])
    return chunks
When dealing with data read in from a file or from the network, you may also need to deal with byte order. Most systems use little-endian byte order (also called least-signficicant byte, or LSB) where the least-significant byte in a multibyte value comes first. For example, the number 1001 can be represented in hexadecimal as 0x03E9, where E9 is the least-significant byte. Represented as a 16-bit little-endian integer, the two bytes are ordered E9 03. As a 32-bit integer, it would be represented as E9 03 00 00.
Big-endian or most-significant byte (MSB) ordering is the opposite: in the 32-bit case, 00 00 03 E9. MSB ordering is frequently used in file formats and when transmitting data over the network. You can use the byte_swap() function to swap the byte order of a SIMD value from big-endian to little-endian or the reverse. For example, if the method above was reading big-endian data, you'd just need to change a single line:
chunk.append(byte_swap(ui32_ptr[i]))
Working with SIMD vectors​
The UnsafePointer type includes load() and store() methods for performing aligned loads and stores of scalar values. It also has methods supporting strided load/store and gather/scatter.
Strided load loads values from memory into a SIMD vector using an offset (the "stride") between successive memory addresses. This can be useful for extracting rows or columns from tabular data, or for extracting individual values from structured data. For example, consider the data for an RGB image, where each pixel is made up of three 8-bit values, for red, green, and blue. If you want to access just the red values, you can use a strided load or store.

Figure 4. Strided load 
The following function uses the strided_load() and strided_store() methods to invert the red pixel values in an image, 8 values at a time. (Note that this function only handles images where the number of pixels is evenly divisible by eight.)
def invert_red_channel(ptr: UnsafePointer[mut=True, UInt8], pixel_count: Int):
    # number of values loaded or stored at a time
    alias simd_width = 8
    # bytes per pixel, which is also the stride size
    bpp = 3
    for i in range(0, pixel_count * bpp, simd_width * bpp):
        red_values = ptr.offset(i).strided_load[width=simd_width](bpp)
        # Invert values and store them in their original locations
        ptr.offset(i).strided_store[width=simd_width](~red_values, bpp)
The gather() and scatter() methods let you load or store a set of values that are stored in arbitrary locations. You do this by passing in a SIMD vector of offsets to the current pointer. For example, when using gather(), the nth value in the vector is loaded from (pointer address) + offset[n].
Safety​
Unsafe pointers are unsafe for several reasons:
* Memory management is up to the user. You need to manually allocate and free memory, and/or be aware of when other APIs are allocating or freeing memory for you.
* UnsafePointer values are nullable-that is, the pointer is not guaranteed to point to anything. And even when a pointer points to allocated memory, that memory may not be initialized.
* UnsafePointer does have an origin parameter so Mojo can track the origin of the data it points to, but it also provides unsafe APIs. For example, when you do pointer arithmetic, the compiler doesn't do any bounds checking.Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit


Edit this page


Intro to GPUs
GPUs are essential for high-performance computation, but programming them has historically been a highly specialized skill. Mojo represents a chance to rethink GPU programming and make it more approachable. But if you've never programmed a GPU before, you first need to understand how the GPU hardware and execution model is different from a CPU. That's what this page is all about-there's no code here, so if you already understand GPU hardware, you can skip to the Get started tutorial or GPU programming fundamentals.
GPU architecture overview​
Graphics Processing Units (GPUs) and Central Processing Units (CPUs) represent fundamentally different approaches to computation. While CPUs feature a few powerful cores optimized for sequential processing and complex decision making, GPUs contain thousands of smaller, simpler cores designed for parallel processing. These simpler cores lack sophisticated features like branch prediction or deep instruction pipelines, but excel at performing the same operation across large datasets simultaneously.
Modern systems take advantage of both CPU and GPU processors' strengths by having the CPU handle primary program flow and complex logic, while offloading parallel computations to the GPU through specialized APIs. Figure 1 illustrates some of the architectural differences between the two, which we'll discuss further below.

Figure 1. High-level comparison of CPU vs GPU architecture, based on CUDA C++ Programming Guide. 
The basic building block of a GPU is called a streaming multiprocessor (SM) on NVIDIA GPUs or a compute unit (CU) on AMD GPUs (they're the same idea and we'll refer to them both as SM). SMs sit between the high-level GPU control logic and the individual execution units, acting as self-contained processing factories that can operate independently and in parallel.
AMD and NVIDIA use different terminology to describe their GPU hardware and programming models. For simplicity, we typically use the NVIDIA terminology throughout the Mojo Manual because it's used more widely in the industry.
Multiple SMs are arranged on a single GPU chip, with each SM capable of handling multiple workloads simultaneously. The GPU's global scheduler assigns work to individual SMs, while the memory controller manages data flow between the SMs and various memory hierarchies (global memory, L2 cache, etc.).
The number of SMs in a GPU can vary significantly based on the model and intended use case, from a handful in entry-level GPUs to dozens or even hundreds in high-end professional cards. This scalable architecture enables GPUs to maintain excellent performance across different workload sizes and types.
Each SM contains several essential components:
* CUDA Cores (NVIDIA)/Stream Processors (AMD): These are the basic arithmetic logic units (ALUs) that perform integer and floating-point calculations. A single SM can contain dozens or hundreds of these cores.
* Tensor Cores (NVIDIA)/Matrix Cores (AMD): Specialized units optimized for matrix multiplication and convolution operations.
* Special Function Units (SFUs): Handle complex mathematical operations like trigonometry, square roots, and exponential functions.
* Register Files: Ultra-fast storage that holds intermediate results and thread-specific data. Modern SMs can have hundreds of kilobytes of register space shared among active threads.
* Shared Memory/L1 Cache: A programmable, low-latency memory space that enables data sharing between threads. This memory is typically configurable between shared memory and L1 cache functions.
* Load/Store Units: Manage data movement between different memory spaces, handling memory access requests from threads.

Figure 2. High-level architecture of a streaming multiprocessor (SM). (Click to enlarge.) 
GPU execution model​
A GPU kernel is simply a function that runs on a GPU, executing a specific computation on a large dataset in parallel across thousands or millions of threads (also known as work items on AMD GPUs). You might already be familiar with threads when programming for a CPU, but GPU threads are different. On a CPU, threads are managed by the operating system and can perform completely independent tasks, such as managing a user interface, fetching data from a database, and so on. But on a GPU, threads are managed by the GPU itself. For a given kernel function, all the threads on a GPU execute the same function, but they each work on a different part of the data.
A grid is the top-level organizational structure of the threads executing a kernel function on a GPU. A grid consists of multiple thread blocks (or workgroups on AMD GPUs), which are further divided into individual threads that execute the kernel function concurrently.

Figure 3. Hierarchy of threads running on a GPU, showing the relationship of the grid, thread blocks, warps, and individual threads, based on HIP Programming Guide 
The division of a grid into thread blocks serves multiple purposes:
* It breaks down the overall workload-managed by the grid-into smaller, more manageable portions that can be processed independently. This division allows for better resource utilization and scheduling flexibility across multiple SMs in the GPU.
* Thread blocks provide a scope for threads to collaborate through shared memory and synchronization primitives, enabling efficient parallel algorithms and data sharing patterns.
* Thread blocks help with scalability by allowing the same program to run efficiently across different GPU architectures, as the hardware can automatically distribute blocks based on available resources.
You must specify the number of thread blocks in a grid and how they are arranged across one, two, or three dimensions. Typically, you determine the dimensions of the grid based on the dimensionality of the data to process. For example, you might choose a 1-dimensional grid for processing large vectors, a 2-dimensional grid for processing matrices, and a 3-dimensional grid for processing the frames of a video. The GPU assigns each block within the grid a unique block index that determines its position within the grid.
Similarly, you also must specify the number of threads per thread block and how they are arranged across one, two, or three dimensions. The GPU also assigns each thread within a block a unique thread index that determines its position within the block. The combination of block index and thread index uniquely identify the position of a thread within the overall grid.
When a GPU assigns a thread block to execute on an SM, the SM divides the thread block into subsets of threads called a warp (or wavefront on AMD GPUs). The size of a warp depends on the GPU architecture, but most modern GPUs currently use a warp size of 32 or 64 threads.
If a thread block contains a number of threads not evenly divisible by the warp size, the SM creates a partially filled final warp that still consumes the full warp's resources. For example, if a thread block has 100 threads and the warp size is 32, the SM creates:
* 3 full warps of 32 threads each (96 threads total).
* 1 partial warp with only 4 active threads but still occupying a full warp's worth of resources (32 thread slots).
The SM effectively disables the unused thread slots in partial warps, but these slots still consume hardware resources. For this reason, you generally should make thread block sizes a multiple of the warp size to optimize resource usage.
Each thread in a warp executes the same instruction at the same time on different data, following the single instruction, multiple threads (SIMT) execution model. If threads within a warp take different execution paths (called warp divergence), the warp serially executes each branch path taken, disabling threads that are not on that path. This means that optimal performance is achieved when all threads in a warp follow the same execution path.
An SM can actively manage multiple warps from different thread blocks simultaneously, helping keep execution units busy. For example, the warp scheduler can quickly switch to another ready warp if the current warp's threads must wait for memory access.
Warps deliver several key performance advantages:
* The hardware needs to manage only warps instead of individual threads, reducing scheduling overhead.
* Threads in a warp can access contiguous memory locations efficiently through memory coalescing.
* The hardware automatically synchronizes threads within a warp, eliminating the need for explicit synchronization.
* The warp scheduler can hide memory latency by switching between warps, maximizing compute resource utilization.
So far, we've explained some basic concepts about the GPU hardware and how the GPU executes parallel processes using threads, but without showing any code. To start programming a GPU with Mojo, you can either read GPU programming fundamentals for an overview of the Mojo programming model, or jump right into a project with the tutorial to Get started with GPU programming.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit



Edit this page



GPU programming fundamentals
This guide explores the fundamentals of GPU programming using the Mojo programming language, covering essential concepts and techniques for developing GPU-accelerated applications that can work on a variety of supported GPUs from different vendors.
Key topics covered in this guide:
* Understanding the CPU-GPU programming model.
* Working with Mojo's GPU support through the standard library.
* Managing GPU devices and contexts using DeviceContext.
* Writing and executing kernel functions for parallel computation.
* Memory management and data transfer between CPU and GPU.
* Organizing threads and thread blocks for optimal performance.
Before diving into GPU programming, ensure you have a compatible GPU and the necessary development environment installed. And if you're new to GPU programming, we suggest you read the Intro to GPU architectures.
Overview of GPU programming in Mojo​
The Mojo language, including its standard library and open source MAX kernels library, allow you to develop GPU-enabled applications. See the What are the GPU requirements? section of the documentation for a list of currently supported GPUs and additional software requirements.
GPU support in the Mojo standard library​
The gpu package of the Mojo standard library includes several subpackages for interacting with GPUs, with the gpu.host package providing most of the commonly used APIs. However, the sys package contains a few basic introspection functions for determining whether a system has a supported GPU:
* has_accelerator(): Returns True if the host system has an accelerator and False otherwise.
* has_amd_gpu_accelerator(): Returns True if the host system has an AMD GPU and False otherwise.
* has_apple_gpu_accelerator(): Returns True if the host system has an Apple silicon GPU and False otherwise.
* has_nvidia_gpu_accelerator(): Returns True if the host system has an NVIDIA GPU and False otherwise.
These functions are useful for conditional compilation or execution depending on whether a supported GPU is available.
detect_gpu.mojo
from sys import has_accelerator

def main():
    @parameter
    if has_accelerator():
        print("GPU detected")
        # Enable GPU processing
    else:
        print("No GPU detected")
        # Print error or fall back to CPU-only execution
Mojo requires a compatible GPU development environment to compile kernel functions, otherwise it raises a compile-time error. In this example, we're using the @parameter decorator to evaluate the has_accelerator() function at compile time and compile only the corresponding branch of the if statement. As a result, if you don't have a compatible GPU development environment, you'll see the following message when you run the program:
No GPU detected
GPU programming model​
GPU programming follows a distinct pattern where work is divided between the CPU and GPU:
* The CPU (host) manages program flow and coordinates GPU operations.
* The GPU (device) executes parallel computations across many threads.
* You must explicitly manage data exchange between host and device memory.
A GPU program generally follows these steps:
1. Initialize data in host (CPU) memory.
2. Allocate device (GPU) memory and transfer data from host to device memory.
3. Execute a kernel function on the GPU to process the data.
4. Transfer results back from device to host memory.
This process typically runs asynchronously, allowing the CPU to perform other tasks while the GPU processes data. Any time that the CPU needs to ensure that the GPU has completed an operation, such as before it copies kernel results from device memory, it must first explicitly synchronize with the GPU as described in Asynchronous operation and synchronizing the CPU and GPU.
A simple example helps to understand this programming model. We'll not go into detail about the specific APIs at this point other than the included comments, but all of the types, functions, and methods are discussed in more detail in later sections of this document.
scalar_add_checked.mojo
from math import iota
from sys import exit, has_accelerator

from gpu.host import DeviceContext
from gpu import block_dim, block_idx, thread_idx

alias num_elements = 20

fn scalar_add(
    vector: UnsafePointer[Float32, MutAnyOrigin],
    size: Int,
    scalar: Float32,
):
    """
    Kernel function to add a scalar to all elements of a vector.

    This kernel function adds a scalar value to each element of a vector stored
    in GPU memory. The input vector is modified in place.

    Args:
        vector: Pointer to the input vector.
        size: Number of elements in the vector.
        scalar: Scalar to add to the vector.

    """

    # Calculate the global thread index within the entire grid. Each thread
    # processes one element of the vector.
    #
    # block_idx.x: index of the current thread block.
    # block_dim.x: number of threads per block.
    # thread_idx.x: index of the current thread within its block.
    idx = block_idx.x * block_dim.x + thread_idx.x

    # Bounds checking: ensure we don't access memory beyond the vector size.
    # This is crucial when the number of threads doesn't exactly match vector
    # size.
    if idx < UInt(size):
        # Each thread adds the scalar to its corresponding vector element
        # This operation happens in parallel across all GPU threads
        vector[idx] += scalar


def main():
    @parameter
    if not has_accelerator():
        print("No GPUs detected")
        exit(0)
    else:
        # Initialize GPU context for device 0 (default GPU device).
        ctx = DeviceContext()

        # Create a buffer in host (CPU) memory to store our input data
        host_buffer = ctx.enqueue_create_host_buffer[DType.float32](
            num_elements
        )

        # Wait for buffer creation to complete.
        ctx.synchronize()

        # Fill the host buffer with sequential numbers (0, 1, 2, ..., size-1).
        iota(host_buffer.unsafe_ptr(), num_elements)
        print("Original host buffer:", host_buffer)

        # Create a buffer in device (GPU) memory to store data for computation.
        device_buffer = ctx.enqueue_create_buffer[DType.float32](num_elements)

        # Copy data from host memory to device memory for GPU processing.
        ctx.enqueue_copy(src_buf=host_buffer, dst_buf=device_buffer)

        # Compile the scalar_add kernel function for execution on the GPU.
        scalar_add_kernel = ctx.compile_function_checked[
            scalar_add, scalar_add
        ]()

        # Launch the GPU kernel with the following arguments:
        #
        # - device_buffer: GPU memory containing our vector data
        # - num_elements: number of elements in the vector
        # - Float32(20.0): the scalar value to add to each element
        # - grid_dim=1: use 1 thread block
        # - block_dim=num_elements: use 'num_elements' threads per block (one
        #   thread per vector element)
        ctx.enqueue_function_checked(
            scalar_add_kernel,
            device_buffer,
            num_elements,
            Float32(20.0),
            grid_dim=1,
            block_dim=num_elements,
        )

        # Copy the computed results back from device memory to host memory.
        ctx.enqueue_copy(src_buf=device_buffer, dst_buf=host_buffer)

        # Wait for all GPU operations to complete.
        ctx.synchronize()

        # Display the final results after GPU computation.
        print("Modified host buffer:", host_buffer)
This application produces the following output:
Original host buffer: HostBuffer([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0])
Modified host buffer: HostBuffer([20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0])
Accessing and managing GPUs with DeviceContext​
The gpu.host package includes the DeviceContext struct, which represents a logical instance of a GPU device. It provides methods for allocating memory on the device, copying data between the host CPU and the GPU, and compiling and running functions (also known as kernels) on the device.
Creating an instance of DeviceContext to access a GPU​
Mojo supports systems with multiple GPUs. GPUs are uniquely identified by integer indices starting with 0, which is considered the "default" device. You can determine the number of GPUs available by invoking the DeviceContext.number_of_devices() static method.
The DeviceContext() constructor returns an instance for interacting with a specified GPU. It accepts two optional arguments:
* device_id: An integer index of a specific GPU on the system. The default value of 0 refers to the "default" GPU for the system.
* api: A String specifying a particular vendor's API. "cuda" (NVIDIA), "hip" (AMD), and "metal" (Apple) are currently supported.
If your system doesn't have a supported GPU-or doesn't have a GPU matching the device_id or api, if provided-then the constructor raises an error.
Asynchronous operation and synchronizing the CPU and GPU​
Typical CPU-GPU interaction is asynchronous, allowing the GPU to process tasks while the CPU is busy with other work. Each DeviceContext has an associated stream of queued operations to execute on the GPU. Operations within a stream execute in the order they are enqueued.
The synchronize() method blocks execution of the current CPU thread until all queued operations on the associated DeviceContext stream have completed. Most commonly, you use this to wait until the result of a kernel function is copied from device memory to host memory before accessing it on the host.
Kernel functions​
A GPU kernel is simply a function that runs on a GPU, executing a specific computation on a large dataset in parallel across thousands or millions of threads. You specify the number of threads when you execute a kernel function, and all threads run the same kernel function. However, the GPU assigns a unique thread index for each thread, and you use the thread index to determine which data elements an individual thread should process.
Multidimensional grids and thread organization​
As discussed in GPU execution model, a grid is the top-level organizational structure of the threads executing a kernel function on a GPU. A grid consists of multiple thread blocks, which are organized across one, two, or three dimensions. Each thread block is further divided into individual threads, which are in turn organized across one, two, or three dimensions.
You specify the grid and thread block dimensions with the grid_dim and block_dim keyword arguments when you enqueue a kernel function to execute using the enqueue_function_checked() method. For example:
# Enqueue the print_threads() kernel function
ctx.enqueue_function_checked[print_threads, print_threads](
    grid_dim=(2, 2, 1),   # 2x2x1 blocks per grid
    block_dim=(4, 4, 2),  # 4x4x2 threads per block
)
For both grid_dim and block_dim, you express the size in the x, y, and z dimensions as a Dim or a Tuple. The y and z dimensions default to 1 if you don't explicitly provide them (that is, (2, 2) is treated as (2, 2, 1) and (8,) is treated as (8, 1, 1)). You can also provide just an Int value to specify only the x dimension (that is, 64 is treated as (64, 1, 1)).

Figure 1. Organization of thread blocks and threads within a grid. 
From within a kernel function, you can access the grid and thread block dimensions and the assigned thread block and thread indices of the individual threads executing the kernel using the following structures:
Struct alias
Description
grid_dim
Dimensions of the grid as x, y, and z values (for example, grid_dim.y).
block_dim
Dimensions of the thread block as x, y, and z values.
block_idx
Index of the block within the grid as x, y, and z values.
thread_idx
Index of the thread within the block as x, y, and z values.
global_idx
The global offset of the thread as x, y, and z values. That is, global_idx.x = block_dim.x * block_idx.x + thread_idx.x, global_idx.y = block_dim.y * block_idx.y + thread_idx.y, and global_idx.z = block_dim.z * block_idx.z + thread_idx.z.
All of these dimensions and indices are UInt values.
Here is a complete example showing a kernel function that simply prints the thread block index, thread index, and global index for each thread executed.
print_threads.mojo
from sys import exit, has_accelerator, has_apple_gpu_accelerator

from gpu.host import DeviceContext
from gpu import block_dim, block_idx, global_idx, grid_dim, thread_idx

fn print_threads():
    """Print thread block and thread indices."""

    print(
        "block_idx: [",
            block_idx.x,
            block_idx.y,
            block_idx.z,
        "]\tthread_idx: [",
            thread_idx.x,
            thread_idx.y,
            thread_idx.z,
        "]\tglobal_idx: [",
            global_idx.x,
            global_idx.y,
            global_idx.z,
        "]\tcalculated global_idx: [",
            block_dim.x * block_idx.x + thread_idx.x,
            block_dim.y * block_idx.y + thread_idx.y,
            block_dim.z * block_idx.z + thread_idx.z,
        "]",
    )

def main():
    @parameter
    if not has_accelerator():
        print("No compatible GPU found")
    elif has_apple_gpu_accelerator():
        print(
            "Printing from a kernel is not currently supported on Apple silicon"
            " GPUs"
        )
    else:
        # Initialize GPU context for device 0 (default GPU device).
        ctx = DeviceContext()

        ctx.enqueue_function_checked[print_threads, print_threads](
            grid_dim=(2, 2, 1),  # 2x2x1 blocks per grid
            block_dim=(4, 4, 2),  # 4x4x2 threads per block
        )

        ctx.synchronize()
        print("Done")
This application produces output similar to this (with the output order indeterminate because of the concurrent execution of multiple threads):
block_idx: [ 0 1 0 ]	thread_idx: [ 0 0 0 ]	global_idx: [ 0 4 0 ]	calculated global_idx: [ 0 4 0 ]
block_idx: [ 0 1 0 ]	thread_idx: [ 1 0 0 ]	global_idx: [ 1 4 0 ]	calculated global_idx: [ 1 4 0 ]
...
block_idx: [ 1 1 0 ]	thread_idx: [ 2 3 1 ]	global_idx: [ 6 7 1 ]	calculated global_idx: [ 6 7 1 ]
block_idx: [ 1 1 0 ]	thread_idx: [ 3 3 1 ]	global_idx: [ 7 7 1 ]	calculated global_idx: [ 7 7 1 ]
Done
Printing from within a kernel function is not currently supported on Apple silicon GPUs.
Writing a kernel function​
Kernel functions must be non-raising. This means that you must define them using the fn keyword and not use the raises keyword. (The Mojo compiler always treats a function declared with def as a raising function, even if the body of the function doesn't contain any code that could raise an error.)
Argument values must be of types that conform to the DevicePassable trait. Additionally, a kernel function can't have a return value. Instead, you must write any result of a kernel function to a memory buffer passed in as an argument. The next two sections, Passing data between CPU and GPU and DeviceBuffer and HostBuffer go into more detail on how to pass values to a kernel function and get back results.
As discussed in GPU execution model, when the GPU executes a kernel, it assigns the grid's thread blocks to various streaming multiprocessors (SMs) for execution. The SM then divides the thread block into subsets of threads called a warp. The size of a warp depends on the GPU architecture, but most modern GPUs currently use a warp size of 32 or 64 threads.

Figure 2. Hierarchy of threads running on a GPU, showing the relationship of the grid, thread blocks, warps, and individual threads, based on HIP Programming Guide (c)2023-2025 Advanced Micro Devices, Inc. 
If a thread block contains a number of threads not evenly divisible by the warp size, the SM creates a partially filled final warp that still consumes the full warp's resources. For example, if a thread block has 100 threads and the warp size is 32, the SM creates:
* 3 full warps of 32 threads each (96 threads total).
* 1 partial warp with only 4 active threads but still occupying a full warp's worth of resources (32 thread slots).
Because of this execution model, you must ensure that the threads in your kernel don't attempt to access out-of-bounds data. Otherwise, your kernel might crash or produce incorrect results. For example, if you pass a 2,000-element vector to a kernel that you execute with single-dimension thread blocks of 512 threads each, and each thread is responsible for processing one element, your kernel could perform a boundary check like this to ensure that it doesn't attempt to process out-of-bounds elements:
from gpu import global_idx

fn process_vector(vector: UnsafePointer[Float32, MutAnyOrigin], size: Int):
    if global_idx.x < size:
        # Process vector[global_idx.x] in some way
Passing data between CPU and GPU​
All values passed to a kernel function must be of types that conform to the DevicePassable trait. The trait declares an associated alias named device_type that maps the type as used on the CPU host to a corresponding type used on the GPU device.
As an example, DeviceBuffer is a host-side representation of a buffer located in the GPU's global memory space. But it defines its device_type associated alias as UnsafePointer, so the data represented by a DeviceBuffer is actually passed to the kernel function as a value of type UnsafePointer. The next section, DeviceBuffer and HostBuffer , describes in more detail how to allocate memory buffers on the host and device and to exchange blocks of data between host and device.
The following table lists the most commonly used types in the Mojo libraries that conform to the DevicePassable trait.
Host type
Device Type
Description
Int
Int
Signed integer
SIMD[dtype, width]
SIMD[dtype, width]
Small vector backed by a hardware vector element
DeviceBuffer[dtype]
UnsafePointer[SIMD[dtype, 1]]
Memory buffer of dtype values
LayoutTensor
LayoutTensor
A powerful abstraction for multi-dimensional data. See Using LayoutTensor for more information.
DeviceBuffer and HostBuffer​
This section describes how to use DeviceBuffer and HostBuffer to allocate memory on the device and host respectively, and to copy data between device and host memory.
Creating a DeviceBuffer​
The DeviceBuffer type represents a block of device memory associated with a particular DeviceContext. Specifically, the buffer is located in the device's global memory space. As such, the buffer is accessible by all threads of all kernel functions executed by the DeviceContext.
As discussed in Passing data between CPU and GPU, DeviceBuffer is the type used by the host to allocate the buffer and to copy data between the host and device. But when you pass a DeviceBuffer to a kernel function, the argument received by the function is of type UnsafePointer. Attempting to use the DeviceBuffer type directly from within a kernel function results in an error.
The DeviceContext.enqueue_create_buffer() method creates a DeviceBuffer associated with that DeviceContext. It accepts the data type as a compile-time DType parameter and the size of the buffer as a run-time argument. So to create a buffer for 1,024 Float32 values, you would execute:
device_buffer = ctx.enqueue_create_buffer[Float32](1024)
As the method name implies, this method is asynchronous and enqueues the operation on the DeviceContext's associated stream of queued operations.
Creating a HostBuffer​
The HostBuffer type is analogous to DeviceBuffer, but represents a block of host memory associated with a particular DeviceContext. It supports methods for transferring data between host and device memory, as well as a basic set of methods for accessing data elements by index and for printing the buffer.
The DeviceContext.enqueue_create_host_buffer() method accepts the data type as a compile-time DType parameter and the size of the buffer as a run-time argument and returns a HostBuffer. As with all DeviceContext methods whose name starts with enqueue_, the method is asynchronous and returns immediately, adding the operation to the queue to be executed by the DeviceContext. Therefore, you need to call the synchronize() method to ensure that the operation has completed before you write to or read from the HostBuffer object.
device_buffer = ctx.enqueue_create_host_buffer[Float32](1024)

# Synchronize to wait until buffer is created before attempting to write to it
ctx.synchronize()

# Now it's safe to write to the buffer
for i in range(1024):
    device_buffer[i] = Float32(i * i)
Copying data between host and device memory​
The enqueue_copy() method is overloaded to support copying from host to device, device to host, or even device to device for systems that have multiple GPUs. Typically, you'll use it to copy data that you've staged in a HostBuffer to a DeviceBuffer before executing a kernel, and then from a DeviceBuffer to a HostBuffer to retrieve the results of kernel execution. The scalar_add.mojo example in GPU programming model shows this pattern in action. In it, the kernel function does an in-place modification of the buffer it receives as an argument and then reuses the original HostBuffer to copy the results back from the device. However, you can allocate a separate DeviceBuffer and HostBuffer for the result of a kernel function if you want to retain the original data.
In addition to copying data between a HostBuffer to a DeviceBuffer, you can use an UnsafePointer as the source or destination of a copy. However, the UnsafePointer must reference host memory for this operation. Attempting to use an UnsafePointer referencing device memory results in an error. For example, this is useful if you have data already staged in a data structure on the host that can expose the data through an UnsafePointer. In that case you would not need to copy the data from the data structure to a HostBuffer before copying it to the DeviceBuffer.
Both DeviceBuffer and HostBuffer also include enqueue_copy_to() and enqueue_copy_from() methods. These are simply convenience methods that call the enqueue_copy() method on their corresponding DeviceContext. For example, the following two method calls are interchangeable:
ctx.enqueue_copy(src_buf=host_buffer, dst_buf=device_buffer)

# Equivalent to:
host_buffer.enqueue_copy_to(dst=device_buffer)
Finally, as a convenience for testing or prototyping, you can use the DeviceBuffer.map_to_host() method to create a host-accessible view of the device buffer's contents. This returns HostBuffer as a context manager that contains a copy of the data from the corresponding DeviceBuffer. Additionally, any modifications that you make to the HostBuffer are automatically copied back to the DeviceBuffer when the with statement exits. For example:
ctx = DeviceContext()
length = 1024

input_device = ctx.enqueue_create_buffer[DType.float32](length)

# Initialize the input

with input_device.map_to_host() as input_host:
    for i in range(length):
        input_host[i] = Float32(i)
However, you should not use this in most production code because of the bidirectional copies and synchronization. The example above is equivalent to:
ctx = DeviceContext()
length = 1024

input_device = ctx.enqueue_create_buffer[DType.float32](length)
input_host = ctx.enqueue_create_host_buffer[DType.float32](length)

input_device.enqueue_copy_to(input_host)
ctx.synchronize()

for i in range(length):
    input_host[i] = Float32(i)

input_host.enqueue_copy_to(input_device)
ctx.synchronize()
Deallocating memory buffers​
Both DeviceBuffer and HostBuffer are subject to Mojo's standard ownership and lifecycle mechanisms. The Mojo compiler analyzes our program to determine the last point that the owner of or a reference to an object is used and automatically adds a call to the object's destructor. This means that you don't explicitly call any method to free the memory represented by a DeviceBuffer or HostBuffer instance. See the Ownership and Intro to value lifecycle sections of the Mojo Manual for more information on Mojo value ownership and value lifecycle management, and the Death of a value section for a detailed explanation of object destruction.
Compiling and enqueuing a kernel function for execution​
The compile_function_checked() method accepts a kernel function as a compile-time parameter and then compiles it for the associated DeviceContext. Then you can enqueue the compiled kernel for execution by passing it to the enqueue_function_checked() method. The example in the GPU programming model demonstrated this pattern:
scalar_add_checked.mojo
...
scalar_add_kernel = ctx.compile_function_checked[scalar_add, scalar_add]()

ctx.enqueue_function_checked(
    scalar_add_kernel,
    device_buffer,
    num_elements,
    Float32(20.0),
    grid_dim=1,
    block_dim=num_elements,
)
When using a compiled kernel function like this, you execute it by calling enqueue_function_checked() with the following arguments in this order:
* The kernel function to execute.
* Any additional arguments specified by the kernel function definition in the order specified by the function.
* The grid dimensions using the grid_dim keyword argument.
* The thread block dimensions using the block_dim keyword argument.
Refer to the Multidimensional grids and thread organization section for more information on grid and thread block dimensions.
The current implementations of compile_function() and enqueue_function() don't typecheck the arguments to the compiled kernel function, which can lead to obscure run-time errors if the argument ordering, types, or count doesn't match the kernel function's definition. Additionally, these methods currently have known issues with Apple silicon GPUs.
For compile-time typechecking, we recommend that you use the compile_function_checked() and enqueue_function_checked() methods, which also work correctly on Apple silicon GPUs.
Note that compile_function_checked() currently requires the kernel function to be provided twice as parameters. This requirement will be removed in a future API update, when typechecking will become the default behavior for both compile_function() and enqueue_function().
The advantage of compiling the kernel as a separate step is that that you can execute the same compiled kernel on the same device multiple times. This avoids the overhead of compiling the kernel each time it's executed.
If your application needs to execute a kernel function only once, you can use an overloaded version of enqueue_function_checked() that compiles the kernel and enqueues it in a single step. Therefore, the following is equivalent to the separate calls to compile_function_checked() and enqueue_function_checked() shown above (note that the kernel function is provided as a compile-time parameter in this case):
ctx.enqueue_function_checked[scalar_add, scalar_add](
    device_buffer,
    num_elements,
    Float32(20.0),
    grid_dim=1,
    block_dim=num_elements,
)
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit



GPU block and warp operations and synchronization
When multiple GPU threads write to the same memory location without a defined order of execution, a race condition occurs. The final outcome of the computation becomes non-deterministic, depending on the scheduling and timing of execution of threads by the GPU hardware. Such bugs are notoriously difficult to debug because they may not appear consistently in every run.
To write correct and robust parallel programs, you need explicit mechanisms to coordinate the execution of threads and manage the visibility of their memory operations. These mechanisms are known as synchronization primitives. They are not merely performance optimizations; they are essential tools for correctness. Without them, threads operate in complete isolation, unable to safely share intermediate results, divide complex tasks, or perform the collective computations that are the hallmark of high-performance GPU algorithms.
A barrier is a fundamental synchronization primitive that creates a meeting point in the program where all participating threads must wait for each other. When a thread reaches a barrier, it pauses execution until every other thread in the group also arrives. This ensures that all threads proceed together past the barrier, maintaining consistent state and preventing race conditions when accessing shared data.
Mojo provides two complementary categories of GPU coordination tools. Synchronization primitives like barrier() and syncwarp() coordinate thread execution and enforce memory visibility, but they don't perform computation themselves-they're pure coordination mechanisms. In contrast, collective operations like reductions, broadcasts, and prefix sums combine synchronization with common computational patterns: they coordinate threads and compute a result. Use synchronization primitives when you need explicit control over when threads coordinate (such as managing access to shared memory between distinct phases of an algorithm), and use collective operations when you need to aggregate or distribute data across threads (such as computing a sum or maximum across a thread block or warp). Both types of tools are essential for writing correct and efficient GPU code, and understanding when to use each is key to building robust parallel algorithms.
This guide covers Mojo's low-level primitives for managing coordination at the thread block and warp levels. For foundational GPU architecture concepts and detailed explanations of the GPU execution model, see Intro to GPUs. For a discussion of basic kernel creation and device management, see GPU programming fundamentals.
We'll explore Mojo's synchronization and collective communication primitives for coordinating parallel work on the GPU. Key topics include:
* Block-level synchronization and operations: How to coordinate all threads within a thread block using barrier() and block reduction operations from the gpu.primitives.block module.
* Warp-level operations: How to perform fine-grained synchronization with syncwarp() and leverage high-speed data exchange using gpu.primitives.warp primitives.
* Best practices and common pitfalls: How to use these primitives correctly to write reliable and portable GPU code.
Block-level synchronization and operations​
This section covers coordination mechanisms for all threads within a thread block:
* The barrier() primitive: The fundamental synchronization primitive that ensures all threads reach the same point before proceeding.
* Block-level reduction operations: Higher-level collective operations (sum, max, min, broadcast, prefix_sum) that combine synchronization with computation.
* Block synchronization example: A complete tiled matrix multiplication demonstrating practical barrier usage.
These tools serve different but complementary purposes: barrier() is a pure synchronization primitive for coordinating execution and memory visibility, while block reduction operations are collective computations that internally handle their own synchronization. You can use barrier() to build custom coordination patterns, or use block reductions when you need both coordination and computation together.
The barrier() primitive​
The gpu.sync.barrier() function is the primary mechanism for coordinating all threads within a single thread block. It creates a synchronization point in the kernel's execution flow that no thread can pass until every other thread in its block has also reached that point.
The barrier() primitive does two things: it acts as both an execution barrier and a memory fence.
* Execution barrier: As an execution barrier, barrier() ensures that the execution of all threads in a block is paused at that point in the program. The hardware scheduler will not allow any thread to proceed past the barrier until all threads in that block have signaled their arrival.
* Memory fence: As a memory fence, barrier() enforces a strict ordering on memory operations. It guarantees that all writes to shared memory (and global memory, with respect to other threads in the same block) performed by any thread before the barrier are completed and made visible to all other threads in the block after they pass the barrier. This guarantee is what prevents race conditions when threads communicate via shared memory.
The most common use case for barrier() is managing access to the fast, on-chip shared memory shared by all threads within a block. Here's how a typical algorithm works:
1. Threads in a block cooperatively load a segment of data from the high-latency global memory into a shared memory array. Each thread is responsible for loading one or more elements.
2. A call to barrier() is made. This is essential to ensure that the entire data segment is fully loaded into shared memory before any thread attempts to use it.
3. Threads perform computations, reading from and writing to the shared memory array. This phase leverages the low latency of shared memory to accelerate the algorithm.
4. If the computation itself involves multiple stages of shared memory communication, another barrier() call may be necessary to ensure the results of one stage are visible before the next begins.
5. Finally, threads write their results from shared memory back to global memory.
Caution
A barrier() must be encountered by all threads within a block to avoid a deadlock. Placing a barrier() inside a conditional statement (such as an if or else block) is a common source of bugs. If the condition causes some threads to execute the barrier() while others skip it, the threads that reach the barrier will wait indefinitely for the other threads to arrive, causing the kernel to hang. Therefore, barrier() should be used in conditional code only if it is guaranteed that all threads in the block will evaluate the condition identically and follow the same execution path.
The Mojo barrier() function is functionally equivalent to the __syncthreads() intrinsic in both NVIDIA CUDA and AMD HIP and threadgroup_barrier(mem_flags::mem_threadgroup) in Apple Metal, providing a portable syntax for this fundamental operation.
For fine-grained synchronization within a single warp, see syncwarp(), which provides faster coordination for threads executing together in the same warp without requiring block-wide synchronization.
Block-level reduction operations​
In addition to the basic barrier() primitive, Mojo provides higher-level block-wide collective operations through the gpu.primitives.block module. These operations combine the coordination functionality of barrier() with common computational patterns, offering both convenience and performance benefits.
The gpu.primitives.block module includes several reduction primitives:
* sum(val): Computes the sum of val across all threads in the block.
* max(val): Computes the maximum val across all threads in the block.
* min(val): Computes the minimum val across all threads in the block.
* broadcast(val, src_thread=0): Broadcasts the value from src_thread to all other threads in the block.
* prefix_sum[exclusive=False](val): Computes an inclusive (default) or exclusive prefix sum (scan) across threads in the block. A prefix sum transforms an input sequence into cumulative sums: given [x0,x1,x2,x3][x_0, x_1, x_2, x_3][x0​,x1​,x2​,x3​], an inclusive scan produces [x0,x0+x1,x0+x1+x2,x0+x1+x2+x3][x_0, x_0+x_1, x_0+x_1+x_2, x_0+x_1+x_2+x_3][x0​,x0​+x1​,x0​+x1​+x2​,x0​+x1​+x2​+x3​] where each thread receives the sum of all values up to and including its own, while an exclusive scan produces [0,x0,x0+x1,x0+x1+x2][0, x_0, x_0+x_1, x_0+x_1+x_2][0,x0​,x0​+x1​,x0​+x1​+x2​] where each thread receives the sum of all values before it.
These operations automatically handle the necessary synchronization and shared memory management internally, making them both easier to use correctly and often more efficient than manually implementing equivalent functionality with barrier() and shared memory operations.
Tip
Use gpu.primitives.block operations when you need to aggregate data across all threads in a thread block (which may span multiple warps). Use gpu.primitives.warp operations, as described in Warp-level reduction operations, when you need to aggregate only within a single warp, as they are significantly faster. For algorithms that reduce large datasets, use a hybrid approach: first reduce within warps using gpu.primitives.warp primitives, then combine warp results using gpu.primitives.block operations.
Using block operations in practice​
Block-level operations are commonly used in multi-stage algorithms where threads must coordinate through shared memory. A typical pattern involves:
1. Load phase: Threads cooperatively load data into shared memory
2. Synchronize: Use barrier() to ensure all data is loaded
3. Compute phase: Process data using shared memory
4. Reduce phase: Use block reduction operations to aggregate results
This pattern appears in algorithms like tiled matrix multiplication, stencil operations, and parallel reductions, where the combination of shared memory and proper synchronization enables significant performance improvements over naive approaches.
Block synchronization example: tiled matrix multiplication​
Matrix multiplication benefits from a technique called tiling, where we break large matrices into smaller tiles that fit in the GPU's fast shared memory. Instead of repeatedly reading from slow global memory, threads in a block cooperatively load a tile into shared memory once, then all threads can access it multiple times. This creates a classic producer-consumer pattern: threads work together to load data (producer phase), then all threads use that data to compute results (consumer phase). Without proper synchronization between these phases, the algorithm produces incorrect results. For a deeper understanding of the tiling strategy, see this section of our blog post on optimizing matrix multiplication on NVIDIA's Blackwell.
tiled_matmul.mojo
from math import ceildiv
from sys import exit, has_accelerator

# GPU programming imports from open source stdlib
from gpu.sync import barrier
from gpu.host import DeviceContext
from gpu import thread_idx, block_idx
from gpu.memory import AddressSpace

# Layout tensor support from open source layout package
from layout import Layout, LayoutTensor

# Data type selection: float32 provides good balance of precision and performance
alias float_dtype = DType.float32

# Matrix dimensions: chosen to be small enough for easy understanding
# while still demonstrating tiling concepts effectively
alias MATRIX_SIZE = 64  # 64x64 matrices
alias MATRIX_M = MATRIX_SIZE  # Number of rows in matrices A and C
alias MATRIX_N = MATRIX_SIZE  # Number of columns in matrices B and C
alias MATRIX_K = MATRIX_SIZE  # Shared dimension (A cols = B rows)

# Tile dimensions: chosen to fit comfortably in GPU shared memory
# and demonstrate clear blocking behavior
alias TILE_SIZE = 16  # 16x16 tiles balance memory usage and parallelism
alias TILE_M = TILE_SIZE  # Tile height for matrix A and C
alias TILE_N = TILE_SIZE  # Tile width for matrix B and C
alias TILE_K = TILE_SIZE  # Tile depth for the K dimension

# Derived constants
alias NUM_TILES_PER_SIDE = MATRIX_SIZE // TILE_SIZE  # Number of tiles per matrix side (4)
alias THREADS_PER_TILE = TILE_SIZE * TILE_SIZE  # Threads needed per tile (256)
alias TOTAL_TILES_TO_PROCESS = NUM_TILES_PER_SIDE  # Tiles to process in K dimension

# LayoutTensor provides type-safe multi-dimensional data access with automatic memory layout handling
# Layout definitions using example matrix dimensions
alias matrix_a_layout = Layout.row_major(MATRIX_M, MATRIX_K)  # A: M x K
alias matrix_b_layout = Layout.row_major(MATRIX_K, MATRIX_N)  # B: K x N
alias matrix_c_layout = Layout.row_major(MATRIX_M, MATRIX_N)  # C: M x N

# Layout definitions for tile access
alias tile_a_layout = Layout.row_major(TILE_M, TILE_K)
alias tile_b_layout = Layout.row_major(TILE_K, TILE_N)


fn tiled_matmul_kernel(
    matrix_a: LayoutTensor[float_dtype, matrix_a_layout, MutAnyOrigin],
    matrix_b: LayoutTensor[float_dtype, matrix_b_layout, MutAnyOrigin],
    matrix_c: LayoutTensor[float_dtype, matrix_c_layout, MutAnyOrigin],
):
    # Thread and block indices
    var thread_x = thread_idx.x
    var thread_y = thread_idx.y
    var block_x = block_idx.x
    var block_y = block_idx.y

    # Global matrix coordinates
    var global_row = block_y * TILE_M + thread_y
    var global_col = block_x * TILE_N + thread_x

    # Tile starting positions
    var tile_row_start = block_y * TILE_M
    var tile_col_start = block_x * TILE_N

    # Allocate shared memory tiles for fast on-chip access
    var tile_a_shared = LayoutTensor[
        float_dtype,
        tile_a_layout,
        MutAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var tile_b_shared = LayoutTensor[
        float_dtype,
        tile_b_layout,
        MutAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    # Initialize accumulator and start tiling loop
    var accumulator: matrix_c.element_type = 0.0

    # Iterate through tiles along K dimension
    # Use @parameter to unroll the loop at compile time
    @parameter
    for k_tile in range(0, MATRIX_K, TILE_K):
        # Cooperative tile loading
        # Calculate global coordinates for tile loading
        var a_global_row = tile_row_start + thread_y
        var a_global_col = UInt(k_tile) + thread_x
        var b_global_row = UInt(k_tile) + thread_y
        var b_global_col = tile_col_start + thread_x

        # Bounds checking
        var load_a_valid = (a_global_row < MATRIX_M) and (
            a_global_col < MATRIX_K
        )
        var load_b_valid = (b_global_row < MATRIX_K) and (
            b_global_col < MATRIX_N
        )

        # Load tiles into shared memory with bounds checking
        if load_a_valid:
            tile_a_shared[thread_y, thread_x] = matrix_a[
                a_global_row, a_global_col
            ]
        else:
            tile_a_shared[thread_y, thread_x] = 0.0

        if load_b_valid:
            tile_b_shared[thread_y, thread_x] = matrix_b[
                b_global_row, b_global_col
            ]
        else:
            tile_b_shared[thread_y, thread_x] = 0.0

        # Ensure all threads finish loading tiles before any thread starts computing
        barrier()

        # Compute dot product using shared memory tiles
        @parameter
        for k in range(TILE_K):
            var a_element = tile_a_shared[thread_y, k]
            var b_element = tile_b_shared[k, thread_x]
            accumulator += a_element * b_element

        # Ensure all threads finish computing before any thread loads next tiles
        barrier()

    # Write final result to global memory with bounds checking
    if (global_row < MATRIX_M) and (global_col < MATRIX_N):
        matrix_c[global_row, global_col] = accumulator
This tiled algorithm leverages the GPU's memory hierarchy for better performance. Shared memory is an on-chip cache that's much faster than global memory, but it's limited in size-a typical block might have only 48KB available. We break the computation into stages: threads cooperatively load small tiles from global memory into this fast shared memory, perform computations on those tiles, then repeat for the next set of tiles. Each thread loads one element per tile, creating coalesced memory accesses that maximize bandwidth. Once a tile sits in shared memory, all threads in the block can access it repeatedly without triggering expensive global memory reads.
The first barrier() call appears immediately after the cooperative tile loading phase. This synchronization point is critical: it ensures that all threads in the block finish writing their elements to shared memory before any thread begins reading from it to compute results. Without this barrier, you'd have a classic read-before-write race condition. Fast threads could race ahead and start reading from shared memory locations that slow threads haven't populated yet, leading to incorrect results from uninitialized data. Even worse, the bug would be non-deterministic-sometimes the code would work (if threads happened to execute in a favorable order), and sometimes it would fail, making debugging extremely difficult. The barrier eliminates this unpredictability by establishing a clear happens-before relationship: all writes complete before any reads begin.
The second barrier() call appears at the end of the computation phase, right before the loop continues to load the next set of tiles. This barrier solves the opposite problem: it prevents write-during-read races. Without it, fast threads could finish their computations and start loading new tile data into shared memory while slow threads are still reading the old data for their calculations. This would corrupt the shared memory with partially overwritten values, again producing incorrect results. The pattern is symmetric: the first barrier protects readers from seeing incomplete writes, while the second protects readers from concurrent overwrites. Together, these two barriers implement a safe producer-consumer cycle: load → barrier → compute → barrier → repeat. Both barriers are absolutely essential-removing either one breaks the algorithm's correctness.
Warp-level operations​
While the thread block is the scope for shared memory communication, the warp is the fundamental unit of execution scheduling. Because threads within a warp are executed simultaneously by the hardware, communication between them happens much faster than communication that requires coordination across different warps. Mojo provides a suite of primitives for these high-speed, intra-warp operations, which are essential for many performance-critical optimization patterns.
This section covers high-speed coordination and data exchange within a single warp:
* Warp-level synchronization: How to use syncwarp() for fine-grained synchronization within a warp, and when it's needed vs. when it's not.
* Warp-level data exchange: Register-to-register communication with shuffle operations (shuffle_up, shuffle_down, shuffle_xor, shuffle_idx, broadcast).
* Warp-level reduction operations: High-performance collective operations (sum, max, min, prefix_sum) that operate only within a warp.
Warp-level synchronization​
The gpu.sync.syncwarp() function provides a more granular synchronization barrier that operates only on the threads within a single warp.
This function handles thread divergence. On some GPU architectures, threads within a warp can follow different execution paths due to conditional branching. syncwarp() forces the specified threads in the warp to reconverge at a single point before proceeding.
For coordinating threads across multiple warps within a thread block, use barrier() instead, which synchronizes all threads in the block and provides memory fence guarantees for shared memory access.
The syncwarp() function takes an optional mask argument. This is a 32-bit or 64-bit integer (depending on the warp size of the architecture) that acts as a bitmask. The ith bit of the mask corresponds to the thread at lane i within the warp. If a bit is set to 1, the corresponding thread participates in the synchronization; if it is 0, it does not. The default value of -1 (all bits set to 1) synchronizes all threads in the warp.
Understanding syncwarp() requires knowing its platform-dependent behavior, which Mojo's portable API abstracts away:
* On NVIDIA GPUs supporting independent thread scheduling (Volta architecture and newer), threads within a warp can genuinely diverge. In this context, syncwarp() compiles to an active hardware instruction (bar.warp.sync) that forces the participating threads to wait for each other. It is necessary for correctness in algorithms that rely on warp-synchronous behavior.
* On AMD GPUs, threads within a wavefront (the AMD equivalent of a warp) are guaranteed by the hardware to execute in lock-step. They cannot diverge in the same way. Consequently, syncwarp() is a no-op on AMD architectures; the Mojo compiler emits no instruction for it.
* On Apple silicon GPUs, this provides only execution synchronization within a SIMD group (the Apple equivalent of a warp), with no memory fence (the Apple Metal equivalent of simdgroup_barrier(mem_flags::mem_none)). Lane masks are not supported, so the mask argument is ignored and all active lanes must reach this point.
This difference highlights a key benefit of Mojo. You write code against a single, portable API. The compiler is responsible for generating the correct, architecture-specific code. Therefore, if an algorithm relies on syncwarp() for correctness on NVIDIA hardware, it still behaves as expected on other vendors' hardware.
Warp shuffle operations (like shuffle_down(), shuffle_xor(), etc.) and warp reduction operations (like max(), prefix_sum(), sum(), etc.) provide implicit synchronization and do not require syncwarp() calls before them. Calling syncwarp() before a warp shuffle or reduction operation is redundant and unnecessary.
Warp-level data exchange​
Shuffle operations are the cornerstone of high-performance warp-level algorithms. These primitives enable threads within a warp to exchange data directly through registers, making them essential for implementing efficient parallel patterns like reductions, stencil computations, and sliding window operations.
Unlike shared memory communication that requires explicit synchronization and memory transactions, shuffle operations use the warp's simultaneous execution to achieve near-zero latency data exchange. This makes them ideal for:
* Neighbor data access: Access elements from adjacent threads in stencil operations or convolutions.
* Tree-structured reductions: Implement butterfly patterns for parallel reductions and prefix operations.
* Data broadcasting: Distribute computed values or constants across all threads in a warp.
* Sliding window algorithms: Efficiently compute running maximums, minimums, or moving averages.
Warp shuffle operations​
The gpu.primitives.warp module provides five shuffle primitives, each optimized for specific data movement patterns:
* shuffle_up(value, delta): Each thread receives the value from a thread with a lower lane ID (that is, from lane current_lane - delta). If the resulting lane ID is less than 0, the thread receives an undefined value.
* shuffle_down(value, delta): Each thread receives a value from a thread with a higher lane ID (that is, from lane current_lane + delta). If the resulting lane ID is greater than or equal to the warp size, the thread receives an undefined value.
* shuffle_xor(value, offset): Each thread exchanges its value with the thread at lane current_lane XOR offset. This is particularly useful for implementing butterfly patterns common in algorithms like FFTs and parallel reductions.
* shuffle_idx(value, src_lane): Each thread receives the value from the thread at the specified src_lane. This is effectively a broadcast from one lane to all others in the warp. Essential for sharing computed results or constants across the entire warp.
* broadcast(value): A convenience wrapper around shuffle_idx() that distributes the value from lane 0 to all other threads in the warp.
All of these primitives other than broadcast() take an optional mask argument that serves a dual purpose:
1. Thread participation: The mask specifies which threads participate in the shuffle operation. It is a 32-bit or 64-bit integer (depending on the warp size) where the ith bit corresponds to lane i. If a bit is set to 1, that thread participates; if 0, it does not.
2. Implicit synchronization: The mask also provides automatic synchronization for all participating threads. All threads whose bits are set in the mask will be synchronized before the shuffle completes, ensuring correct data exchange even after divergent control flow.
The default value of -1 (all bits set to 1) includes all threads in the warp.
Warning
When using a full mask (all bits set) in divergent code, all threads in the warp must eventually reach the shuffle instruction, even if some threads don't actively use the result. If some threads take a path that never reaches the shuffle, those threads will never arrive at the synchronization point, causing the other threads to hang indefinitely waiting for them.
These five primitives form the foundation for complex warp-level algorithms and serve as building blocks for higher-level collective operations.
Choosing the right shuffle primitive​
While each shuffle primitive can technically perform any data exchange pattern, certain operations naturally fit specific use cases. Understanding these patterns helps you write more efficient and readable code.
* If you need to share one thread's data with everyone else, reach for broadcast() when the source is lane 0, or shuffle_idx() for any other lane. Think of distributing a loop bound that one thread computed, or sharing a decision that a "leader" thread made.
* For algorithms that process neighboring data-like stencil operations or convolutions-shuffle_up() and shuffle_down() are your best bet. These let you grab values from adjacent threads without the coordination overhead of shared memory. A sliding window average becomes as simple as adding your neighbors' values to your own.
* When implementing tree-structured algorithms like parallel reductions, shuffle_xor() shines. Its butterfly communication pattern naturally maps to how these algorithms exchange data. Most high-performance reduction implementations use shuffle_xor() because it has excellent instruction scheduling properties.
Here are some specific patterns where each primitive excels.
shuffle_idx() and broadcast() work well for:
* Distributing computed constants or array bounds
* Implementing voting mechanisms across the warp
* Sharing results from a designated "leader" thread
shuffle_up() and shuffle_down() are perfect for:
* Stencil computations that need neighboring grid points
* Finite difference schemes requiring adjacent values
* Any sliding window algorithm (moving averages, local extrema)
shuffle_xor() excels at:
* Parallel reductions using butterfly patterns
* Any computation with power-of-2 communication strides
When optimizing performance, prefer shuffle operations over shared memory for register-sized data, and remember that shuffle_xor() typically has the best instruction scheduling characteristics for reduction patterns.
Warp-level reduction operations​
The gpu.primitives.warp module also provides higher-level functions for performing common reduction computations across all threads in a warp. These functions take advantage of hardware-specific intrinsics where possible, and fall back to shuffle-based reduction on other architectures:
* max(value): Computes the maximum value across all threads in the warp. The result is broadcast to all lanes.
* min(value): Computes the minimum value across all threads in the warp. The result is broadcast to all lanes.
* sum(value): Computes the sum of value across all threads in the warp. The result is broadcast to all lanes.
* prefix_sum[exclusive=False](value): Computes an inclusive (default) or exclusive prefix sum (scan) across threads in the warp. A prefix sum transforms an input sequence into cumulative sums: given [x0,x1,x2,x3][x_0, x_1, x_2, x_3][x0​,x1​,x2​,x3​], an inclusive scan produces [x0,x0+x1,x0+x1+x2,x0+x1+x2+x3][x_0, x_0+x_1, x_0+x_1+x_2, x_0+x_1+x_2+x_3][x0​,x0​+x1​,x0​+x1​+x2​,x0​+x1​+x2​+x3​] where each thread receives the sum of all values up to and including its own, while an exclusive scan produces [0,x0,x0+x1,x0+x1+x2][0, x_0, x_0+x_1, x_0+x_1+x_2][0,x0​,x0​+x1​,x0​+x1​+x2​] where each thread receives the sum of all values before it.
You'll find these primitives most useful when computing aggregates across threads that are already working closely together. Use sum() for computing totals, averages, or accumulating values across small data segments. The max() and min() functions work well for finding extremes in your data or implementing voting mechanisms where threads need consensus. prefix_sum() is particularly valuable for scan operations-computing running totals or building cumulative results as you process data. It's essential for algorithms that need to track "how much have we processed so far?" at each step. These operations are significantly faster than a block-level reduction that uses shared memory and barrier() calls.
Using warp operations in practice​
Warp operations excel in algorithms that require frequent, fine-grained data exchange between nearby threads. Common patterns include:
* Sliding window operations: Use shuffle_up() and shuffle_down() to access neighboring lane values.
* Butterfly reductions: Use shuffle_xor() for efficient tree-like reduction patterns that minimize the number of shuffle steps.
* Broadcasting computed values: Use broadcast() to share a single thread's computation result (like a loop bound or pointer) across the entire warp.
These operations are particularly valuable in algorithms where the overhead of block-level synchronization would be prohibitive, such as in inner loops of compute-intensive kernels or when processing data that naturally aligns with warp boundaries.
Advanced synchronization mechanisms​
Beyond the fundamental barrier() and syncwarp() primitives, Mojo provides additional synchronization mechanisms for specialized use cases and architecture-specific optimizations. These advanced primitives enable fine-grained control over memory ordering, asynchronous operations, and instruction scheduling. However, most of these mechanisms are available only on specific architectures. Consult the Mojo API reference documentation for the latest information on availability.
Mechanisms currently available only on NVIDIA GPUs:
* Semaphores (gpu.sync.semaphore.Semaphore, gpu.sync.semaphore.NamedBarrierSemaphore): Device-wide semaphore implementations for inter-CTA synchronization using shared lock variables. Provides fetch(), wait(), release(), and state() methods for state management methods for coordinating work across thread blocks.
* Named barriers (gpu.sync.named_barrier(), gpu.sync.named_barrier_arrive()): Hardware-accelerated block-level barriers using barrier IDs (0-16) for split-phase synchronization patterns. Useful for TMA operations and high-performance pipeline algorithms.
* Memory barriers: A suite of functions for tracking asynchronous memory operations and coordinating phased synchronization in shared memory. Includes gpu.sync.mbarrier_init(), (gpu.sync.mbarrier_arrive(), gpu.sync.mbarrier_arrive_expect_tx_shared(), gpu.sync.mbarrier_arrive_expect_tx_relaxed(), gpu.sync.mbarrier_test_wait(), and gpu.sync.mbarrier_try_wait_parity_shared()).
* Thread fence (gpu.intrinsics.threadfence()): Memory ordering fence (no execution barrier) that ensures memory operations are visible within a specified scope (block, GPU-wide, or system). Essential for lock-free algorithms and cross-block communication.
* Async bulk copy synchronization (gpu.sync.cp_async_bulk_commit_group(), gpu.sync.cp_async_bulk_wait_group()): Functions for coordinating asynchronous bulk memory transfer groups. Essential for managing pipeline stages with bulk memory operations.
Mechanisms currently available only on AMD GPUs:
* Schedule barriers (gpu.sync.schedule_barrier(), gpu.sync.schedule_group_barrier()): Compiler instruction scheduling controls that allow selective reordering of instruction types across barriers. Enables performance optimizations by controlling which instruction categories can cross the barrier.
* Wait count (gpu.sync.s_waitcnt(), gpu.sync.s_waitcnt_barrier()): Precise synchronization primitives that wait for outstanding memory operations to complete based on counter values (vector memory, export, and LGKM counters). Available on AMD CDNA GPUs only (not available on older AMD architectures).
Best practices and common pitfalls​
Now that we've covered the core synchronization primitives, let's focus on using them correctly to write reliable and portable GPU code. Understanding common pitfalls is essential for avoiding bugs that are difficult to reproduce and debug.
This section provides guidance for writing correct, portable, and efficient GPU code:
* Writing correct synchronized code: Avoiding race conditions, deadlocks, and understanding when to use syncwarp() vs. when shuffle operations handle synchronization automatically.
* Choosing the right synchronization level: When to use warp-level operations vs. block-level synchronization.
* Writing portable GPU code: Using Mojo's abstractions to write code that works across NVIDIA, AMD, and Apple hardware.
Writing correct synchronized code​
Correctness should always be your first priority. The following issues are common sources of bugs in parallel programs.
Understanding and avoiding race conditions​
As a quick reminder, a race condition occurs when multiple threads write to the same memory location without a defined order of execution, leading to a non-deterministic outcome. Here's a simple example where threads attempt to update a shared counter:
# INCORRECT: Race condition
shared_counter[0] += my_value  # Multiple threads modify same location
This leads to "lost updates" because the read-modify-write sequence isn't atomic. To prevent this, you must use synchronization primitives like barrier() to coordinate access or use Atomic operations for simple updates. For example, you could use the Atomic.fetch_add() method to atomically increment the counter:
# CORRECT: Atomic increment
_ = Atomic.fetch_add(shared_counter[0], my_value)
Avoiding deadlocks with barrier()​
A barrier() must be encountered by all threads within a block to avoid a deadlock. Placing a barrier() inside a conditional statement is a frequent source of bugs. If the condition causes some threads to execute the barrier() while others skip it, the threads that reach the barrier will wait indefinitely for the others to arrive, causing the kernel to hang.
Therefore, barrier() should be used in conditional code only if it's guaranteed that all threads in the block will evaluate the condition identically and follow the same execution path.
When to use syncwarp()​
The syncwarp() primitive is needed when coordinating access to shared or global memory after divergent control flow within a warp. However, it is not needed before warp shuffle operations or warp reduction operations, as those operations provide their own implicit synchronization via the mask parameter.
Use syncwarp() when:
* Threads in a warp diverge and then need to synchronize before accessing shared memory
* You need to ensure all threads in a warp have completed their divergent execution paths before proceeding to a shared memory operation
Do not use syncwarp() before:
* Warp shuffle operations (shuffle_down(), shuffle_xor(), etc.) - these synchronize automatically
* Warp reduction operations (warp.sum(), warp.max(), etc.) - these also synchronize automatically
Here's an example where syncwarp() is needed (for shared memory coordination):
if thread_idx.x < 16:
    shared_data[thread_idx.x] = compute_something()
else:
    shared_data[thread_idx.x] = compute_something_else()

# syncwarp() needed here because threads diverged before writing to shared memory
syncwarp()
var result = shared_data[some_index]  # Now safe to read
And here's an example where syncwarp() is not needed (shuffle operations):
if thread_idx.x < 16:
    value = compute_something()
else:
    value = compute_something_else()

# No syncwarp() needed - shuffle_down() synchronizes automatically via its mask
result = warp.shuffle_down(value, 1)
Handling shuffle boundary conditions​
When using shuffle_up() and shuffle_down(), be mindful of edge cases. A thread will receive an undefined value if the source lane is out of bounds (for example, current_lane - delta < 0). When implementing patterns like sliding windows, you must add logic to handle these boundary conditions correctly.
Choosing the right synchronization level​
The core principle for designing efficient GPU algorithms is to coordinate between warps with barrier() and shared memory, and optimize within warps with gpu.primitives.warp primitives.
This hierarchical approach mirrors the GPU's architecture. Intra-warp communication is extremely fast, while cross-warp communication is more expensive.
* Use gpu.primitives.warp primitives for:
o High-frequency operations inside tight loops.
o Data exchange between neighboring threads (stencils, sliding windows).
o Reductions or scans over small, warp-sized chunks of data.
o Anywhere performance is latency-critical.
* Use barrier() and gpu.primitives.block primitives for:
o Coordinating access to shared memory between multiple warps.
o Implementing multi-phase algorithms with distinct load, compute, and store stages.
o Aggregating results from multiple warps within a block.
Writing portable GPU code​
Mojo is designed to write portable GPU code, but it's helpful to understand how.
First, Mojo's GPU operations have automatic fallback mechanisms. For example, a gpu.primitives.warp.max() call will automatically use specialized redux instructions on the newest NVIDIA hardware but will fall back to a shuffle-based implementation that works on any other GPU. You get performance where available and correctness everywhere else.
Second, always avoid hardcoding hardware-specific values. The most common mistake is assuming a warp size of 32. Use the gpu.WARP_SIZE constant to ensure your code works correctly on all vendors' hardware.
Finally, for highly-tuned kernels, you can use @parameter if blocks to write architecture-specific code paths while keeping a single source file.
from sys import is_amd_gpu, is_apple_gpu, is_nvidia_gpu

fn adaptive_algorithm():
    @parameter
    if is_nvidia_gpu():
        nvidia_optimized_path()
    elif is_amd_gpu():
        amd_optimized_path()
    elif is_apple_gpu():
        apple_optimized_path()
    else:
        # Conservative fallback for future hardware support
        portable_path()
Debugging synchronization issues​
Synchronization bugs can be tricky. Here are some strategies to find them:
* Isolate the problem: Use simple, predictable data patterns (like each thread's ID) to verify your logic before using real data. Validate your parallel algorithm's output against a simple, sequential CPU version.
* Trace execution: Add print() statements to trace intermediate values and understand how data flows through your warp-level shuffles or reduction trees.
Printing from within a kernel function is not currently supported on Apple silicon GPUs.
* Expose scheduling-dependent bugs: Test with different thread block sizes. A bug that appears with one configuration but not another often points to a race condition.
* Use dedicated tools: For complex issues, use vendor-provided GPU debugging tools (like the NVIDIA Compute Sanitizer) which can detect race conditions and memory access errors.
Conclusion and key takeaways​
Summary of primitives and patterns​
We've covered Mojo's low-level toolkit for managing concurrency and communication in GPU kernels. These primitives are the fundamental building blocks for writing correct and high-performance parallel algorithms.
* gpu.sync.barrier(): The essential primitive for correctness across warps. It provides a block-wide synchronization point that acts as both an execution barrier and a memory fence, primarily to coordinate access to shared memory.
* gpu.sync.syncwarp(): A fine-grained primitive for managing thread divergence within a single warp. It's necessary for correctness on hardware that supports independent thread scheduling.
* gpu.primitives.block operations: High-level primitives that combine synchronization with common computational patterns (like reductions) across all threads in a block. They simplify code and are often more efficient than manual implementations.
* gpu.primitives.warp primitives: The essential toolkit for performance. By enabling direct register-to-register communication within a warp, these primitives allow for extremely fast collective operations that avoid the higher latency of shared memory.
The core mental model​
The effective use of these primitives stems from a hierarchical approach to GPU algorithm design. Your key takeaway should be to coordinate between warps with barrier() and shared memory, and optimize within warps with gpu.primitives.warp primitives.
This principle encourages you to structure algorithms to maximize intra-warp computation and communication, which is extremely fast, and to use the more costly block-level synchronization only when necessary to combine results or manage data dependencies between warps.
Next steps​
To gain hands-on experience with the concepts in this guide, we encourage you to explore the following resources:
* Mojo GPU Puzzles: An interactive, hands-on guide to mastering GPU programming patterns in Mojo, including parallel reductions and other algorithms that rely on these primitives.
* MAX AI Kernels Library: For higher-level examples, the MAX AI Kernels library contains numerous production-grade kernels that use these low-level primitives to build highly optimized operations for AI and numerical computing.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Introduction to layouts
Mojo's layout package provides a number of APIs for working with dense multidimensional arrays, which simplify writing algorithms for handling linear algebra.
This package includes the following main types:
* The Layout struct describes an arrangement of data in memory. A layout is a function that maps a set of logical coordinates (like (x, y) in a two-dimensional array) to a linear index value. Layouts can be hierarchical (for example, representing a 2D matrix that's further subdivided into tiles).
* LayoutTensor is a flexible tensor type that combines a Layout and a pointer to data.
* The IntTuple struct is a hierarchical tuple type, where each element of the tuple can either be an integral value or a nested IntTuple. The IntTuple type is used extensively for defining and indexing layouts and layout tensors.
Example code
You can find most of the code examples on this page in the public GitHub repo.
Some of the concepts presented here can be a little hard to grasp from static examples, so we recommend downloading the example code and experimenting.
What's a Layout?​
A layout is a function that maps a set of logical coordinates to a single linear index value.
For example, a layout could describe a 2x4 row-major matrix, or a 6x6 column-major matrix.
from layout import Layout, print_layout

var l2x4row_major = Layout.row_major(2, 4)
var l6x6col_major = Layout.col_major(6, 6)
Layouts are made up of two tuples: shape and stride, where shape describes the logical coordinate space and the stride determines the mapping to the linear index value. A layout can be written as (shape:stride). For example, a contiguous vector of length 4 can be represented as (4:1):

Figure 1. 1D layout (4:1) 
A 3x4 row-major layout can be represented as ((3, 4):(4, 1)). That is, the shape is 3x4 and the strides are 4 and 1. You can break this down into two sub-layouts or modes: a row mode and a column mode: 3 rows with a stride of 4 (3:4, the first numbers from each tuple) and 4 columns with a stride of 1 (4:1, the second numbers from each tuple).
The print_layout() function generates an ASCII diagram of any 2D layout, showing the coordinates on the outside and the corresponding index values in the grid.
var l3x4row_major = Layout.row_major(3, 4)
print_layout(l3x4row_major)
Output:
((3, 4):(4, 1))
       0    1    2    3
    +----+----+----+----+
 0  |  0 |  1 |  2 |  3 |
    +----+----+----+----+
 1  |  4 |  5 |  6 |  7 |
    +----+----+----+----+
 2  |  8 |  9 | 10 | 11 |
    +----+----+----+----+
The coordinate to index mapping is performed by calculating the dot product of the logical coordinates and the corresponding strides. For example, given the coordinates (i, j) and the layout shown above, the index value is i∗4+j∗1i*4 + j*1i∗4+j∗1. So coordinate (1, 1) maps to 5, as shown in the diagram.
The following example shows how to use a Layout to convert between coordinates and index values.
var coords = IntTuple(1, 1)
var idx = l3x4row_major(coords)
print("index at coordinates (1, 1): ", idx)
print("coordinates at index 7:", l3x4row_major.idx2crd(7))
Output:
index at coordinates (1, 1):  5
coordinates at index 7: (1, 3)
As this example shows, the layout is a function that takes a set of integer coordinates and returns a single integer (the linear index). The Layout struct also provides an idx2crd() method that transforms a linear index into a set of logical coordinates.
Printing layouts
You can use print_layout() to print a diagram of any 2D layout. You can pass any layout to the built-in print() function to print a string representation of the layout in the form of a (shape:stride) pair.
IntTuple: representing hierarchical shapes and strides​
A layout's shape and stride are represented using the IntTuple type. Each element of an IntTuple is either an integer value or a nested IntTuple. You can create nested IntTuples using the IntTuple constructor:
var shape1 = IntTuple(4, IntTuple(2, 2))
A layout's shape and stride tuples must be congruent-that is, they need to have the same hierarchical structure: the tuples must have the same number of elements, and any elements that are nested tuples must also have the same number of elements.
The int_tuple package provides a number of functions for working with IntTuple. For example, it provides a congruent() function for testing the congruency of two tuples.
Modes​
A layout has one or more modes, where a mode is a shape:stride pair. For example, the 1D vector layout (8:1) has a single mode: 8 elements with a stride of 1:

Figure 2. 1D layout 
The 2D row-major matrix layout ((2, 4):(4, 1)) has two modes, 2:4 (the first numbers from each tuple) and 4:1 (the second numbers from each tuple). Taking them right to left, the second mode describes 4 columns with a stride of one. The first mode specifies that there are two of these groups with a stride of 4:

Figure 3. 2D layout with strides 
In a column-major layout, the row number varies the fastest, so a column-major 2x4 matrix has the layout ((2, 4):(1, 2)) and looks like this:

Figure 4. 2D column-major layout with strides 
A layout's rank is the number of modes in its shape. A rank-1 (or 1D) layout describes a vector. A rank-2 layout describes a 2D matrix, and so on.
A layout's size is defined as the product of all of the modes in the layout's shape. To put it another way, it's the number of elements that the layout addresses: that is, the domain of the layout function.
Modes can also be nested to represent more complicated strides along a dimension. For example, the layout (8:1) represents a 1D vector of 8 elements.

Figure 5. 1D vector layout 
The layout (((4, 2):(1, 4))) is also a 1D vector of 8 elements. The extra set of parentheses indicates a nested or hierarchical mode. Instead of being represented by a single mode like 8:1, this layout's single dimension is represented by the multi-mode (4, 2):(1, 4):

Figure 6. 1D layout with nested modes 
Note that in the nested modes, there's no notion of row and column. You can think of the first mode as the "inner" mode (defining a group) and the next mode as an "outer" mode (defining a repeat of the group) as shown above.
A set of nested modes (a multi-mode) counts as a single mode when considering the parent layout's rank. For example, the layouts (8:1) and (((4, 2):(1, 4))) are both rank-1 layouts.
This gets more interesting when we move to two dimensions. Consider the following 2D layouts:

Figure 7. Two 2D layouts 
Layouts A and B are both 2D matrix layouts with the same overall 2D shape, but with the elements in a different order. Layout B is tiled, so instead of being in row-major or column-major order, four consecutive indices are grouped into each 2x2 tile. This is sometimes called tile-major order.
We can break this tiled layout into two modes, one for the rows and one for the columns:
* Layout B has a row mode of (2, 2):(1, 4). We can further break this into two sub-modes: the inner mode, 2:1, defines a group of two rows with a stride of one. The outer mode, 2:4, specifies that the group occurs twice with a stride of 4.
* The column has the mode (2, 2):(2, 8). Once again we can break this into two sub-modes: (2:2) defines a group of two columns with a stride of two, and the group occurs twice with a stride of 8 (2:8).
If all of those modes are swimming before your eyes, take a moment to study the figure and trace out the strides yourself.
Coordinates​
Coordinates for layouts can be written in the same format as the shape tuple. For example, coordinates for layout B above can be written ((i, j), (k, l)). However, this layout can also be addressed as a logical 2D matrix, just like layout A. So ((0, 1), (0, 1)) and (2, 2) are both valid coordinates that map to the same index.
In fact, this is true for any layout: the layout can be addressed with 1D or 2D coordinates as well as its "natural" coordinates. When mapping coordinates, the dimensions are traversed in colexicographical order (that is, a generalized column-major order, where the leftmost coordinate varies fastest). Table 1 shows how different 1D and 2D coordinates map to the "natural" coordinates of the ((2, 2), (2, 2)) shape shown above:
1D
2D
Natural
0
(0, 0)
((0, 0), (0, 0))
1
(1, 0)
((1, 0), (0, 0))
2
(2, 0)
((0, 1), (0, 0))
3
(3, 0)
((1, 1), (0, 0))
4
(0, 1)
((0, 0), (1, 0))
5
(1, 1)
((1, 0), (1, 0))
6
(2, 1)
((0, 1), (1, 0))
7
(3, 1)
((1, 1), (1, 0))
8
(0, 2)
((0, 0), (0, 1))
...
...
...
15
(3, 3)
((1, 1), (1, 1))
Table 1. Mapping between 1D, 2D, and natural coordinates 
Making layouts​
There are multiple ways to create layouts. The row_major() and col_major() static methods are probably the simplest ways to create a layout. The row_major() method creates a generalized row-major layout: that is, the rightmost coordinate varies the fastest. The col_major() method creates a generalized column-major layout, where the leftmost coordinate varies the fastest.
print(Layout.row_major(4, 4, 4))
print(Layout.col_major(4, 4, 4))
Output:
((4, 4, 4):(16, 4, 1))
((4, 4, 4):(1, 4, 16))
If you know the shape and strides in advance, you can construct an arbitrarily complex layout using the Layout constructor. For example:
var tiled_layout = Layout(
    IntTuple(IntTuple(3, 2), IntTuple(2, 5)), # shape
    IntTuple(IntTuple(1, 6), IntTuple(3, 12)) # strides
)
print_layout(tiled_layout)
Output:

(((3, 2), (2, 5)):((1, 6), (3, 12)))
       0    1    2    3    4    5    6    7    8    9
    +----+----+----+----+----+----+----+----+----+----+
 0  |  0 |  3 | 12 | 15 | 24 | 27 | 36 | 39 | 48 | 51 |
    +----+----+----+----+----+----+----+----+----+----+
 1  |  1 |  4 | 13 | 16 | 25 | 28 | 37 | 40 | 49 | 52 |
    +----+----+----+----+----+----+----+----+----+----+
 2  |  2 |  5 | 14 | 17 | 26 | 29 | 38 | 41 | 50 | 53 |
    +----+----+----+----+----+----+----+----+----+----+
 3  |  6 |  9 | 18 | 21 | 30 | 33 | 42 | 45 | 54 | 57 |
    +----+----+----+----+----+----+----+----+----+----+
 4  |  7 | 10 | 19 | 22 | 31 | 34 | 43 | 46 | 55 | 58 |
    +----+----+----+----+----+----+----+----+----+----+
 5  |  8 | 11 | 20 | 23 | 32 | 35 | 44 | 47 | 56 | 59 |
    +----+----+----+----+----+----+----+----+----+----+
The result is a 6x10 tile-major layout. The layout is indexed vertically in 2 groups of 3 rows (3, 2) : (1, 6) ( and horizontally in 5 groups of 2 columns (2, 5):(3, 12). Alternatively, you can think of this as a layout consisting of 3x2 column-major tiles ((3, 2):(1, 3)) that are arranged into two rows of 5, ((2, 5):(6, 12)).
The Layout constructor works fine if you know the shape and strides in advance, but calculating the strides for a complicated layout isn't always intuitive.
An easier way to generate this layout is the tile_to_shape() function. This takes a layout (representing the tile) and a final shape to tile to:
var tts = tile_to_shape(Layout.col_major(3, 2), IntTuple(6, 10))
print_layout(tts)
Output:
(((3, 2), (2, 5)):((1, 6), (3, 12)))
       0    1    2    3    4    5    6    7    8    9
    +----+----+----+----+----+----+----+----+----+----+
 0  |  0 |  3 | 12 | 15 | 24 | 27 | 36 | 39 | 48 | 51 |
    +----+----+----+----+----+----+----+----+----+----+
 1  |  1 |  4 | 13 | 16 | 25 | 28 | 37 | 40 | 49 | 52 |
    +----+----+----+----+----+----+----+----+----+----+
 2  |  2 |  5 | 14 | 17 | 26 | 29 | 38 | 41 | 50 | 53 |
    +----+----+----+----+----+----+----+----+----+----+
 3  |  6 |  9 | 18 | 21 | 30 | 33 | 42 | 45 | 54 | 57 |
    +----+----+----+----+----+----+----+----+----+----+
 4  |  7 | 10 | 19 | 22 | 31 | 34 | 43 | 46 | 55 | 58 |
    +----+----+----+----+----+----+----+----+----+----+
 5  |  8 | 11 | 20 | 23 | 32 | 35 | 44 | 47 | 56 | 59 |
    +----+----+----+----+----+----+----+----+----+----+
A variation on tile_to_shape() is the blocked_product() function. The main difference is that where tile_to_shape() takes an output shape, blocked_product() takes a tiler layout: essentially, every element in the tiler layout is replaced by a tile. The following example generates the same tiled layout using blocked_product(). It also prints out the two input layouts.
# Define 2x3 tile
var tile = Layout.col_major(3, 2)
# Define a 2x5 tiler
var tiler = Layout.col_major(2, 5)
var blocked = blocked_product(tile, tiler)

print("Tile:")
print_layout(tile)
print("\nTiler:")
print_layout(tiler)
print("\nTiled layout:")
print(blocked)
Output:
Tile:
((3, 2):(1, 3))
      0   1
    +---+---+
 0  | 0 | 3 |
    +---+---+
 1  | 1 | 4 |
    +---+---+
 2  | 2 | 5 |
    +---+---+

Tiler:
((2, 5):(1, 2))
       0    1    2    3    4
    +----+----+----+----+----+
 0  |  0 |  2 |  4 |  6 |  8 |
    +----+----+----+----+----+
 1  |  1 |  3 |  5 |  7 |  9 |
    +----+----+----+----+----+

Tiled layout:
(((3, 2), (2, 5)):((1, 6), (3, 12)))

As you can see, blocked_product() combines two simple layouts to generate a more complex one.
Finally, if you know the shape you want and the order in which you want to iterate through the dimensions, you can use the make_ordered_layout() function. For example, the following example is yet one more way to generate the previous tiled layout:
var ordered = make_ordered_layout(
    IntTuple(IntTuple(3, 2), IntTuple(2, 5)), # shape
    IntTuple(IntTuple(0, 2), IntTuple(1, 3))  # order
)
print(ordered)
Output:
(((3, 2), (2, 5)):((1, 6), (3, 12)))
The generated layout's strides follow the same ordering as order-that is, the dimension with the smallest corresponding order value has the smallest stride value, and so on. The strides are computed such that the layout is dense-that is, the logical multidimensional array is contiguous.
Non-contiguous layouts​
All of the examples so far have been dense layouts, where all of the elements are contiguous in memory. However, layouts can also describe sparse logical arrays. For example, a (4:2) layout is a sparse 1D array:

Figure 8. 1D sparse layout (4:2) 
A layout's cosize is the size of the layout's codomain, which you can think of as the size of the smallest contiguous array that can contain all of the layout's elements. The cosize is the largest linear index value generated by the layout plus 1. So in the example in Figure 8, the layout has a size of 4, but a cosize of 7.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Edit this page


Edit this page

Using LayoutTensor
A LayoutTensor provides a view of multi-dimensional data stored in a linear array. LayoutTensor abstracts the logical organization of multi-dimensional data from its actual arrangement in memory. You can generate new tensor "views" of the same data without copying the underlying data. This facilitates essential patterns for writing performant computational algorithms, such as:
* Extracting tiles (sub-tensors) from existing tensors. This is especially valuable on the GPU, allowing a thread block to load a tile into shared memory, for faster access and more efficient caching.
* Vectorizing tensors-reorganizing them into multi-element vectors for more performant memory loads and stores.
* Partitioning a tensor into thread-local fragments to distribute work across a thread block.
LayoutTensor is especially valuable for writing GPU kernels, and a number of its APIs are GPU-specific. However, LayoutTensor can also be used for CPU-based algorithms.
A LayoutTensor consists of three main properties:
* A layout, defining how the elements are laid out in memory.
* A DType, defining the data type stored in the tensor.
* A pointer to memory where the data is stored.
Figure 1 shows the relationship between the layout and the storage.

Figure 1. Layout and storage for a 2D tensor 
Figure 1 shows a 2D column-major layout, and the corresponding linear array of storage. The values shown inside the layout are offsets into the storage: so the coordinates (0, 1) correspond to offset 2 in the storage.
Because LayoutTensor is a view, creating a new tensor based on an existing tensor doesn't require copying the underlying data. So you can easily create a new view, representing a tile (sub-tensor), or accessing the elements in a different order. These views all access the same data, so changing the stored data in one view changes the data seen by all of the views.
Each element in a tensor can be either a single (scalar) value or a SIMD vector of values. For a vectorized layout, you can specify an element layout that determines how the vector elements are laid out in memory. For more information, see Vectorizing tensors.
Accessing tensor elements​
For tensors with simple row-major or column-major layouts, you can address a layout tensor like a multidimensional array to access elements:
element = tensor2d[x, y]
tensor2d[x, y] = z
The number of indices passed to the subscript operator must match the number of coordinates required by the tensor. For simple layouts, this is the same as the layout's rank: two for a 2D tensor, three for a 3D tensor, and so on. If the number of indices is incorrect, you may see a cryptic runtime error.
# Indexing into a 2D tensor requires two indices
el1 = tensor2d[x, y]  # Works
el2 = tensor2d[x]  # Runtime error
For more complicated "nested" layouts, such as tiled layouts, the number of indices doesn't match the rank of the tensor. For details, see Tensor indexing and nested layouts.
The __getitem__() method returns a SIMD vector of elements, and the compiler can't statically determine the size of the vector (which is the size of the tensor's element layout). This can cause type checking errors at compile time, because some APIs can only accept scalar values (SIMD vectors of width 1). For example, consider the following code:
i: Int = SIMD[DType.int32, width](15)
If width is 1, the vector can be implicitly converted to an Int, but if width is any other value, the vector can't be implicitly converted. If width isn't known at compile time, this produces an error.
If your tensor stores scalar values, you can work around this by explicitly taking the first item in the vector:
var element = tensor[row, col][0]  # element is guaranteed to be a scalar value
You can also access elements using the load() and store() methods, which let you specify the vector width explicitly:
var elements: SIMD[DType.float32, 4]
var elements = tensor.load[4](row, col)
elements = elements * 2
tensor.store(row, col, elements)
Tensor indexing and nested layouts​
A tensor's layout may have nested modes (or sub-layouts), as described in Introduction to layouts. These layouts have one or more of their dimensions divided into sub-layouts. For example, Figure 2 shows a tensor with a nested layout:

Figure 2. Tensor with nested layout 
Figure 2 shows a tensor with a tile-major nested layout. Instead of being addressed with a single coordinate on each axis, it has a pair of coordinates per axis. For example, the coordinates ((1, 0), (0, 1)) map to the offset 6.
You can't pass nested coordinates to the subscript operator [], but you can pass a flattened version of the coordinates. For example:
# retrieve the value at ((1, 0), (0, 1))
element = nested_tensor[1, 0, 0, 1][0]
The number of indices passed to the subscript operator must match the flattened rank of the tensor.
You can't currently use the load() and store() methods for tensors with nested layouts. However, these methods are usually used on tensors that have been tiled, which yields a tensor with a simple layout.
Creating a LayoutTensor​
There are several ways to create a LayoutTensor, depending on where the tensor data resides:
* On the CPU.
* In GPU global memory.
* In GPU shared or local memory.
In addition to methods for creating a tensor from scratch, LayoutTensor provides a number of methods for producing a new view of an existing tensor.
No bounds checking
The LayoutTensor constructors don't do any bounds-checking to verify that the allocated memory is large enough to hold all of the elements specified in the layout. It's up to the user to ensure that the proper amount of space is allocated.
Creating a LayoutTensor on the CPU​
While LayoutTensor is often used on the GPU, you can also use it to create tensors for use on the CPU.
To create a LayoutTensor for use on the CPU, you need a Layout and a block of memory to store the tensor data. A common way to allocate memory for a LayoutTensor is to use an InlineArray:
alias rows = 8
alias columns = 16
alias layout = Layout.row_major(rows, columns)
var storage = InlineArray[Float32, rows * columns](uninitialized=True)
var tensor = LayoutTensor[DType.float32, layout](storage).fill(0)
InlineArray is a statically-sized, stack-allocated array, so it's a fast and efficient way to allocate storage for most kinds of LayoutTensor. There are target-dependent limits on how much memory can be allocated this way, however.
You can also create a LayoutTensor using an UnsafePointer. This may be preferable for very large tensors.
alias rows = 1024
alias columns = 1024
alias buf_size = rows * columns
alias layout = Layout.row_major(rows, columns)
var ptr = alloc[Float32](buf_size)
memset(ptr, 0, buf_size)
var tensor = LayoutTensor[DType.float32, layout](ptr)
Note that this example uses memset() instead of the LayoutTensor fill() method. The fill() method performs elementwise initialization of the tensor, so it may be slow for large tensors.
Creating a LayoutTensor on the GPU​
When creating a LayoutTensor for use on the GPU, you need to consider which memory space the tensor data will be stored in:
* GPU global memory can only be allocated from the host (CPU), as a DeviceBuffer.
* GPU shared or local memory can be statically allocated on the GPU.
Creating a LayoutTensor in global memory​
You must allocate global memory from the host side, by allocating a DeviceBuffer. You can either construct a LayoutTensor using this memory on the host side, before invoking a GPU kernel, or you can construct a LayoutTensor inside the kernel itself:
* On the CPU, you can construct a LayoutTensor using a DeviceBuffer as its storage. Although you can create this tensor on the CPU and pass it in to a kernel function, you can't directly modify its values on the CPU, since the memory is on the GPU.
* On the GPU: When a DeviceBuffer is passed in to enqueue_function_checked(), the kernel receives a corresponding UnsafePointer in place of the DeviceBuffer. The kernel can then create a LayoutTensor using the pointer.
In both cases, if you want to initialize data for the tensor from the CPU, you can call enqueue_copy() or enqueue_memset() on the buffer prior to invoking the kernel. The following example shows initializing a LayoutTensor from the CPU and passing it to a GPU kernel.
def initialize_tensor_from_cpu_example():
    alias dtype = DType.float32
    alias rows = 32
    alias cols = 8
    alias block_size = 8
    alias row_blocks = rows // block_size
    alias col_blocks = cols // block_size
    alias input_layout = Layout.row_major(rows, cols)
    alias size: Int = rows * cols


    fn kernel(tensor: LayoutTensor[dtype, input_layout, MutAnyOrigin]):
        if global_idx.y < UInt(tensor.shape[0]()) and global_idx.x < UInt(
            tensor.shape[1]()
        ):
            tensor[global_idx.y, global_idx.x] = (
                tensor[global_idx.y, global_idx.x] + 1
            )


    try:
        var ctx = DeviceContext()
        var host_buf = ctx.enqueue_create_host_buffer[dtype](size)
        var dev_buf = ctx.enqueue_create_buffer[dtype](size)
        ctx.synchronize()

        var expected_values = List[Scalar[dtype]](capacity=size)

        for i in range(size):
            host_buf[i] = Scalar[dtype](i)
            expected_values[i] = Scalar[dtype](i + 1)
        ctx.enqueue_copy(dev_buf, host_buf)
        var tensor = LayoutTensor[dtype, input_layout](dev_buf)

        ctx.enqueue_function_checked[kernel, kernel](
            tensor,
            grid_dim=(col_blocks, row_blocks),
            block_dim=(block_size, block_size),
        )
        ctx.enqueue_copy(host_buf, dev_buf)
        ctx.synchronize()

        for i in range(rows * cols):
            if host_buf[i] != expected_values[i]:
                raise Error(
                    String("Error at position {} expected {} got {}").format(
                        i, expected_values[i], host_buf[i]
                    )
                )
    except error:
        print(error)
Creating a LayoutTensor in shared or local memory​
To create a tensor on the GPU in shared memory or local memory, use the LayoutTensor.stack_allocation() static method to create a tensor with backing memory in the appropriate memory space.
Both shared and local memory are very limited resources, so a common pattern is to copy a small tile of a larger tensor into shared memory or local memory to reduce memory access time.
alias tile_layout = Layout.row_major(16, 16)

var shared_tile = LayoutTensor[
    dtype,
    tile_layout,
    MutAnyOrigin,
    address_space = AddressSpace.SHARED,
].stack_allocation()
In the case of shared memory, all threads in a thread block see the same allocation. For local memory, each thread gets a separate allocation.
There's no way to explicitly allocate register memory. However, the compiler can promote some local memory allocations to registers. To enable this optimization, keep the size of the tensor small, and all indexing into the tensor static-for example, using @parameter for loops.
The name stack_allocation() is misleading. It is a static allocation, meaning the allocation is processed at compile time. The allocation is like a C/C++ stack allocation in that its lifetime ends when the function in which it was allocated returns. This API may be subject to change in the near future.
Tiling tensors​
A fundamental pattern for using a layout tensor is to divide the tensor into smaller tiles to achieve better data locality and cache efficiency. In a GPU kernel you may want to select a tile that corresponds to the size of a thread block. For example, given a 2D thread block of 16x16 threads, you could use a 16x16 tile (with each thread handling one element in the tile) or a 64x16 tile (with each thread handling 4 elements from the tensor).
Tiles are most commonly 1D or 2D. For element-wise calculations, where the output value for a given tensor element depends on only one input value, 1D tiles are easy to reason about. For calculations that involve neighboring elements, 2D tiles can help maintain data locality. For example, matrix multiplication or 2D convolution operations usually use 2D tiles.
LayoutTensor provides a tile() method for extracting a single tile. You can also iterate through tiles using the LayoutTensorIter type.
When tiling a tensor that isn't an exact multiple of the tile size, you can create the tensor as a masked tensor (with the optional masked parameter set to True). When tiling a masked tensor, the tile operations will return partial tiles at the edges of the tensor. These tiles will be smaller than the requested tile size. You can use the tensor.dim(axis) method to query the tile dimensions at runtime.
Extracting a tile​
The LayoutTensor.tile() method extracts a tile with a given size at a given set of coordinates:
alias rows = 2
alias columns = 4
alias tile_size = 32
alias tile_layout = Layout.row_major(tile_size, tile_size)
alias tiler_layout = Layout.row_major(rows, columns)
alias tiled_layout = blocked_product(tile_layout, tiler_layout)
var storage = InlineArray[Float32, tiled_layout.size()](uninitialized=True)
for i in range(tiled_layout.size()):
    storage[i] = i
var tensor = LayoutTensor[DType.float32, tiled_layout](storage)
var tile = tensor.tile[tile_size, tile_size](0, 1)
This code creates a 64x128 tensor with 32x32 tiles, and extracts the tile at row 0, column 1, as shown in Figure 3.

Figure 3. Extracting a tile from a tensor 
Note that the coordinates are specified in tiles.
The layout of the extracted tile depends on the layout of the parent tensor. For example, if the parent tensor has a column-major layout, the extract tile has a column-major layout.
If you're extracting a tile from a tensor with a tiled layout, the extracted tile must match the tile boundaries of the parent tensor. For example, if the parent tensor is composed of 8x8 row-major tiles, a tile size of 8x8 yields an extracted tile with an 8x8 row-major layout.
Trying to extract a tile that's not an even multiple of the parent tile size usually results in an error.
If you need to know the type of the tile (to declare a variable, for example), you can use the TileType alias, with the same tile size parameters.
var my_tile: tensor.TileType[tile_size, tile_size]
for i in range (rows):
    for j in range(columns):
        my_tile = tensor.tile[tile_size, tile_size](i, j)
        # ... do something with the tile ...
Tiled iterators​
The LayoutTensorIter struct provides a way to iterate through a block of memory, generating a layout tensor for each position. There are two ways to use LayoutTensorIter:
* Starting with a memory buffer, you can generate a series of tiles.
* Given an existing layout tensor, you can extract a set of tiles along a given axis.
Tiling a memory buffer​
When you start with a memory buffer, LayoutTensorIter iterates through the memory one tile at a time. This essentially treats the memory as a flat array of tiles.
alias buf_size = 128
var storage = InlineArray[Int16, buf_size](uninitialized=True)
for i in range(buf_size):
    storage[i] = i
alias tile_layout = Layout.row_major(4, 4)
var iter = LayoutTensorIter[
    DType.int16,
    tile_layout,
    MutAnyOrigin
](storage.unsafe_ptr(), buf_size)

for i in range(ceildiv(buf_size, tile_layout.size())):
    var tile = iter[]
    # ... do something with tile
    iter += 1
The iterator constructor takes all of the parameters you'd use to construct a LayoutTensor-a DType, layout, and an origin-and as arguments it takes a pointer and the size of the memory buffer.
Note that the iterator doesn't work like a standard iterator, and you can't use it directly in a for statement like you would use a collection. Instead, you can use either the dereference operator (iter[]) or the get() method to retrieve a LayoutTensor representing the tile at the current position.
You can advance the iterator by incrementing it, as shown above. The iterator also supports next() and next_unsafe() methods, which return a copy of the iterator incremented by a specified offset (default 1). This means you can also use a pattern like this:
for i in range(num_tiles):
    current_tile = iter.next(i)[]
    ...
LayoutTensorIter also has an optional boolean circular parameter. A LayoutTensorIter created with circular=True treats the memory buffer as circular; when it hits the end of the buffer, it starts over again at the beginning.
Tiling a LayoutTensor​
To iterate over an existing tensor, call the tiled_iterator() method, which produces a LayoutTensorIter:
# given a tensor of size rows x cols
alias num_row_tiles = ceildiv(rows, tile_size)
alias num_col_tiles = ceildiv(cols, tile_size)

for i in range(num_row_tiles):
    var iter = tensor.tiled_iterator[tile_size, tile_size, axis=1](i, 0)

    for _ in range(num_col_tiles):
        var tile = iter[]
        # ... do something with the tile
        iter += 1
Vectorizing tensors​
When working with tensors, it's frequently efficient to access more than one value at a time. For example, having a single GPU thread calculate multiple output values ("thread coarsening") can frequently improve performance. Likewise, when copying data from one memory space to another, it's often helpful for each thread to copy a SIMD vector worth of values, instead of a single value. Many GPUs have vectorized copy instructions that can make copying more efficient.
To choose the optimum vector size, you need to know the vector operations supported for your current GPU for the data type you're working with. (For example, if you're working with 4 byte values and the GPU supports 16 byte copy operations, you can use a vector width of 4.) The LayoutTensor copy operations support copy sizes up to 16 bytes.
The vectorize() method creates a new view of the tensor where each element of the tensor is a vector of values.
var vectorized_tensor = tensor.vectorize1, 4
The vectorized tensor is a view of the original tensor, pointing to the same data. The underlying number of scalar values remains the same, but the tensor layout and element layout changes, as shown in Figure 4.

Figure 4. Vectorizing a tensor 
Partitioning a tensor across threads​
When working with tensors on the GPU, it's sometimes desirable to distribute the elements of a tensor across the threads in a thread block. The distribute() method takes a thread layout and a thread ID and returns a thread-specific fragment of the tensor.
The thread layout is tiled across the tensor. The Nth thread receives a fragment consisting of the Nth value from each tile. For example, Figure 5 shows how distribute() forms fragments given a 4x4, row-major tensor and a 2x2, column-major thread layout:

Figure 5. Partitioning a tensor into fragments 
In Figure 5, the numbers in the data layout represent offsets into storage, as usual. The numbers in the thread layout represent thread IDs.
The example in Figure 5 uses a small thread layout for illustration purposes. In practice, it's usually optimal to use a thread layout that's the same size as the warp size of your GPU, so the work is divided across all available threads. For example, the following code vectorizes and partitions a tensor over a full warp worth of threads:
alias thread_layout = Layout.row_major(WARP_SIZE // simd_size, simd_size)
var fragment = tile.vectorize[1, simd_size]().distribute[thread_layout](lane_id())
Given a 16x16 tile size, a warp size of 32 and a simd_size of 4, this code produces a 16x4 tensor of 1x4 vectors. The thread layout is an 8x4 row major layout.
Copying tensors​
The layout-tensor package provides a large set of utilities for copying tensors. A number of these are specialized for copying between various GPU memory spaces. All of the layout tensor copy methods respect the layouts-so you can transform a tensor by copying it to a tensor with a different layout.
LayoutTensor itself provides two methods for copying tensor data:
* copy_from() copies data from a source tensor to the current tensor, which may be in a different memory space.
* copy_from_async() is an optimized copy mechanism for asynchronously copying from GPU global memory to shared memory.
Both of these methods copy the entire source tensor. To divide the copying work among multiple threads, you need to use distribute() to create thread-local tensor fragments, as described in Partitioning a tensor across threads.
The following code sample demonstrates using both copy methods to copy data to and from shared memory.
fn copy_from_async_example():
    alias dtype = DType.float32
    alias rows = 128
    alias cols = 128
    alias block_size = 16
    alias num_row_blocks = rows // block_size
    alias num_col_blocks = cols // block_size
    alias input_layout = Layout.row_major(rows, cols)
    alias simd_width = 4

    fn kernel(tensor: LayoutTensor[dtype, input_layout, MutAnyOrigin]):
        # extract a tile from the input tensor.
        var global_tile = tensor.tile[block_size, block_size](
            Int(block_idx.y), Int(block_idx.x)
        )
        alias tile_layout = Layout.row_major(block_size, block_size)
        var shared_tile = LayoutTensor[
            dtype,
            tile_layout,
            MutAnyOrigin,
            address_space = AddressSpace.SHARED,
        ].stack_allocation()

        # Create thread layouts for copying
        alias thread_layout = Layout.row_major(
            WARP_SIZE // simd_width, simd_width
        )
        var global_fragment = global_tile.vectorize[
            1, simd_width
        ]().distribute[thread_layout](lane_id())
        var shared_fragment = shared_tile.vectorize[
            1, simd_width
        ]().distribute[thread_layout](lane_id())

        shared_fragment.copy_from_async(global_fragment)
        @parameter
        if is_nvidia_gpu():
            async_copy_wait_all()
        barrier()

        # Put some data into the shared tile that we can verify on the host.
        if global_idx.y < rows and global_idx.x < cols:
            shared_tile[thread_idx.y, thread_idx.x] =
                shared_tile[thread_idx.y, thread_idx.x] + 1

        global_fragment.copy_from(shared_fragment)

    try:
        var ctx = DeviceContext()
        var host_buf = ctx.enqueue_create_host_buffer[dtype](rows * cols)
        var dev_buf = ctx.enqueue_create_buffer[dtype](rows * cols)
        for i in range(rows * cols):
            host_buf[i] = i
        var tensor = LayoutTensor[dtype, input_layout](dev_buf)
        ctx.enqueue_copy(dev_buf, host_buf)
        ctx.enqueue_function_checked[kernel, kernel](
            tensor,
            grid_dim=(num_row_blocks, num_col_blocks),
            block_dim=(block_size, block_size),
        )
        ctx.enqueue_copy(host_buf, dev_buf)
        ctx.synchronize()
        for i in range(rows * cols):
            if host_buf[i] != i + 1:
                raise Error(
                    String("Unexpected value ", host_buf[i], " at position ", i)
                )
    except error:
        print(error)
Thread-aware copy functions​
The layout_tensor package also includes a number of specialized copy functions for different scenarios, such as copying from shared memory to local memory. These functions are all thread-aware: instead of passing in tensor fragments, you pass in a thread layout which the function uses to partition the work.
As with the copy_from() and copy_from_async() methods, use the vectorize() method prior to copying to take advantage of vectorized copy operations.
Many of the thread-aware copy functions have very specific requirements for the shape of the copied tensor and thread layout, based on the specific GPU and data type in use.
Summary​
In this document, we've explored the fundamental concepts and practical usage of LayoutTensor. At its core, LayoutTensor provides a powerful abstraction for working with multi-dimensional data. By combining a layout (which defines memory organization), a data type, and a memory pointer, LayoutTensor enables flexible and efficient data manipulation without unnecessary copying of the underlying data.
We covered several essential tensor operations that form the foundation of working with LayoutTensor, including creating tensors, accessing tensor elements, and copying data between tensors.
We also covered key patterns for optimizing data access:
* Tiling tensors for data locality. Accessing tensors one tile at a time can improve cache efficiency. On the GPU, tiling can allow the threads of a thread block to share high-speed access to a subset of a tensor.
* Vectorizing tensors for more efficient data loads and stores.
* Partitioning or distributing tensors into thread-local fragments for processing.
These patterns provide the building blocks for writing efficient kernels in Mojo while maintaining clean, readable code.
To see some practical examples of LayoutTensor in use, see Optimize custom ops for GPUs with Mojo .
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit


Python interoperability
Not only does Mojo use a Pythonic syntax, our plan is to provide full compatibility with the Python ecosystem. There are two types of compatibility (or interoperability) that we support:
* Calling Python from Mojo:
You can import existing Python modules and use them in a Mojo program. This is 100% compatible because we use the CPython runtime without modification for full compatibility with existing Python libraries. You can construct Python objects and call Python functions directly from Mojo, using the CPython interpreter as a dynamic library (shown as libpython.dylib in figure 1).
* Calling Mojo from Python:
You can extend your Python code with high-performance Mojo code (or incrementally migrate Python code to Mojo). Because Mojo is a compiled language, we can't directly "evaluate" Mojo code from Python. Instead, you must declare which Mojo functions and types are available to be called from Python (declare the "bindings"), and then you can import them in your Python code (shown as mojo_module in figure 1) just like any other module-there's no extra compilation step.
Figure 1. A simplified look at how a Mojo program calls into Python and a Python program calls into a Mojo module. 
By embracing both directions of language interop, you can choose how to use Mojo with Python in a way that works best for your use case.
To learn more about bridging Python ↔ Mojo, continue reading:

Calling Python from Mojo
How to import and use Python modules in Mojo code.

Calling Mojo from Python
How to import and use Mojo modules in Python code.

Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit


Calling Python from Mojo
The Python ecosystem is full of useful libraries, so you shouldn't have to rewrite them in Mojo. Instead, you can simply import Python packages and call Python APIs from Mojo. The Python code runs in a standard Python interpreter (CPython), so your existing Python code doesn't need to change.
Specify your Python version​
Mojo doesn't include a CPython interpreter-it uses the CPython interpreter provided by your environment's default Python version. So be sure you know which Python version you're using in each environment where your Mojo code will run.
To ensure you get consistent results, we recommend you use Pixi to manage your package dependency and virtual environment. In a Pixi project, you can specify the Python version like this:
pixi add "python==3.11"
Now, even if your operating system's default Python version is something else, your Pixi project (and the Mojo code inside) always uses Python 3.11.
pixi run python --version
Python 3.11.0
Import a Python module in Mojo​
To import a Python module in Mojo, just call Python.import_module() with the module name. The following shows an example of importing the standard Python NumPy package:
🔥 Mojo
from python import Python

def main():
    # This is equivalent to Python's `import numpy as np`
    np = Python.import_module("numpy")

    # Now use numpy as if writing in Python
    array = np.array(Python.list(1, 2, 3))
    print(array)
Running this program produces the following output:
[1 2 3]
Assuming that you have the NumPy package installed in your environment, this imports NumPy and you can use any of its features.
If you want to use Python builtin APIs, you just need to import the builtins module the same way. For example:
🔥 Mojo
from python import Python

def main():
    np = Python.import_module("numpy")
    array = np.array(Python.list(1, 2, 3))

    builtins = Python.import_module("builtins")
    print(builtins.type(array))
<class 'numpy.ndarray'>
A few things to note:
* The import_module() method returns a reference to the module in the form of a PythonObject wrapper. You must store the reference in a variable and then use it as shown in the example above to access functions, classes, and other objects defined by the module. See Mojo wrapper objects for more information about the PythonObject type.
* Currently, you cannot import individual members (such as a single Python class or function). You must import the whole Python module and then access members through the module name.
* Mojo doesn't yet support top-level code, so the import_module() call must be inside another method. This means you may need to import a module multiple times or pass around a reference to the module. This works the same way as Python: importing the module multiple times won't run the initialization logic more than once, so you don't pay any performance penalty.
* import_module() may raise an exception (for example, if the module isn't installed). If you're using it inside an fn function, you need to either handle errors (using a try/except clause), or add the raises keyword to the function signature. You'll also see this when calling Python functions that may raise exceptions. (Raising exceptions is much more common in Python code than in the Mojo standard library, which limits their use for performance reasons.)
* We recommend using a package manager such as pixi, uv, or conda to manage your environment. For instructions on setting up a Mojo project with pixi, see Create a Mojo project in the Get started with Mojo tutorial.
Caution
mojo build doesn't include the Python packages used by your Mojo project. Instead, Mojo loads the Python interpreter and Python packages at runtime, so they must be provided in the environment where you run the Mojo program (such as inside the pixi environment where you built the executable).
Import a local Python module​
If you have some local Python code you want to use in Mojo, just add the directory to the Python path and then import the module.
For example, suppose you have a Python file named mypython.py:
🐍 mypython.py
import numpy as np

def gen_random_values(size, base):
    # generate a size x size array of random numbers between base and base+1
    random_array = np.random.rand(size, size)
    return random_array + base
Here's how you can import it and use it in a Mojo file:
🔥 main.mojo
from python import Python

def main():
    Python.add_to_path("path/to/module")
    mypython = Python.import_module("mypython")

    values = mypython.gen_random_values(2, 3)
    print(values)
Both absolute and relative paths work with add_to_path(). For example, you can import from the local directory like this:
🔥 Mojo
Python.add_to_path(".")
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit



Calling Mojo from Python
If you have an existing Python project that would benefit from Mojo's high-performance computing, you shouldn't have to rewrite the whole thing in Mojo. Instead, you can write just the performance-critical parts your code in Mojo and then call it from Python.
Beta feature
Calling Mojo code from Python is in early development. You should expect a lot of changes to the API and ergonomics. Likewise, this documentation is still a work in progress. See below for known limitations.
Import a Mojo module in Python​
To illustrate what calling Mojo from Python looks like, we'll start with a simple example, and then dig into the details of how it works and what is possible today.
Consider a project with the following structure:
project
├── 🐍 main.py
└── 🔥 mojo_module.mojo
The main entrypoint is a Python program called main.py, and the Mojo code includes functions to call from Python.
For example, let's say we want a Mojo function to take a Python value as an argument:
🔥 mojo_module.mojo
fn factorial(py_obj: PythonObject) raises -> Python
    var n = Int(py_obj)
    return math.factorial(n)
And we want to call it from Python like this:
🐍 main.py
import mojo_module

print(mojo_module.factorial(5))
However, before we can call the Mojo function from Python, we must declare it so Python knows it exists.
Because Python is trying to load mojo_module, it looks for a function called PyInit_mojo_module(). (If our file was called foo.mojo, the function Python looked for would be PyInit_foo().) Within the PyInit_mojo_module(), we must declare all Mojo functions and types that are callable from Python using PythonModuleBuilder.
So the complete Mojo code looks like this:
🔥 mojo_module.mojo
from python import PythonObject
from python.bindings import PythonModuleBuilder
import math
from os import abort

@export
fn PyInit_mojo_module() -> PythonObject:
    try:
        var m = PythonModuleBuilder("mojo_module")
        m.def_function[factorial]("factorial", docstring="Compute n!")
        return m.finalize()
    except e:
        return abort[PythonObject](String("error creating Python Mojo module:", e))

fn factorial(py_obj: PythonObject) raises -> PythonObject:
    # Raises an exception if `py_obj` is not convertible to a Mojo `Int`.
    var n = Int(py_obj)

    return math.factorial(n)
On the Python side, we add the directory containing mojo_module.mojo to the Python path, and then use a normal import statement to load our Mojo code:
🐍 main.py
import mojo.importer
import sys

sys.path.insert(0, "")

import mojo_module

print(mojo_module.factorial(5))
That's it! Try it:
python main.py
120
How it works​
Python supports a standard mechanism called Python extension modules that enables compiled languages (like Mojo, C, C++, or Rust) to make themselves callable from Python in an intuitive way. Concretely, a Python extension module is simply a dynamic library that defines a suitable PyInit_*() function.
Mojo comes with built-in functionality for defining Python extension modules. The special stuff happens in the mojo.importer module we imported.
If we have a look at the filesystem after Python imports the Mojo code, we'll notice there's a new __mojocache__ directory, with dynamic library (.so) file inside:
project
├── main.py
├── mojo_module.mojo
└── __mojocache__
    └── mojo_module.hash-ABC123.so
Loading mojo.importer loads our Python Mojo import hook, which behind the scenes looks for a .mojo (or .🔥) file that matches the imported module name, and if found, compiles it using mojo build --emit shared-lib to generate a static library. The resulting file is stored in __mojocache__, and is rebuilt only when it becomes stale (typically, when the Mojo source file changes).
The __mojocache__ directory should only contain derived artifacts. It is always safe to delete the contents of a __mojocache__ directory. Needed artifacts will simply be rebuilt the next time the Mojo module is imported.
Now that we've looked at the basics of how Mojo can be used from Python, let's dig into the available features and how you can leverage them to accelerate your Python with Mojo.
Bindings features​
Binding Mojo types​
You can bind any Mojo type for use in Python using PythonModuleBuilder. For example:
🔥 Mojo
@fieldwise_init
struct Person(Movable, Representable):
    var name: String
    var age: Int

    fn __repr__(self) -> String:
        return String("Person(", self.name, ", ", self.age, ")")

@export
fn PyInit_person_module() -> PythonObject:
    try:
        var mb = PythonModuleBuilder("person_module")
        var person_type = mb.add_type[Person]("Person")
    except e:
        return abort[PythonObject]("error creating Mojo module")
When you call add_type(), it returns a PythonTypeBuilder, which you can then use to bind the type constructor (see binding Python initializers, below) and methods.
Any Mojo type bound using a PythonTypeBuilder has the resulting Python 'type' object globally registered, enabling two features:
* Constructing Python objects that wrap Mojo values for use from Python using PythonObject(alloc=Person(..)).
* Downcasting using python_obj.downcast_value_ptr[Person]()
Mojo types must implement Representable to be bound for use in Python. Additional traits are required for specific binding features: Movable for custom initializers (def_py_init), and both Defaultable and Movable for default initializers (def_init_defaultable).
However, merely binding a Mojo type to a Python type object isn't very useful on its own. Next, we'll tell Python how to interact with our Mojo type-starting with how to construct instances of our Mojo type from within Python.
Constructing Mojo objects in Python​
Mojo types can be constructed from Python by declaring a Mojo constructor function as a Python-compatible object initializer using def_py_init() when you add the type to your module. For example:
🔥 Mojo
@export
fn PyInit_person_module() -> PythonObject:
    try:
        var mb = PythonModuleBuilder("person_module")
        _ = mb.add_type[Person]("Person").def_py_init[Person.py_init]()
        return mb.finalize()
    except e:
        return abort[PythonObject](
            String("error creating Python Mojo module:", e)
        )

@fieldwise_init
struct Person(Movable, Representable):
    var name: String
    var age: Int

    fn __repr__(self) -> String:
        return String("Person(", self.name, ", ", self.age, ")")

    @staticmethod
    fn py_init(
        out self: Person, args: PythonObject, kwargs: PythonObject
    ) raises:
        # Validate argument count
        if len(args) != 2:
            raise Error("Person() takes exactly 2 arguments")

        # Convert Python arguments to Mojo types
        var name = String(args[0])
        var age = Int(args[1])

        self = Self(name, age)
With this Mojo binding, you can create Person instances in Python:
🐍 Python
person = person_module.Person("Sarah", 32)
print(person)
Person(Sarah, 32)
For types that support default construction, you can use the simpler def_init_defaultable() method:
🔥 Mojo
var counter_type = m.add_type[Counter]("Counter")
counter_type.def_init_defaultable[Counter]()
This enables Python code to create instances without arguments:
🐍 Python
counter = counter_module.Counter()  # Creates Counter()
"Constructor" vs "Initializer"
In Python, object construction happens across both the __new__() and __init__() methods, so the __init__() method is technically just the attribute initializer. However, in a Mojo struct, there's no __new__() method, so we prefer to always call __init__() the constructor.
Returning Mojo objects to Python​
Mojo functions called from Python don't just need to be able to accept PythonObject values as arguments, they also need to be able to return new values. And sometimes, they even need to be able to return Mojo native values back to Python. This is possible by using the PythonObject(alloc=<value>) constructor.
An example of this looks like:
🔥 Mojo
fn create_person() -> PythonObject:
    var person = Person("Sarah", 32)
    return PythonObject(alloc=person^)
Caution
PythonObject(alloc=...) will raise an exception if the provided Mojo object type had not previously been registered using PythonModuleBuilder.add_type().
PythonObject to Mojo values​
Within any Mojo code that is handling a PythonObject, but especially within Mojo functions called from Python, it's common to expect an argument of a particular type.
There are two ways in which a PythonObject can be turned into a native Mojo value:
* Converting a Python object into a newly constructed Mojo value that has the same logical value as the original Python object. This is handled by the ConvertibleFromPython trait.
* Downcasting a Python object that holds a native Mojo value to a pointer to that inner value. This is handled by PythonObject.downcast_value_ptr().
PythonObject conversions​
Many Mojo types support conversion directly from equivalent Python types, via the ConvertibleFromPython trait:
🔥 Mojo
# Given a person, clone them and give them a different name.
fn create_person(
    name_obj: PythonObject,
    age_obj: PythonObject
) raises -> PythonObject:
    # These conversions will raise an exception if they fail
    var name = String(name_obj)
    var age = Int(age_obj)

    return PythonObject(alloc=Person(name, age))
Which could be called from Python using:
🐍 Python
person = mojo_module.create_person("John Smith")
Passing invalid arguments will result in a runtime argument error:
🐍 Python
person = mojo_module.create_person(42)
PythonObject downcasts​
Downcasting from PythonObject values to the inner Mojo value:
🔥 Mojo
fn print_age(person_obj: PythonObject) raises:
    # Raises if `obj` does not contain an instance of the Mojo `Person` type.
    var person = person_obj.downcast_value_ptr[Person]()

    print("Person is", person[].age, "years old")
Unsafe mutation via downcasting is also supported. It is up to the user to ensure that this mutable pointer does not alias any other pointers to the same object within Mojo:
🔥 Mojo
fn birthday(person_obj: PythonObject):
    var person = person_obj.downcast_value_ptr[Person]()

    person[].age += 1
Entirely unchecked downcasting-which does no type checking-can be done using:
🔥 Mojo
fn get_person(person_obj: PythonObject):
    var person = person_obj.unchecked_downcast_value_ptr[Person]()
Unchecked downcasting can be used to eliminate overhead when optimizing a tight inner loop with Mojo, and you've benchmarked and measured that type checking downcasts is a significant bottleneck.
Methods​
When binding Mojo objects for use from Python, you can expose chosen methods to Python as well, using PythonTypeBuilder.def_method().
Currently, Mojo methods being exposed to Python must be written with a modification compared to normal Mojo methods: they must be a @staticmethod that takes either py_self: PythonObject or self_ptr: UnsafePointer[Self]:
🔥 Mojo
from python import PythonObject
from python.bindings import PythonModuleBuilder
from os import abort

@export
fn PyInit_mojo_module() -> PythonObject:
    try:
        var mb = PythonModuleBuilder("mojo_module")
        _ = mb.add_type[Person]("Person")
            .def_method[Person.get_name]("get_name")
            .def_method[Person.set_age]("set_age")
        return mb.finalize()
    except e:
        return abort[PythonObject]("error creating Mojo module")

struct Person(Representable):
    var name: String
    var age: Int

    @staticmethod
    fn get_name(py_self: PythonObject) raises -> PythonObject:
        var self_ptr = py_self.downcast_value_ptr[Self]()
        return self_ptr[].name

    @staticmethod
    fn set_age(
        self_ptr: UnsafePointer[mut=True, Self],
        new_age: PythonObject,
    ) raises:
        self_ptr[].age = Int(new_age)

    fn __repr__(self) -> String:
        return String("Person(", self.name, ", ", self.age, ")")
Taking py_self: PythonObject allows access to the full PythonObject allocation that a Mojo object instance is stored inside of. Typically though, taking py_self: UnsafePointer[Self] will minimize boilerplate in the common case that a method merely needs to access the fields of an object.
Mojo methods called from Python are currently required to take non-standard self types due to limitations that will be lifted in future versions of Python Mojo bindings.
Static methods​
Python Mojo bindings supports exposing Python @staticmethods, bound using PythonTypeBuilder.def_staticmethod(). A function declared using def_staticmethod() is callable as a static method on the type within Python, without needing an object instance.
🔥 Mojo
from python import PythonObject
from python.bindings import PythonModuleBuilder
from os import abort

@export
fn PyInit_mojo_module() -> PythonObject:
    try:
        var mb = PythonModuleBuilder("mojo_module")
        mb.add_type[Person]("Person")
            .def_staticmethod[Person.is_valid_age]("is_valid_age")
        return mb.finalize()
    except e:
        return abort[PythonObject]("error creating Mojo module")

struct Person(Representable):
    var name: String
    var age: Int

    @staticmethod
    fn is_valid_age(age_obj: PythonObject) raises -> PythonObject:
        var age = Int(age_obj)
        return 0 <= age <= 130

    fn __repr__(self) -> String:
        return String("Person(", self.name, ", ", self.age, ")")
Calling a Mojo function bound as a static method looks like a typical Python static method call directly on the type object:
🐍 main.py
from mojo_module import Person

print(Person.is_valid_age(45)) # Prints 'True'
print(Person.is_valid_age(-1)) # Prints 'False'
Keyword arguments​
Keyword arguments in Mojo come in two forms:
1. Keyword-only arguments: fn foo(*, x: Int) This is not currently supported in Python Mojo bindings.
2. Variadic keyword arguments: fn foo(**kwargs: Int) This is supported in Python Mojo bindings when used in the unsugared form: fn foo(kwargs: OwnedKwargsDict). (The **kwargs syntax limitation will be removed in the future.)
You can define Mojo functions that accept variadic keyword arguments using OwnedKwargsDict[PythonObject] as the last argument. A simple example looks like:
🐍 Python
import mojo_module

result = mojo_module.sum_kwargs_ints(a=10, b=20, c=30)  # returns 60
🔥 Mojo
from collections import OwnedKwargsDict

def sum_kwargs_ints(kwargs: OwnedKwargsDict[PythonObject]) -> PythonObject:
    var total = 0
    for entry in kwargs.items():
        total += Int(entry.value)
    return PythonObject(total)
Keyword arguments are also supported following normal positional arguments. Additionally, getting specific keyword arguments is a dictionary lookup on the OwnedKwargsDict:
🔥 Mojo
from collections import OwnedKwargsDict

def duration_in_seconds(
    hours_obj: PythonObject,
    minutes_obj: PythonObject,
    kwargs: OwnedKwargsDict[PythonObject]
) -> PythonObject:
    var hours = Int(hours_obj)
    var minutes = Int(minutes_obj)

    var seconds = Int(kwargs["seconds"])

    return hours * 3600 + minutes * 60 + seconds
In this example, if a call to duration_in_seconds() is missing the required "seconds" named argument, a runtime exception will occur:
🐍 main.py
from mojo_module import duration_in_seconds

# Pass hours and minutes, missing "seconds"
duration_in_seconds(4, 5) # ERROR: KeyError
Keyword arguments are supported when bindings top-level functions, methods, and static methods.
Variadic arguments​
Python and Mojo variadic arguments are normally written using the following syntax:
🔥 Mojo
fn foo(*args: Int):
    ...
However, this syntax is not yet supported in Python/Mojo bindings, because functions bound using def_function() support only fixed-arity functions.
As a workaround, you can expose Mojo functions that accept a variadic number of arguments to Python using the lower-level def_py_function() interface, which leaves it to the user to validate the number of arguments provided:
🔥 Mojo
@export
fn PyInit_mojo_module() -> PythonObject:
    try:
        var b = PythonModuleBuilder("mojo_module")
        b.def_py_function[count_args]("count_args")
        b.def_py_function[sum_args]("sum_args")
        b.def_py_function[lookup]("lookup")

fn count_args(py_self: PythonObject, args_tuple: PythonObject) raises:
    return len(args_tuple)

fn sum_args(py_self: PythonObject, args_tuple: PythonObject) raises:
    var total = args_tuple[0]
    for i in range(1, len(args_tuple)):
        total += args_tuple[i]
    return total

fn lookup(py_self: PythonObject, args_tuple: PythonObject) raises:
    if len(args_tuple) != 2 and len(args_tuple) != 3:
        raise Error("lookup() expects 2 or 3 arguments")

    var collection = args_tuple[0]
    var key = args_tuple[1]

    try:
        return collection[key]
    except e:
        if len(args) == 3:
            return args_tuple[2]
        else:
            raise e
Strategies for porting Python to Mojo​
Writing Pythonic code in Mojo​
In this approach to bindings, we embrace the flexibility of Python, and eschew trying to convert PythonObject arguments into the narrowly constrained, strongly-typed space of the Mojo type system, in favor of just writing some code and letting it raise an exception at runtime if we got something wrong.
The flexibility of PythonObject enables a unique programming style, wherein Python code can be "ported" to Mojo with relatively few changes.
🐍 Python
def foo(x, y, z):
    x[y] = int(z)
    x = y + z
Rule of thumb: Any Python builtin function should be accessible in Mojo using Python.<builtin>().
🔥 Mojo
fn foo(x: PythonObject, y: PythonObject, z: PythonObject) -> PythonObject:
    x[y] = Python.int(z)
    x = y + z
Building Mojo extension modules​
You can create and distribute your Mojo modules for Python in the following ways:
* As source files, compiled on demand using the Python Mojo importer hook.
The advantage of this approach is that it's easy to get started with, and keeps your project structure simple, while ensuring that your imported Mojo code is always up to date after you make an edit.
* As pre-built Python extension module .so dynamic libraries, compiled using:
mojo build mojo_module.mojo --emit shared-lib -o mojo_module.so
This has the advantage that you can specify any other necessary build options manually (optimization or debug flags, import paths, etc.), providing an "escape hatch" from the Mojo import hook abstraction for advanced users.
Known limitations​
While we have big ambitions for Python to Mojo interoperability-our goal is for Mojo to be the best way to extend Python-this feature is still in early and active development, and there are some limitations to be aware of. These will be lifted over time.
* Functions taking more than 6 arguments. Currently PyTypeBuilder.add_function() and related function bindings only support Mojo functions that take up to 6 PythonObject arguments: fn(PythonObject, PythonObject, PythonObject, PythonObject, PythonObject, PythonObject).
* Keyword arguments syntax. Currently, Mojo functions called from Python only accept keyword arguments when using a trailing kwargs: OwnedKwargsDict[PythonObject] argument. Support for native **kwargs syntax will be added in the future.
* Mojo package dependencies. Mojo code that has dependencies on packages other than the Mojo stdlib (like those in the ever-growing Modular Community package channel) are currently only supported when building Mojo extension modules manually, as the Mojo import hook does not currently support a way to specify import paths for Mojo package dependencies.
* Properties. Computed properties getter and setters are not currently supported.
* Expected type conversions. A handful of Mojo standard library types can be constructed directly from equivalent Python builtin object types, by implementing the ConvertibleFromPython trait. However, many Mojo standard library types do not yet implement this trait, so may require manual conversion logic if needed.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Python types
When calling Python methods, Mojo needs to convert back and forth between native Python objects and native Mojo objects. Most of these conversions happen automatically, but there are a number of cases that Mojo doesn't handle yet. In these cases you may need to do an explicit conversion, or call an extra method.
Mojo types in Python​
Mojo primitive types implicitly convert into Python objects. Today we support integers, floats, booleans, and strings.
To demonstrate, the following example dynamically creates an in-memory Python module named py_utils containing a type_printer() function, which simply prints the type of a given value. Then you can see how different Mojo values convert into corresponding Python types.
from python import Python

def main():
    py_module = """
def type_printer(value):
    print(type(value))
"""
    py_utils = Python.evaluate(py_module, file=True, name="py_utils")

    py_utils.type_printer(4)
    py_utils.type_printer(3.14)
    py_utils.type_printer(True)
    py_utils.type_printer("Mojo")
<class 'int'>
<class 'float'>
<class 'bool'>
<class 'str'>
Python types in Mojo​
You can also create and use Python objects from Mojo.
Mojo wrapper objects​
When you use Python objects in your Mojo code, Mojo adds the PythonObject wrapper around the Python object. This object exposes a number of common double underscore methods (dunder methods) like __getitem__() and __getattr__(), passing them through to the underlying Python object. Most of the time, you can treat the wrapped object just like you'd treat it in Python. You can use dot-notation to access attributes and call methods, and use the [] operator to access an item in a sequence.
You can explicitly create a wrapped Python object by initializing a PythonObject with a Mojo integer, float, boolean, or string. Additionally, you can create several types of Python collections directly in Mojo using the Python.dict(), Python.list(), and Python.tuple() static methods.
For example, to create a Python dictionary, use the Python.dict() method:
from python import Python

def main():
    py_dict = Python.dict()
    py_dict["item_name"] = "whizbang"
    py_dict["price"] = 11.75
    py_dict["inventory"] = 100
    print(py_dict)
{'item_name': 'whizbang', 'price': 11.75, 'inventory': 100}
With the Python.list() method, you can create a Python list and optionally initialize it:
from python import Python

def main():
    py_list = Python.list("cat", 2, 3.14159, 4)
    n = py_list[2]
    print("n =", n)
    py_list.append(5)
    py_list[0] = "aardvark"
    print(py_list)
n = 3.14159
['aardvark', 2, 3.14159, 4, 5]
The Python.tuple() method creates a Python tuple of values:
from python import Python

def main():
    py_tuple = Python.tuple("cat", 2, 3.1415, "cat")
    n = py_tuple[2]
    print("n =", n)
    print("Number of cats:", py_tuple.count("cat"))
n = 3.1415
Number of cats: 2
If you want to construct a Python type that doesn't have a literal Mojo equivalent, you can also use the Python.evaluate() method. For example, to create a Python set:
from python import Python

def main():
    var py_set = Python.evaluate('{2, 3, 2, 7, 11, 3}')
    num_items = len(py_set)
    print(num_items, "items in the set.")
    contained = 7 in py_set
    print("Is 7 in the set:", contained)
4 items in the set.
Is 7 in the set: True
PythonObject implements the Writable trait. This allows you to print Python values using the built-in print() function, as shown in several of the previous examples.
However, most other Mojo APIs don't accept PythonObject values directly. In these cases you'll need to explicitly convert a Python value into a native Mojo value. For example:
from python import Python
from python import PythonObject


def main():
    var py_string = PythonObject("Hello, Mojo!")
    var py_bool = PythonObject(True)
    var py_int = PythonObject(123)
    var py_float = PythonObject(3.14)

    var mojo_string = String(py_string)
    var mojo_bool = Bool(py_bool)
    var mojo_int = Int(py_int)
    var mojo_float = Float64(py_float)
Comparing Python types in Mojo​
You can use Python objects in Mojo comparison expressions, and the Mojo is operator also works to compare the identity of two Python objects. Python values like False and None evaluate as false in Mojo boolean expressions as well.
If you need to know the type of the underlying Python object, you can use the Python.type() method, which is equivalent to the Python type() builtin. You can test if a Python object is of a particular type by performing an identity comparison against the type as shown below:
from python import Python
from python import PythonObject

def main():
    var value1: PythonObject = 3.7
    value2 = Python.evaluate("10/3")

    # Compare values
    print("Is value1 greater than 3:", value1 > 3)
    print("Is value1 greater than value2:", value1 > value2)

    # Compare identities
    value3 = value2
    print("value1 is value2:", value1 is value2)
    print("value2 is value3:", value2 is value3)

    # Compare types
    py_float_type = Python.evaluate("float")
    print("Python float type:", py_float_type)
    print("value1 type:", Python.type(value1))
    print("Is value1 a Python float:", Python.type(value1) is py_float_type)
Is value1 greater than 3: True
Is value1 greater than value2: True
value1 is value2: False
value2 is value3: True
Python float type: <class 'float'>
value1 type: <class 'float'>
Is value1 a Python float: True
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Mojo vision
Our vision for Mojo is to be the one programming language developers need to target diverse hardware-CPUs, GPUs, and other accelerators-using Python's intuitive syntax combined with modern systems programming capabilities.
Although this vision focuses on the Mojo language, we recognize it's just one part of a larger Mojo ecosystem. When combined, the developer tools, the community, and the landscape of Mojo libraries are arguably more important at scale. However, the purpose of this document is to share more about our motives and aspirations for the language itself, because it supports everything else.
This vision serves as a baseline to guide our decision-making as the language continues to evolve. This is a "directional" vision, not an engineering plan. For a look at some of the planned work, see the Mojo roadmap.
Mojo's role in Modular's mission​
Mojo plays a key role in Modular's mission to democratize AI compute. Let's break down the mission into its component parts:
* Democratize: This is a social statement, saying that we want to free, unlock, and enable more people to participate.
* AI compute: We have long passed the end of Moore's law, and are awash with a wide range of accelerators: GPUs, TPUs, and accelerated CPUs, spanning IoT, edge, client, datacenter, and supercomputer applications. (Our ambition is to eventually expand into "All Compute," but there are a few steps between here and there.)
Mojo is how we bring these two ideas together-democratization and AI compute-into a single, coherent solution. To achieve this we want to:
* Unite developers across domains, skill levels, and backgrounds (enterprise engineers, academics, hobbyists, etc.). We aim to solve the complexity of juggling Python, C++, Rust, CUDA, and more (the "N language problem") by enabling developers to grow their skill-sets incrementally within a single language.
* Unify hardware by giving developers access to a wide range of hardware-CPUs, GPUs, TPUs, and emerging accelerators-with consistent tools and programming models.
It should be easy to start using Mojo as a Python developer, and incrementally adopt new Mojo features to master CPU performance and scale your abilities into GPU programming and other accelerated hardware.
This mission is vast and ambitious. Many have attempted to solve this and have fallen short. Achieving it will take years of focused development, but we feel it is worth doing. We believe Mojo can help unlock creativity, productivity, and applications we haven't yet imagined.
Why Mojo was built from scratch​
Modern accelerators are complex and very different from traditional CPUs. They have features like tensor cores, systolic arrays, dedicated convolutional units, explicit memory hierarchies, memory transfer accelerators (such as the Tensor Memory Accelerator on Hopper and Blackwell GPUs), and a variety of exotic and rapidly-evolving data types like float6. Achieving our mission to unify hardware development means Mojo must provide full programmability and deliver the full performance potential of any given accelerator chip.
There are only three ways to tackle the problems we are trying to solve. Let's briefly evaluate the pros and cons of each:
1. Extend an existing language like C++, Rust, Julia, Swift:
o Pro: You get an existing implementation and community.
o Con: None of these languages support the hardware features we need-they were designed for CPUs. They are also all 10+ years old, don't provide the modern meta-programming features we need, and weren't designed to support hardware features required for AI (such as float6).
2. Create an embedded DSL for a language like Python or C++:
o Pro: This is comparatively easy to implement.
o Con: The tooling, UX, and predictability of these systems are very problematic and they are limited by the base language syntax. This is particularly problematic if you're trying to introduce fundamental new concepts because you can't change the grammar of Python or C++. More about eDSLs
3. Build an entirely new programming language from scratch:
o Pro: You get full control to create the best quality result.
o Con: This is extremely expensive and difficult to do. There are many ways to get this wrong and you must have a strong set of principles to guide development. For comparison, CUDA is a C++ extension and runtime-nothing as ambitious as a new programming language.
We ruled out the first two options because they're insufficient for achieving the full scope of our vision. Failure to understand the constraints of each approach and denial about the boundaries within them is a core reason many previous systems had a promising start but ultimately hit a ceiling for their generality and usability that prevents them from fully democratizing AI compute.
We believe GPUs, TPUs, and other accelerators are the natural evolution of compute going forward and demand high-quality software to achieve their full potential. Therefore, we believe it's worthwhile to bet big, rather than do something easier that might get near-term results but wither away over time as AI and accelerator hardware continues to rapidly evolve.
Overarching design principles​
Because Mojo will evolve over time, it's essential to prioritize deliberately-staying focused on our long-term goals while making pragmatic short-term decisions. The following are the high-level design principles that guide Mojo's development.
Member of the Python family​
Mojo adopts Python's syntax and should feel familiar to Python developers-Python is not only one of the most popular programming languages in the world, but it's also the dominant language in AI. Python is beloved for its clean and readable syntax, small core language (compared to many alternatives), powerful metaprogramming, and its role as a "universal superglue" for integrating complex systems across language boundaries.
That's why Mojo supports the core features Python programmers instinctively reach for-if/for statements, lists, dictionaries, etc.-so it's easy to migrate code. Mojo will support more Python features over time, but our primary focus is on building features that unlock high-performance, portable compute-not on quickly achieving surface-level Python compatibility.
Scalable AI kernel development​
A key principle for Mojo is to overcome the fundamental scalability limitations that plague traditional kernel libraries and ML compilers, and become a unified language for kernel development.
Kernel libraries, while initially useful, become hard to manage as systems grow. ML compilers, despite their sophistication, often lack the generality needed for diverse tasks like data loading, pre-processing, dynamic shapes, and sparsity-they failed to provide an "it just works" experience. Even other MLIR-based compiler systems failed to solve this due to a fragmented development process that couldn't scale to handle the constantly changing requirements in numerics, data types, AI modeling, and hardware.
Thus, while building our inference engine for the Modular Platform, we wanted a new way to write kernels that could scale with the ever-evolving AI industry. We took inspiration from kernel programming systems (CUDA, CUTLASS, DSLs, etc), and built a way to express common kernel development patterns in MLIR. Then we took a step further and generalized those patterns into a new language that's suitable for high-performance kernel development. For example, Mojo includes zero-cost abstractions, knobs that can be tuned for optimal hardware performance, a library-first design, and metaprogramming to allow specialization for particular hardware.
A modern systems programming language​
While Mojo builds on Python's syntax, it must address the realities of modern accelerators, which are essentially high-performance embedded systems. For example, you don't want to upload megabytes of code just to run a matrix multiplication, and you can't afford implicit performance overhead in inner loops. These requirements drive Mojo to go beyond Python with capabilities designed for low-level numerical and hardware-focused programming.
Mojo introduces systems programming constructs such as a static type system (dynamic typing will come later), memory management control, and predictable performance semantics. It draws on lessons from modern languages like Swift, C++, Rust, and Zig-and goes beyond them by embracing new ideas that target the breadth of exotic hardware AI developers now face.
For more details, see the architectural bets for Mojo below.
Managing language complexity​
The complexity of some systems programming languages (notably C++) has spiraled out of control by continually adding new features that don't quite fit together. This happens due to a "tragedy of the commons" situation where every individual language feature is justified by some specific cohort or use-case, but all users of the language suffer from the aggregate complexity.
Python isn't perfect, but it has retained a relative simplicity-notably evolving from Python 2 to 3 with care to improve its consistency and orthogonality. Other programming languages like Go pride themselves on maintaining simplicity and saying "no" to proposals that don't benefit long-term goals (example blog post explaining this).
Mojo must go beyond Python by adding new systems programming features aligned with our mission-but in doing so, we face the same scope-creep pressures that every growing language confronts.
We aim to control complexity through a few specific strategies:
1. Use Mojo heavily inside Modular: Modular is Mojo's largest user and maintains the world's largest Mojo codebase (which is open source). This gives us direct insight into real-world usability and performance. We use our own experience, as well as feedback from our enthusiastic community, to guide prioritization.
2. Align with Python wherever possible: If Python already supports a feature, we adopt its design rather than inventing something new. Any deviation from Python requires a strong, mission-driven justification.
3. We adopt proven ideas from modern languages: When new features are required (such as static types, traits, metaprogramming), we draw from languages like Rust, Swift, and Zig rather than create novel and untested solutions.
4. Innovate only when necessary: Where existing designs fall short-such as ergonomics in Rust or compile-time error messages in Zig-we aim beyond them to meet Mojo's goals.
5. Emphasize composability and simplicity: Every Mojo feature must work reliably in all situations and combine seamlessly with other features (compose orthogonally). We're not satisfied with features that work 80% of the time but fail in edge cases.
6. Defer syntactic sugar: Language sugar is often tempting, but we prioritize core "big rocks" first. Only once the fundamentals are solid do we revisit syntactic enhancements.
These are guiding principles, not a rigid recipe-language design is fundamentally about balancing tradeoffs. The Mojo team draws on deep experience, learns through continuous implementation and iteration, and listens to feedback from the broader community.
Architectural bets for Mojo​
We believe building a new programming model for the future of AI and systems programming requires first-principles thinking, not incremental evolution. That's why our vision for Mojo is built upon a few specific architectural bets, as described in this section.
From the start, our team made a foundational bet: By uniting three key technologies, we can build a new kind of systems programming architecture that scales across the full range of hardware targets while maximizing software reuse across heterogeneous hardware.
Thus, Mojo's architecture is built upon the following technologies:
1. Powerful parametric meta-programming
2. MLIR Core
3. MAX framework integration
This design is built on experience, not speculation. In 2022, we spent most of the year prototyping and validating this approach through deep compiler R&D. Eventually, we had the architectural conviction that the idea was viable, but only if we could improve and scale it. The next step was to make this power accessible to developers, not just compiler engineers. That's what led us to create Mojo.
Let's explore each of these architectural pillars in more detail.
Powerful parametric meta-programming​
Accelerators are incredibly diverse, and they're constantly evolving. Our goal is to drastically reduce the time and effort required to bring up a software stack for a new chip. We believe the work should be proportional to how different that chip is, rather than starting from scratch for every architecture.
The core insight behind our approach is this: while no two accelerators are exactly alike, their target workloads and macro-architectures share deep structural similarities. For example, NVIDIA's Hopper architecture extends from Ampere. AMD's MI300 has meaningful overlap with both. And across the industry, the "tensor core" has become ubiquitous, showing up in CPUs, GPUs, and custom ASICs. These units may be quirky in their own ways, but their purpose is the same: efficiently run matrix multiplications.
Previous attempts to democratize AI compute often failed to capitalize on this commonality. Many were built on fragmented, vendor-specific libraries like cuBLAS or rocBLAS, which prevented true cross-architecture unification. At Modular, we made a different bet: we could reimplement and unify these software stacks ourselves-for example, build a graph compiler and runtime stack (MAX) without CUDA-and use that as a basis to abstract across architectures.
Of course, this only works if it scales. The challenge is combinatorial: the cross-product of all data types, operators, and hardware targets is too large to implement by hand. That's why we leaned into powerful meta-programming. We took the ideas behind C++ templates (compile-time polymorphism and specialization) and built something dramatically more usable, with better error messages, faster compile times, more expressiveness, and a smoother developer experience.
In late 2022, we validated this approach with an early prototype of the Mojo parameter system. Even in its primitive form, it enabled us to implement matrix multiplication in a unified way and match or exceed vendor BLAS libraries across a range of CPUs. This architectural bet has paid off many times over-it's what allows MAX to scale across hardware with high performance and maintainable code.
MLIR Core​
MLIR is a widely used compiler infrastructure for building domain-specific compilers-powering systems across AI accelerators, CPUs, hardware design, quantum computing, and more. Within it, you can think of MLIR Core as a flexible "compiler construction toolkit," providing the building blocks needed to create powerful custom compilers.
The broader MLIR project includes many AI-related dialects, such as linalg, affine, and scf, but Mojo doesn't use any of these-Mojo is built purely on top of MLIR Core.
Mojo is powered by a novel compiler framework, historically code-named KGEN (for "kernel generator"). KGEN is built using MLIR Core and forms the backbone of Mojo's metaprogramming capabilities. It allows explicitly parametric code to be represented before instantiation, which enables a host of benefits: faster compile times, clearer error messages, and support for compiling the same source code to multiple target devices.
Another key design choice in Mojo is that it acts as syntactic sugar for MLIR. This means Mojo code can directly express MLIR dialect operations, without modifying the Mojo compiler itself. While not all MLIR dialects are supported, Mojo is designed to cover the most important ones needed for accelerator programming. Broader dialect support is possible in the future, but it's not a near-term priority.
For a deep dive into how Mojo uses MLIR and KGEN, see the video, Modular Tech Talk: Kernel Programming and Mojo.
MAX framework integration​
Mojo's low-level programming model and MLIR-based foundation make it easy to write high-performance code, but raw kernel performance isn't the whole story. In AI and other advanced domains, some of the biggest gains come from graph-level optimizations like kernel fusion.
That's why Mojo is designed to integrate seamlessly into the MAX framework-our graph compiler and runtime stack. This integration allows developers to directly extend MAX using Mojo (for example, write custom graph ops)-without modifying the graph compiler itself-and still benefit from advanced optimizations and code transformations. This is made possible by Mojo building an MLIR representation of the kernel code before instantiating it with parameters. This intermediate representation (IR) allows the graph compiler to reflect over the kernel to understand the inputs, outputs, as well as transforming the IR of custom kernels directly.
As Mojo evolves, a key goal remains enabling and enriching the MAX framework. We want to unlock new forms of optimization and fusion that only become possible when you reason across combinations of kernels-not just individual operators. These kinds of transformations can dramatically improve performance, reduce memory usage, and lower the cost of deploying high-performance AI systems at scale.
Looking ahead​
Language design is expensive. It's difficult and ambiguous. But when done right, it creates leverage that compounds over time, enabling not just performance, but creativity, composability, and community growth.
Although Mojo is still early in its journey, it stands on a carefully engineered foundation-one designed to scale across devices, abstractions, and time. These investments are already paying off, and we believe they position Mojo to grow into a truly transformative technology for the AI era and beyond.
We're building it together, and we're building it to last. We want to eventually create a vibrant ecosystem where people use Mojo to build and share a wide range of AI applications, large scale distributed systems, database connectors, and much more-putting the power of the world's compute hardware at your fingertips. That's when we'll feel like Mojo is truly on fire 🔥!
For more detail about what's left to do, see the Mojo roadmap.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit

Mojo roadmap
This page provides a high-level roadmap of how we expect the Mojo programming language to evolve over a series of phases. This roadmap provides directional guidance (not an engineering plan) and is subject to change. As we build, learn, and expand Mojo's use cases, we will iterate, adapt, and invest wherever necessary to unblock priorities.
We also periodically share roadmap updates in the Modular forum announcements section.
Phases
The phases below are conceptual groups of work-not version numbers-and they have no timeline for completion. We haven't yet established a long-term versioning scheme, finalized our approach to language evolution, or decided what "stability" will mean or when it will happen.
Work items
An empty box ⬜ indicates work that's not started, a barricade 🚧 is for work in progress, and a checked box ✅ means it's done. However, this is not an exhaustive list of work and the status might be out of date. Also, the items in each list aren't necessarily ordered by priority and some items may be "nice to have" and not required to complete the phase.
Mojo's north star​
As described in the Mojo vision, we created Mojo to unite developers with a single language that provides ergonomic programming features for accelerator hardware and that scales to solve other challenges in AI and systems programming.
Our goals are ambitious and we're taking on challenges that many languages and systems have struggled with for decades. We believe Mojo has the right blend of technology, design principles, and community-first philosophy to succeed, but it'll work only if we stay focused and make deliberate tradeoffs aligned with our long-term vision.
The stakes are high-programming languages that succeed shape entire ecosystems, support millions of developers, and define how software is built. That means we must resist the urge to chase short-term wins at the expense of long-term clarity, consistency, and quality.
Our approach is to keep short-term development focused and anchored on measurable outcomes, while building for generality so Mojo can eventually become a general-purpose language spanning CPUs, GPUs, and other hardware, plus the myriad of applications that Python is used for.
We know the path won't be perfect and we'll make mistakes. But with a strong foundation and an engaged, thoughtful community, we can learn, iterate, and improve together.
Phase 0: Initial bring-up​
✅
COMPLETE
Phase 0 focused on foundational language work: implementing the core parser, defining memory types, functions, structs, initializers, argument conventions, and more.
As development accelerated, multiple libraries emerged to fill immediate needs, often overlapping in functionality (e.g., multiple pointer types). As the language stabilized, we consolidated these libraries into a coherent and consistent foundation.
Phase 1: High performance CPU + GPU coding​
🚧
IN PROGRESS
Phase 1 takes Mojo from a "prototype kernel DSL" to a viable foundation for real-world accelerated compute workloads. This phase focuses on making Mojo a powerful and expressive language for writing high-performance kernels on CPUs, GPUs, and ASICs. We also want to unlock other performance use-cases for CPUs, particularly the ability to extend Python packages in a seamless way.
But performance alone isn't enough. We're equally focused on:
* Expressiveness for building robust libraries
* Good error messages for developer productivity
* Fast compile times to support iteration speed
The conclusion of phase 1 is the natural point to open source the Mojo compiler.
Generics and metaprogramming features​
The backbone of phase 1 is Mojo's metaprogramming system, combined with a modern generic type system that catches errors at compile time, before code is instantiated. The following features capture how we think Mojo should evolve, but may change as we grow into them:
* ✅ Compile-time constructs: The parameter system, compile-time interpreter, if and for loops, etc.
* ✅ Trait unions: Allow Copyable & Movable intersections for precise trait conformance. See the trait composition proposal.
* ✅ Predictable dependent types: Support advanced parametric algorithms while avoiding rebinding.
* ✅ Parametric aliases: Compute parametric values, going beyond types and functions. See the parametric aliases proposal.
* ✅ Default trait methods: Enable static composition (mixin-style).
* 🚧 Closure refinement: Improve capture modeling and unify compile-time/runtime representations.
* 🚧 where clauses: Enable early constraint checking and better error messages for generics. See the where clauses proposal.
* 🚧 Conditional conformance: Allow trait conformance based on predicates over parameters.
* 🚧 Struct extensions: Post-hoc type extension and better modular refactoring. See the struct extension proposal.
* 🚧 Linear types: Support ownership patterns; blocked on conditional conformance.
* ⬜ Stabilization markers: Mechanism to tag standard library APIs with maturity levels.
* ⬜ Parametric raises: The ability to throw types other than Error and therefore support higher order functions like map that propagate the "raisability" of their closure argument.
Python interoperability​
We want Mojo to be an approachable way to extend and speed up existing Python code. We'd like to incorporate the key features of popular libraries like "nanobind" as a guideline:
* ✅ Build integration: Seamless connection between Mojo's build system and Python packaging.
* 🚧 Python export: Support for exposing functions and initializers to Python.
* ⬜ Debugger/LSP support for mixed Python/Mojo applications.
Core language usability and ergonomics​
Mojo should "just work" for core Python-like tasks, while offering the control systems programmers expect:
* ✅ Basic constructs: Functions, structs, control flow (if, for, etc.).
* ✅ Literal support: Infinite-precision int/float, collections, comprehensions.
* 🚧 Collections: Round out core types like List, Dict, Iterator, SIMD, String, etc., to utilize the language feature set of phase 1. For instance, see making SIMD comparable and the string type proposal.
* 🚧 Unsafe programming: Refine UnsafePointer and low-level primitives.
* 🚧 Variadic args: Full support for *args, **kwargs.
* ⬜ Lambda syntax for inline closure declarations.
* ⬜ Stable and robust toolchain: Cross-compilation, packaging and build system, testing and benchmarking framework in the standard library, debugger, profiler etc. We expect to make a big dent on this, but won't fully complete everything interesting in this space in phase 1.
Syntax and surface language polish​
These may seem small, but they significantly impact developer ergonomics and reduce future source incompatibilities:
* ✅ Argument conventions: Refine init/del behaviors, naming, and default conventions.
* ✅ Literal refinements: Improve reliability of infinite precision literals using dependent types. See the collection literal design and fixing simple literals proposal.
* 🚧 Attribute macros: Replacing ad-hoc constructs like @register_passable("trivial"), @nonmaterializable, @value, etc., using traits and other existing language features (shrinking the language). For instance, see the upgrading "trivial" proposal and upgrading @value decorator.
Non-goals​
We are intentionally not pursuing the following in phase 1:
* Syntax sugar: We defer most sugar until the core language is stable and composable.
* Untyped Python-style code: For now, explicit PythonObject type annotations are required.
* Python library parity: We expect there will be many missing APIs (e.g. potentially file system APIs) in phase 1.
Phase 2: Systems application programming​
⏰
NOT STARTED
Once the core generic type system and systems programming features converge and stabilize, we'll begin expanding Mojo to support application-level programming-the kinds of problems typically addressed with languages like Rust and C++.
That said, we are not aiming to match Rust or C++ feature-for-feature. Our goal is to keep Mojo a relatively small and teachable language-one that solves specific problems without unnecessary complexity.
None of the features below are currently scoped, and we aren't interested in design discussions about them. We list them here so we can understand non-goals for phase 1 and rationalize how things add up over time. Our focus remains firmly on completing phase 1 before investing significant effort here.
Language and abstraction features​
* First-class async support: Fully integrated with Mojo's type and memory models.
* Existentials / dynamic traits: For building flexible runtime abstractions.
* Richer metatypes: Extending support for type-level programming.
* Algebraic data types & pattern matching: Enabling expressive state modeling.
* Dynamic reflection features.
* Initial distributed programming support.
Memory safety model: fast, expressive, gradual​
Mojo code should be memory-safe by default, while still being fast, expressive, and gradually more complex for beginners. Although phase 1 provides a strong framework for memory safety, there still exist situations where safety is not guaranteed by default and we expect won't be fully resolved until phase 2.
* Fast: Mojo should enable the programmer to write high-performance memory-safe code by default, while continuing to provide unsafe primitives for expert use.
* Expressive: Mojo's origin system should enable more mutable aliasing to provide a more expressive programming model than languages like Rust. This is desirable because aliased mutable objects occur widely in both the Python and C++ communities, often for legitimate reasons of expressiveness, simplicity, and/or performance.
* Gradual: We want progressive disclosure of complexity-it should be easy to get started and be immediately productive in Mojo, and users can gradually learn the more advanced features as they need them.
This will likely require:
* Finalize indirect origins.
* Support mutable aliasing of struct fields.
* Support mutable aliasing of function arguments.
* Prevent dereferencing any references to fields of deallocated reference-counted objects.
* Access control features, e.g. private modifiers (or formalizing the underscore convention) to prevent violating abstraction boundaries.
* Eliminate undefined behavior, especially in core types and built-in operations.
This will get us closer to the goal of making Mojo's memory safety model fast, expressive, and gradually more complex for beginners-making Mojo more approachable to Python and C++ developers.
Other application-level programming features​
We might explore hygienic, importable macros to extend Mojo's surface syntax in controlled ways. For example, enable constructs like parallel_for that feel as seamless as a built-in for loop but that are defined in libraries.
We also might explore making the Mojo optimizer extensible with "library defined" optimizations. For example, allow Mojo library authors to define rewrites like String.__add__(x, String.__add__(y, z)) into String.concat(x, y, z) . The exact shape and ultimate capability of this will be defined later.
Phase 3: Dynamic object oriented programming​
⏰
NOT STARTED
Eventually, we want Mojo to support the core dynamic features that make Python great, including support for untyped variables, classes, inheritance, etc. We have some thoughts about how these features will compose into the other features, but defer detailed planning and scoping until the earlier phases are done.
As Mojo matures through phase 3, we believe Mojo will become increasingly compatible with Python code and deeply familiar to Python users, except more efficient, powerful, coherent, and safe. Mojo may or may not evolve into a full superset of Python, and it's okay if it doesn't.
We're encouraged by how well AI-assisted coding tools already help migrate Python to Mojo today, and we're confident that future tooling and ecosystem maturity will make this evolution even smoother.
Continuous investments​
The following topics will remain in progress throughout Mojo's lifetime and are not tied to any specific phase:
* Error messages and diagnostics: Always room for improvement, but parameter inference and elaborator errors need improvement in particular.
* Compile times: We will continue pushing for faster developer iteration cycles.
* Standard library cleanup: API consolidation, regularization, and new capabilities.
* Hardware support: Extending Mojo's backend to support new architectures.
Contributing to Mojo​
We're committed to open-sourcing all of Mojo, but the language is still very young and we believe a tight-knit group of engineers with a common vision moves faster than a community-driven effort. So we will continue to plan and prioritize the Mojo roadmap within Modular until more of its internal architecture is fleshed out.
However, the Mojo standard library is already open sourced and we'd love to accept your contributions there. Although this roadmap mentions some standard library types because they depend on changes in the Mojo compiler, a lot of work on the standard library is not accounted for here.
You can learn more about contributing to the standard library from the contributor doc below:
* Contributor guide
If you encounter any bugs with Mojo, please submit an issue on GitHub.
Was this page helpful?
Thank you! We'll create more content like this.
Thank you for helping us improve!
😔 What went wrong?

Some code doesn't work

It includes inaccurate information

It's missing information I need

It was difficult to understand

Other
Submit


